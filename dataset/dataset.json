[
    {
        "statement": "Let \\(\\{C_j\\}_{j\\in J}\\) be an arbitrary family of convex sets. Then\n\\[\nC:=\\bigcap\\{C_j:\\; j\\in J\\}\n\\]\nis convex.",
        "title": "",
        "label": "prop:CAMA-chap3-1.2.1",
        "lean_tag": [
            "CAMA_chap_3_1_2_1"
        ],
        "lean_formalization": "lemma CAMA_chap_3_1_2_1 {n : ℕ}\n  (J : Set ℕ)\n  (C : ℕ → Set (EuclideanSpace ℝ (Fin n)))\n  (hC : ∀ i, Convex ℝ (C i)) :\n  Convex ℝ (⋂ (j : ℕ) (_ : j ∈ J), (C j)) := by\n  sorry",
        "proof": "Immediate from the very Definition 1.1.1."
    },
    {
        "statement": "For $i=1,\\dots,k$, let $C_i\\subset\\mathbb{R}^{n_i}$ be convex sets. Then $C_1\\times\\cdots\\times C_k$ is a convex set of $\\mathbb{R}^{n_1}\\times\\cdots\\times\\mathbb{R}^{n_k}$.",
        "title": "",
        "label": "prop:CAMA-chap3-1.2.3",
        "lean_tag": "no-lean-tag",
        "proof": "Straightforward."
    },
    {
        "statement": "Let $A:\\mathbb{R}^n\\to\\mathbb{R}^m$ be an affine mapping and $C$ a convex set of $\\mathbb{R}^n$. The image $A(C)$ of $C$ under $A$ is convex in $\\mathbb{R}^m$.\n\nIf $D$ is a convex set of $\\mathbb{R}^m$, the inverse image\n\\[\nA^{-1}(D):=\\{x\\in\\mathbb{R}^n:\\;A(x)\\in D\\}\n\\]\nis convex in $\\mathbb{R}^n$.",
        "title": "",
        "label": "prop:CAMA-chap3-1.2.4",
        "lean_tag": [
            "CAMA_chap_3_1_2_4"
        ],
        "lean_formalization": "lemma CAMA_chap_3_1_2_4 {m n : ℕ}\n  (A : AffineMap ℝ (EuclideanSpace ℝ (Fin n)) (EuclideanSpace ℝ (Fin m)))\n  (C : Set (EuclideanSpace ℝ (Fin n)))\n  (D : Set (EuclideanSpace ℝ (Fin m)))\n  (hC : Convex ℝ C)\n  (hD : Convex ℝ D) :\n  Convex ℝ (Set.image A C) ∧\n  (Convex ℝ (Set.preimage A D)) := by\n  sorry",
        "proof": "For $x$ and $x'$ in $\\mathbb{R}^n$, the image under $A$ of the segment $[x,x']$ is clearly the segment $[A(x),A(x')]\\subset\\mathbb{R}^m$. This proves the first claim, but also the second: indeed, if $x$ and $x'$ are such that $A(x)$ and $A(x')$ are both in the convex set $D$, then every point of the segment $[x,x']$ has its image in $[A(x),A(x')]\\subset D$."
    },
    {
        "statement": "If $C$ is convex, so are its interior $\\operatorname{int}C$ and its closure $\\overline{C}$.",
        "title": "",
        "label": "prop:CAMA-chap3-1.2.7",
        "lean_tag": [
            "CAMA_chap_3_1_2_7"
        ],
        "lean_formalization": "lemma CAMA_chap_3_1_2_7 {n : ℕ}\n  (C : Set (EuclideanSpace ℝ (Fin n)))\n  (hC : Convex ℝ C) :\n  Convex ℝ (interior C) ∧ Convex ℝ (closure C):= by\n  sorry",
        "proof": "For given different $x$ and $x'$, and $\\alpha\\in]0,1[$, we set $x''=\\alpha x+(1-\\alpha)x'\\in]x,x'[$.\n\nTake first $x$ and $x'$ in $\\operatorname{int}C$. Choosing $\\delta>0$ such that $B(x',\\delta)\\subset C$, we show that $B(x'',(1-\\alpha)\\delta)\\subset C$. As often in convex analysis, it is probably best to draw a picture. The ratio $\\|x''-x\\|/\\|x'-x\\|$ being precisely $1-\\alpha$, Fig.\\ 1.2.3 clearly shows that $B(x'',(1-\\alpha)\\delta)$ is just the set $\\alpha x+(1-\\alpha)B(x',\\delta)$, obtained from segments with endpoints in $\\operatorname{int}C$: $x''\\in\\operatorname{int}C$.\n\nNow, take $x$ and $x'$ in $\\operatorname{cl}C$: we select in $C$ two sequences $\\{x_k\\}$ and $\\{x_k'\\}$ converging to $x$ and $x'$ respectively. Then, $\\alpha x_k + (1-\\alpha)x_k'$ is in $C$ and converges to $\\alpha x + (1-\\alpha)x'$, which is therefore in $\\operatorname{cl}C$."
    },
    {
        "statement": "A set $C\\subset\\mathbb{R}^n$ is convex if and only if it contains every convex combination of its elements.",
        "title": "",
        "label": "prop:CAMA-chap3-1.3.3",
        "lean_tag": [
            "CAMA_chap_3_1_3_3"
        ],
        "lean_formalization": "lemma CAMA_chap_3_1_3_3 {n : ℕ}\n  (C : Set (EuclideanSpace ℝ (Fin n))) :\n  (Convex ℝ C) ↔ (∀ z ∈ (conv C), z ∈ C) := by\n  sorry",
        "proof": "The condition is sufficient: convex combinations of two elements just make up the segment joining them. To prove necessity, take $x_1,\\dots,x_k$ in $C$ and $\\alpha=(\\alpha_1,\\dots,\\alpha_k)\\in\\Delta_k$. One at least of the $\\alpha_i$'s is positive, say $\\alpha_1>0$. Then form\n\\[\ny_2:=\\frac{\\alpha_1}{\\alpha_1+\\alpha_2}x_1+\\frac{\\alpha_2}{\\alpha_1+\\alpha_2}x_2\n\\quad\\left[= \\frac{1}{\\alpha_1+\\alpha_2}(\\alpha_1x_1+\\alpha_2x_2)\\right]\n\\]\nwhich is in $C$ by Definition 1.1.1 itself. Therefore,\n\\[\ny_3:=\\frac{\\alpha_1+\\alpha_2}{\\alpha_1+\\alpha_2+\\alpha_3}y_2+\\frac{\\alpha_3}{\\alpha_1+\\alpha_2+\\alpha_3}x_3\n\\quad\\left[= \\frac{1}{\\sum_{i=1}^3\\alpha_i}\\sum_{i=1}^3\\alpha_i x_i\\right]\n\\]\nis in $C$ for the same reason; and so on until\n\\[\ny_k:=\\frac{\\alpha_1+\\cdots+\\alpha_{k-1}}{1}y_{k-1}+\\frac{\\alpha_k}{1}x_k\n\\quad\\left[= \\tfrac{1}{1}\\sum_{i=1}^k\\alpha_i x_i\\right].\n\\]"
    },
    {
        "statement": "The convex hull can also be described as the set of all convex combinations:\n\\[\n\\operatorname{co}S:=\\bigcap\\{C: C\\ \\text{is convex and contains }S\\}\n= \\Big\\{x\\in\\mathbb{R}^n : \\text{for some }k\\in\\mathbb{N}_*,\\ \\text{there exist }x_1,\\dots,x_k\\in S\\ \\text{and }\n\\alpha=(\\alpha_1,\\dots,\\alpha_k)\\in\\Delta_k\\ \\text{such that }\\sum_{i=1}^k\\alpha_i x_i=x\\Big\\}.\n\\tag{1.3.2}\n\\]",
        "title": "",
        "label": "prop:CAMA-chap3-1.3.4",
        "lean_tag": [
            "CAMA_chap_3_1_3_4"
        ],
        "lean_formalization": "lemma CAMA_chap_3_1_3_4 {n : ℕ}\n  (C : Set (EuclideanSpace ℝ (Fin n))) :\n  (convexHull ℝ C) = (conv C) := by\n  sorry",
        "proof": "Call $T$ the set described in the rightmost side of (1.3.2). Clearly, $T\\supset S$. Also,\nif $C$ is convex and contains $S$, then it contains all convex combinations of elements\n\nFor this, take two points $x$ and $y$ in $T$, characterized respectively by $(x_{1},\\alpha_{1}),\\dots,$\n$(x_{k},\\alpha_{k})$ and by $(y_{1},\\beta_{1}),\\dots,(y_{\\ell},\\beta_{\\ell})$; take also\n$\\lambda\\in[0,1]$. Then $\\lambda x+(1-\\lambda)y$ is a certain combination of $k+\\ell$ elements of $S$; this combination is convex because its coefficients $\\lambda\\alpha_{i}$ and $(1-\\lambda)\\beta_{j}$ are nonnegative, and their sum is\n\\[\n\\lambda\\sum_{i=1}^{k}\\alpha_{i}+(1-\\lambda)\\sum_{j=1}^{\\ell}\\beta_{j}=\\lambda+1-\\lambda=1.\n\\]"
    },
    {
        "statement": "(C. Carathéodory) Any $x\\in\\operatorname{co}S\\subset\\mathbb{R}^n$ can be represented as a convex combination of $n+1$ elements of $S$.",
        "title": "",
        "label": "thm:CAMA-chap3-1.3.6",
        "lean_tag": [
            "CAMA_chap_3_1_3_6"
        ],
        "lean_formalization": "lemma CAMA_chap_3_1_3_6 {n : ℕ}\n  (S : Set (EuclideanSpace ℝ (Fin n))) :\n  ∀ x ∈ (convexHull ℝ S),\n  ∃ (v : Fin (n + 1) → EuclideanSpace ℝ (Fin n)) (a : EuclideanSpace ℝ (Fin (n + 1))),\n  (∀ i, v i ∈ S) ∧ (a ∈ Δκ (n + 1)) ∧ (x = ∑ i, a i • v i) := by\n  sorry",
        "proof": "Take an arbitrary convex combination $x=\\sum_{i=1}^k \\alpha_i x_i$, with $k>n+1$. We will show that one of the $x_i$'s can be assigned a $0$-coefficient without changing $x$. For this, assume that all coefficients $\\alpha_i$ are positive (otherwise we are done).\n\nThe $k>n+1$ elements $x_i$ are certainly affinely dependent: (1.3.1) tells us that we can find $\\delta_1,\\dots,\\delta_k$, not all zero, such that\n\\[\n\\sum_{i=1}^k \\delta_i x_i = 0 \\quad\\text{and}\\quad \\sum_{i=1}^k \\delta_i = 0.\n\\]\n\nThere is at least one positive $\\delta_i$; and we can set $\\alpha_i':=\\alpha_i-t^*\\delta_i$ for $i=1,\\dots,k$, where\n\\[\nt^*:=\\max\\{t\\ge 0:\\ \\alpha_i-t\\delta_i\\ge 0\\ \\text{for }i=1,\\dots,k\\}\n=\\min_{\\delta_j>0}\\frac{\\alpha_j}{\\delta_j}.\n\\]\n\nClearly enough,\n\\[\n\\alpha_i'\\ge 0\\quad\\text{for }i=1,\\dots,k \\qquad[\\text{automatic if }\\delta_i\\le 0,\\ \\text{by construction of }t^*\\text{ if }\\delta_i>0]\n\\]\n\\[\n\\sum_{i=1}^k \\alpha_i'=\\sum_{i=1}^k \\alpha_i - t^*\\sum_{i=1}^k \\delta_i =1;\n\\]\n\\[\n\\sum_{i=1}^k \\alpha_i' x_i = x - t^* \\sum_{i=1}^k \\delta_i x_i = x;\n\\]\nand there exists $i_0$ such that $\\alpha_{i_0}'=0$. \\qquad [by construction of $t^*$]\n\nIn other words, we have expressed $x$ as a convex combination of $k-1$ among the $x_i$'s; our claim is proved.\n\nNow, if $k-1=n+1$, the proof is finished. If not, we can apply the above construction to the convex combination $x=\\sum_{i=1}^{k-1}\\alpha_i' x_i$ and so on. The process can be continued until there remain only $n+1$ elements (which may be affinely independent)."
    },
    {
        "statement": "(W. Fenchel and L. Bunt) If $S\\subset\\mathbb{R}^n$ has no more than $n$ connected components (in particular, if $S$ is connected), then any $x\\in\\operatorname{co}S$ can be expressed as a convex combination of $n$ elements of $S$.",
        "title": "",
        "label": "thm:CAMA-chap3-1.3.7",
        "lean_tag": "no-lean-tag",
        "proof": ""
    },
    {
        "statement": "The closed convex hull \\(\\overline{\\operatorname{co}}S\\) of Definition 1.4.1 is the closure \\(\\operatorname{cl}(\\operatorname{co}S)\\) of the convex hull of \\(S\\).",
        "title": "",
        "label": "prop:CAMA-chap3-1.4.2",
        "lean_tag": "no-lean-tag",
        "proof": "Because \\(\\operatorname{cl}(\\operatorname{co}S)\\) is a closed convex set containing \\(S\\), it contains \\(\\overline{\\operatorname{co}}S\\) as well. On the other hand, take a closed convex set \\(C\\) containing \\(S\\); being convex, \\(C\\) contains \\(\\operatorname{co}S\\); being closed, it contains also the closure of \\(\\operatorname{co}S\\). Since \\(C\\) was arbitrary, we conclude \\(\\bigcap C \\supset\\operatorname{cl}\\operatorname{co}S\\)."
    },
    {
        "statement": "If $S$ is bounded [resp.\\ compact], then $\\operatorname{co}S$ is bounded [resp.\\ compact].",
        "title": "",
        "label": "thm:CAMA-chap3-1.4.3",
        "lean_tag": [
            "CAMA_chap_3_1_4_3"
        ],
        "lean_formalization": "lemma CAMA_chap_3_1_4_3 {n : ℕ}\n  (S : Set (EuclideanSpace ℝ (Fin n))) :\n  (Bornology.IsBounded S → Bornology.IsBounded (convexHull ℝ S)) ∧\n  (IsCompact S → IsCompact (convexHull ℝ S)) := by\n  sorry",
        "proof": "Let $x=\\sum_{i=1}^{n+1}\\alpha_i x_i\\in\\operatorname{co}S$. If $S$ is bounded, say by $M$, we can write\n\\[\n\\|x\\|\\le \\sum_{i=1}^{n+1}\\alpha_i\\|x_i\\|\\le M\\sum_{i=1}^{n+1}\\alpha_i = M.\n\\]\n\nNow take a sequence $\\{x^k\\}\\subset\\operatorname{co}S$. For each $k$ we can choose\n\\[\nx^k_1,\\dots,x^k_{n+1}\\in S\\qquad\\text{and}\\qquad \\alpha^k=(\\alpha^k_1,\\dots,\\alpha^k_{n+1})\\in\\Delta_{n+1}\n\\]\nsuch that $x^k=\\sum_{i=1}^{n+1}\\alpha^k_i x^k_i$. Note that $\\Delta_{n+1}$ is compact. If $S$ is compact, we can extract a subsequence as many times as necessary (not more than $n+2$ times) so that $\\{\\alpha^k\\}$ and each $\\{x^k_i\\}$ converge: we end up with an index set $K\\subset\\mathbb{N}$ such that, when $k\\to+\\infty$,\n\\[\n\\{x^k_i\\}_{k\\in K}\\to x_i\\in S\\qquad\\text{and}\\qquad \\{\\alpha^k\\}_{k\\in K}\\to \\alpha\\in\\Delta_{n+1}.\n\\]\n\nPassing to the limit for $k\\in K$, we see that $\\{x^k\\}_{k\\in K}$ converges to a point $x$, which can be expressed as a convex combination of points of $S$: $x\\in\\operatorname{co}S$, whose compactness is thus established."
    },
    {
        "statement": "Let $S$ be a nonempty compact set such that $0\\notin\\operatorname{co}S$. Then\n\\[\n\\overline{\\operatorname{cone}S}=\\mathbb{R}^{+}(\\operatorname{co}S)\\quad[=\\operatorname{cone}S].\n\\]",
        "title": "",
        "label": "prop:CAMA-chap3-1.4.7",
        "lean_tag": "no-lean-tag",
        "proof": "The set $C:=\\operatorname{co}S$ is compact and does not contain the origin; we prove that $\\mathbb{R}^{+}C$ is closed. Let $\\{t_kx_k\\}\\subset\\mathbb{R}^{+}C$ converge to $y$; extracting a subsequence if necessary, we may suppose $x_k\\to x\\in C$; note: $x\\neq 0$. We write\n\\[\nt_k\\frac{x_k}{\\|x_k\\|}\\longrightarrow\\frac{y}{\\|x\\|},\n\\]\nwhich implies $t_k\\to\\|y\\|/\\|x\\|=:t\\ge 0$. Then, $t_kx_k\\to tx=y$, which is thus in $\\mathbb{R}^{+}C$."
    },
    {
        "statement": "Let the two convex sets $C_1$ and $C_2$ satisfy $\\operatorname{ri}C_1\\cap\\operatorname{ri}C_2\\neq\\varnothing$. Then\n\\begin{align}\n\\operatorname{ri}(C_1\\cap C_2) &= \\operatorname{ri}C_1\\cap\\operatorname{ri}C_2 \\label{eq:2.1.1}\\\\\n\\operatorname{cl}(C_1\\cap C_2) &= \\operatorname{cl}C_1\\cap\\operatorname{cl}C_2 .\\label{eq:2.1.2}\n\\end{align}",
        "title": "",
        "label": "prop:CAMA-chap3-2.1.10",
        "lean_tag": [
            "CAMA_chap_3_2_1_10"
        ],
        "lean_formalization": "lemma CAMA_chap_3_2_1_10 {n : ℕ}\n  (C₁ : Set (EuclideanSpace ℝ (Fin n)))\n  (C₂ : Set (EuclideanSpace ℝ (Fin n)))\n  (hC₁ : Convex ℝ C₁)\n  (hC₂ : Convex ℝ C₂)\n  (hC : Set.Nonempty (intrinsicInterior ℝ C₁ ∩ intrinsicInterior ℝ C₂)) :\n  (intrinsicInterior ℝ (C₁ ∩ C₂) = intrinsicInterior ℝ C₁ ∩ intrinsicInterior ℝ C₂) ∧\n  (closure (C₁ ∩ C₂) = closure C₁ ∩ closure C₂):= by\n  sorry",
        "proof": "First we show that $\\operatorname{cl}C_1\\cap\\operatorname{cl}C_2 \\subset \\operatorname{cl}(C_1\\cap C_2)$ (the converse inclusion is always true). Given $x\\in \\operatorname{cl}C_1\\cap\\operatorname{cl}C_2$, we pick $x'$ in the nonempty $\\operatorname{ri}C_1\\cap\\operatorname{ri}C_2$. From Lemma 2.1.6 applied to $C_1$ and to $C_2$,\n\\[\n]x,x'[\\subset \\operatorname{ri}C_1\\cap\\operatorname{ri}C_2 .\n\\]\nTaking the closure of both sides, we conclude\n\\[\nx\\in \\operatorname{cl}(\\operatorname{ri}C_1\\cap\\operatorname{ri}C_2)\\subset \\operatorname{cl}(C_1\\cap C_2),\n\\]\nwhich proves \\eqref{eq:2.1.2} because $x$ was arbitrary; the above inclusion is actually an equality.\n\nNow, we have just seen that the two convex sets $\\operatorname{ri}C_1\\cap\\operatorname{ri}C_2$ and $C_1\\cap C_2$ have the same closure. According to Remark 2.1.9, they have the same relative interior:\n\\[\n\\operatorname{ri}(C_1\\cap C_2)=\\operatorname{ri}(\\operatorname{ri}C_1\\cap\\operatorname{ri}C_2)\\subset \\operatorname{ri}C_1\\cap\\operatorname{ri}C_2.\n\\]\n\nIt remains to prove the converse inclusion, so let $y\\in \\operatorname{ri}C_1\\cap\\operatorname{ri}C_2$. If we take $x'\\in C_1$ [resp.\\ $C_2$], the segment $[x',y]$ is in aff $C_1$ [resp.\\ aff $C_2$] and, by definition of the relative interior, this segment can be stretched beyond $y$ and yet stay in $C_1$ [resp.\\ $C_2$] (see Fig.\\ 2.1.3). Take in particular $x'\\in \\operatorname{ri}(C_1\\cap C_2)$, $x'\\neq y$ (if such an $x'$ does not exist, we are done). The above stretching singles out an $x\\in C_1\\cap C_2$ such that $y\\in ]x,x'[:$\n\\[\ny=\\alpha x+(1-\\alpha)x' \\quad\\text{for some }\\alpha\\in ]0,1[.\n\\]\nThen Lemma 2.1.6 applied to $C_1\\cap C_2$ tells us that $y\\in \\operatorname{ri}(C_1\\cap C_2)$."
    },
    {
        "statement": "Let $A:\\mathbb{R}^n\\to\\mathbb{R}^m$ be an affine mapping and $C$ a convex set of $\\mathbb{R}^n$. Then\n\\[\n\\operatorname{ri}[A(C)] = A(\\operatorname{ri} C).\n\\tag{2.1.3}\n\\]\nIf $D$ is a convex set of $\\mathbb{R}^m$ satisfying $A^{-1}(\\operatorname{ri}D)\\neq\\varnothing$, then\n\\[\n\\operatorname{ri}\\bigl[A^{-1}(D)\\bigr] = A^{-1}(\\operatorname{ri} D).\n\\tag{2.1.4}\n\\]",
        "title": "",
        "label": "prop:CAMA-chap3-2.1.12",
        "lean_tag": [
            "CAMA_chap_3_2_1_12"
        ],
        "lean_formalization": "lemma CAMA_chap_3_2_1_12 {n : ℕ} {m : ℕ}\n  (A : AffineMap ℝ (EuclideanSpace ℝ (Fin n)) (EuclideanSpace ℝ (Fin m)))\n  (C : Set (EuclideanSpace ℝ (Fin n)))\n  (D : Set (EuclideanSpace ℝ (Fin m)))\n  (hC : Convex ℝ C)\n  (hD₀ : Convex ℝ D)\n  (hD₁ : Set.Nonempty (Set.preimage A (intrinsicInterior ℝ D))) :\n  (intrinsicInterior ℝ (Set.image A C) = Set.image A (intrinsicInterior ℝ C)) ∧\n  (intrinsicInterior ℝ (Set.preimage A D) = Set.preimage A (intrinsicInterior ℝ D)) := by\n  sorry",
        "proof": "First, note that the continuity of $A$ implies $A(\\operatorname{cl}S)\\subset\\operatorname{cl}[A(S)]$ for any $S\\subset\\mathbb{R}^n$. Apply this result to $\\operatorname{ri}C$, whose closure is $\\operatorname{cl} C$ (Proposition 2.1.8), and use the monotonicity of the closure operation:\n\\[\nA(C)\\subset A(\\operatorname{cl}C)=A[\\operatorname{cl}(\\operatorname{ri}C)]\\subset\\operatorname{cl}[A(\\operatorname{ri}C)]\\subset\\operatorname{cl}[A(C)];\n\\]\nthe closed set $\\operatorname{cl}[A(\\operatorname{ri}C)]$ is therefore $\\operatorname{cl}[A(C)]$. Because $A(\\operatorname{ri}C)$ and $A(C)$ have the same closure, they have the same relative interior (Remark 2.1.9):\n\\[\n\\operatorname{ri}\\,A(C)=\\operatorname{ri}[A(\\operatorname{ri}C)]\\subset A(\\operatorname{ri}C).\n\\]\n\nTo prove the converse inclusion, let $w=A(y)\\in A(\\operatorname{ri}C)$, with $y\\in\\operatorname{ri}C$. We choose $z'=A(x')\\in\\operatorname{ri}A(C)$, with $x'\\in C$ (we assume $z'\\neq w$, hence $x'\\neq y$).\n\nUsing in $C$ the same stretching mechanism as in Fig.\\ 2.1.3, we single out $x\\in C$\nsuch that $y\\in ]x,x'[$, to which corresponds $z=A(x)\\in A(C)$. By affinity, $A(y)\\in\n]A(x),A(x')[=]z,z'[$. Thus, $z$ and $z'$ fulfill the conditions of Lemma 2.1.6 applied\nto the convex set $A(C)$: $w\\in\\operatorname{ri}A(C)$, and (2.1.3) is proved.\n\nThe proof of (2.1.4) uses the same technique."
    },
    {
        "statement": "If $C\\neq\\varnothing$, then $\\operatorname{ri}C\\neq\\varnothing$.  In fact, $\\dim(\\operatorname{ri}C)=\\dim C$.",
        "title": "",
        "label": "thm:CAMA-chap3-2.1.3",
        "lean_tag": [
            "CAMA_chap_3_2_1_3"
        ],
        "lean_formalization": "lemma CAMA_chap_3_2_1_3 {n : ℕ}\n  (C : Set (EuclideanSpace ℝ (Fin n)))\n  (hC₀ : Nonempty C)\n  (hC₁ : Convex ℝ C) :\n  (Set.Nonempty (intrinsicInterior ℝ C)) ∧\n  (Module.finrank ℝ (affineSpan ℝ C).direction) =\n  (Module.finrank ℝ (affineSpan ℝ (intrinsicInterior ℝ C)).direction) := by\n  sorry",
        "proof": "Let $k:=1+\\dim C$.  Since $\\operatorname{aff}C$ has dimension $k-1$, $C$ contains $k$ elements affinely independent $x_1,\\dots,x_k$.  Call $\\Delta:=\\operatorname{co}\\{x_1,\\dots,x_k\\}$ the simplex that they generate; see fig.\\ 2.1.1; $\\operatorname{aff}\\Delta=\\operatorname{aff}C$ because $\\Delta\\subset C$ and $\\dim\\Delta=k-1$.  The proof will be finished if we show that $\\Delta$ has nonempty relative interior.\n\nTake $\\bar{x}:=1/k\\sum_{i=1}^k x_i$ (the ``center'' of $\\Delta$) and describe $\\operatorname{aff}\\Delta$ by points of the form\n\\[\n\\bar{x}+y=\\bar{x}+\\sum_{i=1}^k \\alpha_i(y)x_i\n=\\sum_{i=1}^k\\Big[\\tfrac{1}{k}+\\alpha_i(y)\\Big]x_i,\n\\]\n\nwhere $\\alpha(y)=(\\alpha_1(y),\\dots,\\alpha_k(y))\\in\\mathbb R^k$ solves\n\\[\n\\sum_{i=1}^k \\alpha_i x_i = y,\\qquad \\sum_{i=1}^k \\alpha_i = 0.\n\\]\n\nBecause this system has a unique solution, the mapping $y\\mapsto\\alpha(y)$ is (linear and) continuous: we can find $\\delta>0$ such that $\\|y\\|\\le\\delta$ implies\n\\[\n|\\alpha_i(y)|\\le 1/k\\quad\\text{for }i=1,\\dots,k,\n\\]\nhence $\\bar x+y\\in\\Delta$.\n\nIn other words, $\\bar x\\in\\operatorname{ri}\\Delta\\subset\\operatorname{ri}C$.\n\nIt follows in particular $\\dim\\operatorname{ri}C=\\dim\\Delta=\\dim C$."
    },
    {
        "statement": "The three convex sets $\\operatorname{ri}C$, $C$ and $\\operatorname{cl}C$ have the same affine hull (and hence the same dimension), the same relative interior and the same closure (and hence the same relative boundary).",
        "title": "",
        "label": "prop:CAMA-chap3-2.1.8",
        "lean_tag": [
            "CAMA_chap_3_2_1_8"
        ],
        "lean_formalization": "lemma CAMA_chap_3_2_1_8 {n : ℕ}\n  (C : Set (EuclideanSpace ℝ (Fin n)))\n  (hC_nonempty : Set.Nonempty C)\n  (hC_convex : Convex ℝ C) :\n  (closure (intrinsicInterior ℝ C) = closure C) ∧\n  (intrinsicInterior ℝ (closure C) = intrinsicInterior ℝ C) := by\n  sorry",
        "proof": "The case of the affine hull was already seen in Theorem 2.1.3. For the others, the key result is Lemma 2.1.6 (as well as for most other properties involving closures and relative interiors). We illustrate it by restricting our proof to one of the properties, say: $\\operatorname{ri}C$ and $C$ have the same closure.\n\nThus, we have to prove that $\\operatorname{cl}C\\subset\\operatorname{cl}(\\operatorname{ri}C)$. Let $x\\in\\operatorname{cl}C$ and take $x'\\in\\operatorname{ri}C$ (it is possible by virtue of Theorem 2.1.3). Because $[x,x']\\subset\\operatorname{ri}C$ (Lemma 2.1.6), we do have that $x$ is a limit of points in $\\operatorname{ri}C$ (and even a \"radial\" limit); hence $x$ is in the closure of $\\operatorname{ri}C$."
    },
    {
        "statement": "The closed convex cone $C_{\\infty}(x)$ does not depend on $x\\in C$.",
        "title": "",
        "label": "prop:CAMA-chap3-2.2.1",
        "lean_tag": [
            "CAMA_chap_3_2_2_1"
        ],
        "lean_formalization": "lemma CAMA_chap_3_2_2_1 {n : ℕ}\n  (C : Set (EuclideanSpace ℝ (Fin n)))\n  (x₁ x₂ : EuclideanSpace ℝ (Fin n))\n  (hC₀ : IsClosed C)\n  (hC₁ : Convex ℝ C)\n  (hx₁ : x₁ ∈ C)\n  (hx₂ : x₂ ∈ C) :\n  (AsymptoticCone C x₁) = (AsymptoticCone C x₂) := by\n  sorry",
        "proof": "See Theorem I.2.3.1 and the pantographic Figure I.2.3.1. Take two different points $x_{1}$ and $x_{2}$ in $C$; it suffices to prove one inclusion, say $C_{\\infty}(x_{1})\\subset C_{\\infty}(x_{2})$. Let $d\\in C_{\\infty}(x_{1})$ and $t>0$, we have to prove $x_{2}+td\\in C$. With $\\varepsilon\\in ]0,1[$, consider the point\n\\[\n\\bar{x}_{\\varepsilon}:=x_{1}+td+(1-\\varepsilon)(x_{2}-x_{1}).\n\\]\n\nWriting it as\n\\[\n\\bar{x}_\\varepsilon = \\varepsilon\\bigl(x_1 + \\tfrac{t}{\\varepsilon} d\\bigr) + (1-\\varepsilon)x_2,\n\\]\nwe see that \\(\\bar{x}_\\varepsilon\\in C\\) (use the definitions of \\(C_\\infty(x_1)\\) and of a convex set). On the other hand,\n\\[\nx_2 + t d = \\lim_{\\varepsilon\\downarrow 0}\\bar{x}_\\varepsilon \\in \\overline{C}.\n\\]"
    },
    {
        "statement": "A closed convex set $C$ is compact if and only if $C_\\infty=\\{0\\}$.",
        "title": "",
        "label": "prop:CAMA-chap3-2.2.3",
        "lean_tag": [
            "CAMA_chap_3_2_2_3"
        ],
        "lean_formalization": "lemma CAMA_chap_3_2_2_3 {n : ℕ}\n  (C : Set (EuclideanSpace ℝ (Fin n)))\n  (hC₀ : IsClosed C)\n  (hC₁ : Convex ℝ C)\n  (hC₂ : Set.Nonempty C) :\n  (IsCompact C) ↔ ∀ x ∈ C, (AsymptoticCone C x) = {0} := by\n  sorry",
        "proof": "If $C$ is bounded, it is clear that $C_\\infty$ cannot contain any nonzero direction.\n\nConversely, let $\\{x_k\\}\\subset C$ be such that $\\|x_k\\|\\to+\\infty$ (we assume $x_k\\neq0$). The sequence $\\{d_k:=x_k/\\|x_k\\|\\}$ is bounded, extract a convergent subsequence: $d=\\lim_{k\\in K}d_k$ with $K\\subset\\mathbb N$ ($\\|d\\|=1$). Now, given $x\\in C$ and $t>0$, take $k$ so large that $\\|x_k\\|\\ge t$. Then, we see that\n\\[\nx+td=\\lim_{k\\in K}\\Big[(1-\\tfrac{t}{\\|x_k\\|})x+\\tfrac{t}{\\|x_k\\|}x_k\\Big]\n\\]\nis in the closed convex set $C$, hence $d\\in C_\\infty$."
    },
    {
        "statement": "\\begin{itemize}\n\\item If $\\{C_j\\}_{j\\in J}$ is a family of closed convex sets having a point in common, then\n\\[\n\\left(\\bigcap_{j\\in J} C_j\\right)_\\infty = \\bigcap_{j\\in J}(C_j)_\\infty .\n\\]\n\n\\item If, for $j=1,\\dots,m$, $C_j$ are closed convex sets in $\\mathbb{R}^{n_j}$, then\n\\[\n(C_1\\times\\cdots\\times C_m)_\\infty = (C_1)_\\infty\\times\\cdots\\times (C_m)_\\infty .\n\\]\n\n\\item Let $A:\\mathbb{R}^n\\to\\mathbb{R}^m$ be an affine mapping. If $C$ is closed convex in $\\mathbb{R}^n$ and $A(C)$ is closed, then\n\\[\nA(C_\\infty)\\subset [A(C)]_\\infty .\n\\]\n\n\\item If $D$ is closed convex in $\\mathbb{R}^m$ with nonempty inverse image, then\n\\[\n\\bigl[\\;A^{-1}(D)\\;\\bigr]_\\infty = A^{-1}(D_\\infty).\n\\]\n\\end{itemize}",
        "title": "",
        "label": "prop:CAMA-chap3-2.2.5",
        "lean_tag": "no-lean-tag",
        "proof": ""
    },
    {
        "statement": "If $C$ is compact, then $\\operatorname{ext}C\\neq\\varnothing$.",
        "title": "",
        "label": "prop:CAMA-chap3-2.3.3",
        "lean_tag": [
            "CAMA_chap_3_2_3_3"
        ],
        "lean_formalization": "lemma CAMA_chap_3_2_3_3 {n : ℕ}\n  (C : Set (EuclideanSpace ℝ (Fin n)))\n  (hC₀ : Set.Nonempty C)\n  (hC₁ : Convex ℝ C)\n  (hC₂ : IsCompact C) :\n  Set.Nonempty (Set.extremePoints ℝ C) := by\n  sorry",
        "proof": "Because $C$ is compact, there is $\\bar x\\in C$ maximizing the continuous function $x\\mapsto\\|x\\|^2$.  We claim that $\\bar x$ is extremal.  In fact, suppose that there are $x_1$ and $x_2$ in $C$ with $\\bar x=\\tfrac12(x_1+x_2)$.  Then, with $x_1\\neq x_2$ and using (2.3.1), we obtain the contradiction\n\\[\n\\|\\bar x\\|^2=\\Big\\|\\tfrac12(x_1+x_2)\\Big\\|^2<\\tfrac12\\big(\\|x_1\\|^2+\\|x_2\\|^2\\big)\\le\\tfrac12\\big(\\|\\bar x\\|^2+\\|\\bar x\\|^2\\big)=\\|\\bar x\\|^2.\n\\]"
    },
    {
        "statement": "(H. Minkowski) Let $C$ be compact, convex in $\\mathbb{R}^n$. Then $C$ is the convex hull of its extreme points: $C=\\operatorname{co}(\\operatorname{ext}C)$.",
        "title": "",
        "label": "thm:CAMA-chap3-2.3.4",
        "lean_tag": [
            "CAMA_chap_3_2_3_4"
        ],
        "lean_formalization": "lemma CAMA_chap_3_2_3_4 {n : ℕ}\n  (C : Set (EuclideanSpace ℝ (Fin n)))\n  (hC₀ : Set.Nonempty C)\n  (hC₁ : Convex ℝ C)\n  (hC₂ : IsCompact C) :\n  C = convexHull ℝ (Set.extremePoints ℝ C) := by\n  sorry",
        "proof": ""
    },
    {
        "statement": "Let $F$ be a face of $C$. Then any extreme point of $F$ is an extreme point of $C$.",
        "title": "",
        "label": "prop:CAMA-chap3-2.3.7",
        "lean_tag": [
            "CAMA_chap_3_2_3_7"
        ],
        "lean_formalization": "lemma CAMA_chap_3_2_3_7 {n : ℕ}\n  (C : Set (EuclideanSpace ℝ (Fin n)))\n  (F : Set (EuclideanSpace ℝ (Fin n)))\n  (hF : Face C F)\n  (hC₀ : Set.Nonempty C)\n  (hC₁ : Convex ℝ C) :\n  ∀ (x : EuclideanSpace ℝ (Fin n)) (_ : x ∈ (Set.extremePoints ℝ F)),\n  x ∈ (Set.extremePoints ℝ C) := by\n  sorry",
        "proof": "Take \\(x\\in F\\subset C\\) and assume that \\(x\\) is not an extreme point of \\(C\\): there are different \\(x_1,x_2\\) in \\(C\\) and \\(\\alpha\\in\\,]0,1[\\) such that \\(x=\\alpha x_1+(1-\\alpha)x_2\\in F\\). From the very definition (2.3.2) of a face, this implies that \\(x_1\\) and \\(x_2\\) are in \\(F\\): \\(x\\) cannot be an extreme point of \\(F\\).\n\\(\\square\\)"
    },
    {
        "statement": "An exposed face is a face.",
        "title": "",
        "label": "prop:CAMA-chap3-2.4.3",
        "lean_tag": [
            "CAMA_chap_3_2_4_3"
        ],
        "lean_formalization": "lemma CAMA_chap_3_2_4_3 {n : ℕ}\n  (C : Set (EuclideanSpace ℝ (Fin n)))\n  (F : Set (EuclideanSpace ℝ (Fin n)))\n  (hC_nonempty : Set.Nonempty C)\n  (hC_convex : Convex ℝ C)\n  (hF : IsExposedFace C F) :\n  Face C F := by\n  sorry",
        "proof": "Let $F$ be an exposed face, with its associated support $H_{s,r}$. Take $x_1$ and $x_2$ in $C$:\n\\begin{equation}\n\\langle s,x_i\\rangle \\le r \\quad\\text{for } i=1,2;\n\\label{2.4.2}\n\\end{equation}\ntake also $\\alpha\\in]0,1[$ such that $\\alpha x_1+(1-\\alpha)x_2\\in F\\subset H_{s,r}$:\n\\[\n\\langle s,\\alpha x_1+(1-\\alpha)x_2\\rangle = r.\n\\]\nSuppose that one of the relations \\eqref{2.4.2} holds as strict inequality. By convex combination, we obtain $(0<\\alpha<1!)$\n\\[\n\\langle s,\\alpha x_1+(1-\\alpha)x_2\\rangle < r,\n\\]\na contradiction."
    },
    {
        "statement": "Let $C$ be convex and compact. For $s\\in\\mathbb{R}^n$, there holds\n\\[\n\\max_{x\\in C}\\langle s,x\\rangle=\\max_{x\\in\\operatorname{ext} C}\\langle s,x\\rangle.\n\\]",
        "title": "",
        "label": "prop:CAMA-chap3-2.4.6",
        "lean_tag": [
            "CAMA_chap_3_2_4_6"
        ],
        "lean_formalization": "lemma CAMA_chap_3_2_4_6 {n : ℕ}\n  (C : Set (EuclideanSpace ℝ (Fin n)))\n  (s : EuclideanSpace ℝ (Fin n))\n  (hC₀ : IsCompact C)\n  (hC₁ : Convex ℝ C)\n  (hC₂ : Set.Nonempty C) :\n  (sSup (Set.image (fun x ↦ inner ℝ s x) C) =\n  sSup (Set.image (fun x ↦ inner ℝ s x) (Set.extremePoints ℝ C))) ∧\n  (Argmax (fun x ↦ inner ℝ s x) C =\n  convexHull ℝ (Argmax (fun x ↦ inner ℝ s x) (Set.extremePoints ℝ C))) := by\n  sorry",
        "proof": ""
    },
    {
        "statement": "A point $y_x\\in C$ is the projection $p_C(x)$ if and only if\n\\[\n\\langle x-y_x,\\;y-y_x\\rangle\\le 0\\qquad\\text{for all }y\\in C .\n\\tag{3.1.3}\n\\]",
        "title": "",
        "label": "thm:CAMA-chap3-3.1.1",
        "lean_tag": [
            "CAMA_chap_3_3_1_1"
        ],
        "lean_formalization": "lemma CAMA_chap_3_3_1_1 {n : ℕ}\n  (C : Set (EuclideanSpace ℝ (Fin n)))\n  (x y : EuclideanSpace ℝ (Fin n))\n  (hC₀ : IsClosed C)\n  (hC₁ : Convex ℝ C)\n  (hC₂ : Set.Nonempty C)\n  (hy : y ∈ C) :\n  y = pC x C hC₀ hC₁ hC₂ ↔ ∀ z ∈ C, inner ℝ (x - y) (z - y) ≤ 0 := by\n  sorry",
        "proof": "Call $y_x$ the solution of (3.1.1); take $y$ arbitrary in $C$, so that $y_x+\\alpha(y-y_x)\\in C$ for any $\\alpha\\in ]0,1[$. Then we can write with the notation (3.1.2)\n\\[\nf_x(y_x)\\le f_x\\big(y_x+\\alpha(y-y_x)\\big)=\\tfrac12\\|y_x-x+\\alpha(y-y_x)\\|^2.\n\\]\nDeveloping the square, we obtain after simplification\n\\[\n0\\le \\alpha\\langle y_x-x,\\;y-y_x\\rangle+\\tfrac12\\alpha^2\\|y-y_x\\|^2.\n\\]\nDivide by $\\alpha\\ (>0)$ and let $\\alpha\\downarrow0$ to obtain (3.1.3).\n\nConversely, suppose that $y_x\\in C$ satisfies (3.1.3). If $y_x=x$, then $y_x$ certainly solves (3.1.1). If not, write for arbitrary $y\\in C$:\n\\[\n0\\ge \\langle x-y_x,\\;y-y_x\\rangle=(x-y_x,\\;y-x+x-y_x)\n\\]\n\\[\n=\\|x-y_x\\|^2+\\langle x-y_x,\\;y-x\\rangle\\ge\\|x-y_x\\|^2-\\|x-y\\|\\;\\|x-y_x\\|,\n\\]\nwhere the Cauchy-Schwarz inequality is used. Divide by $\\|x-y_x\\|>0$ to see that $y_x$ solves (3.1.1)."
    },
    {
        "statement": "For all $(x_1,x_2)\\in\\mathbb{R}^n\\times\\mathbb{R}^n$, there holds\n\\[\n\\|p_C(x_1)-p_C(x_2)\\|^2 \\le \\langle p_C(x_1)-p_C(x_2),\\,x_1-x_2\\rangle.\n\\]",
        "title": "",
        "label": "prop:CAMA-chap3-3.1.3",
        "lean_tag": [
            "CAMA_chap_3_3_1_3"
        ],
        "lean_formalization": "lemma CAMA_chap_3_3_1_3 {n : ℕ}\n  (C : Set (EuclideanSpace ℝ (Fin n)))\n  (x₁ x₂ : EuclideanSpace ℝ (Fin n))\n  (hC₀ : IsClosed C)\n  (hC₁ : Convex ℝ C)\n  (hC₂ : Set.Nonempty C) :\n  ‖(pC x₁ C hC₀ hC₁ hC₂) - (pC x₂ C hC₀ hC₁ hC₂)‖ ^ 2\n  ≤ inner ℝ ((pC x₁ C hC₀ hC₁ hC₂) - (pC x₂ C hC₀ hC₁ hC₂)) (x₁ - x₂) := by\n  sorry",
        "proof": "Write (3.1.3) with $x=x_1$, $y=p_C(x_2)\\in C$:\n\\[\n\\langle p_C(x_2)-p_C(x_1),\\,x_1-p_C(x_1)\\rangle \\le 0;\n\\]\nlikewise,\n\\[\n\\langle p_C(x_1)-p_C(x_2),\\,x_2-p_C(x_2)\\rangle \\le 0,\n\\]\nand conclude by addition\n\\[\n\\langle p_C(x_1)-p_C(x_2),\\,x_2-x_1 + p_C(x_1)-p_C(x_2)\\rangle \\le 0.\n\\]"
    },
    {
        "statement": "Let $K$ be a closed convex cone.  Then $y_x = p_K(x)$ if and only if\n\\[\ny_x \\in K,\\qquad x-y_x\\in K^\\circ,\\qquad \\langle x-y_x,y_x\\rangle=0.\n\\tag{3.2.1}\n\\]",
        "title": "",
        "label": "prop:CAMA-chap3-3.2.3",
        "lean_tag": [
            "CAMA_chap_3_3_2_3"
        ],
        "lean_formalization": "lemma CAMA_chap_3_3_2_3 {n : ℕ}\n  (K : Set (EuclideanSpace ℝ (Fin n)))\n  (x y : EuclideanSpace ℝ (Fin n))\n  (hK₀ : IsConvexCone K)\n  (hK₁ : IsClosed K)\n  (hK₂ : Set.Nonempty K) :\n  y = pC x K hK₁ (convexCone_isConvex hK₀) hK₂ ↔\n  (y ∈ K) ∧ (x - y ∈ PolarCone K) ∧ (inner ℝ (x - y) y = 0) := by\n  sorry",
        "proof": "We know from Theorem 3.1.1 that $y_x = p_K(x)$ satisfies\n\\[\n\\langle x-y_x,y-y_x\\rangle\\le 0\\qquad\\text{for all }y\\in K.\n\\tag{3.2.2}\n\\]\nTaking $y=\\alpha y_x$, with arbitrary $\\alpha\\ge 0$, this inequality implies\n\\[\n(\\alpha-1)\\langle x-y_x,y_x\\rangle\\le 0\\qquad\\text{for all }\\alpha\\ge 0.\n\\]\nSince $\\alpha-1$ can have either sign, this implies $\\langle x-y_x,y_x\\rangle=0$ and (3.2.2) becomes\n\\[\n\\langle y,x-y_x\\rangle\\le 0\\quad\\text{for all }y\\in K,\\qquad\\text{i.e.}\\quad x-y_x\\in K^\\circ.\n\\]\n\nConversely, let $y_x$ satisfy (3.2.1).  For arbitrary $y\\in K$, use the notation (3.1.2):\n\\[\nf_x(y)=\\tfrac12\\|x-y_x+y_x-y\\|^2\\ge f_x(y_x)+\\langle x-y_x,y_x-y\\rangle;\n\\]\nbut (3.2.1) shows that\n\\[\n\\langle x-y_x,y_x-y\\rangle=-\\langle x-y_x,y\\rangle\\ge 0,\n\\]\nhence $f_x(y)\\ge f_x(y_x)$: $y_x$ solves (3.1.1)."
    },
    {
        "statement": "Let $K$ be a closed convex cone. For the three elements $x,x_1$ and $x_2$ in $\\mathbb{R}^n$, the properties below are equivalent:\n\\begin{enumerate}\n\\item[(i)] $x=x_1+x_2$ with $x_1\\in K$, $x_2\\in K^\\circ$ and $\\langle x_1,x_2\\rangle=0$;\n\\item[(ii)] $x_1=\\mathrm{p}_K(x)$ and $x_2=\\mathrm{p}_{K^\\circ}(x)$.\n\\end{enumerate}",
        "title": "J.-J. Moreau",
        "label": "thm:CAMA-chap3-3.2.5",
        "lean_tag": [
            "CAMA_chap_3_3_2_5"
        ],
        "lean_formalization": "lemma CAMA_chap_3_3_2_5 {n : ℕ}\n  (K : Set (EuclideanSpace ℝ (Fin n)))\n  (x x₁ x₂ : EuclideanSpace ℝ (Fin n))\n  (hK₀ : IsConvexCone K)\n  (hK₁ : IsClosed K)\n  (hK₂ : Set.Nonempty K) :\n  let K' := (PolarCone K)\n  x = x₁ + x₂ ∧ x₁ ∈ K ∧ x₂ ∈ K' ∧ inner ℝ x₁ x₂ = 0 ↔\n  x₁ = pC x K hK₁ (convexCone_isConvex hK₀) hK₂ ∧\n  x₂ = pC x K' polarCone_isClosed polarCone_isConvex polarCone_isNonempty := by\n  sorry",
        "proof": "Straightforward, from (3.2.3) and the characterization (3.2.1) of $x_1=\\mathrm{p}_K(x)$."
    },
    {
        "statement": "Let \\(C\\subset\\mathbb{R}^n\\) be nonempty closed convex, and let \\(x\\notin C\\). Then there exists \\(s\\in\\mathbb{R}^n\\) such that\n\\[\n\\langle s,x\\rangle > \\sup_{y\\in C}\\langle s,y\\rangle .\n\\tag{4.1.1}\n\\]",
        "title": "",
        "label": "thm:CAMA-chap3-4.1.1",
        "lean_tag": [
            "CAMA_chap_3_4_1_1"
        ],
        "lean_formalization": "lemma CAMA_chap_3_4_1_1 {n : ℕ}\n  (C : Set (EuclideanSpace ℝ (Fin n)))\n  (x : EuclideanSpace ℝ (Fin n))\n  (hC_nonempty : Set.Nonempty C)\n  (hC_closed : IsClosed C)\n  (hC_convex : Convex ℝ C)\n  (hx : x ∉ C) :\n  ∃ (s : EuclideanSpace ℝ (Fin n)), (s ≠ 0) ∧ (inner ℝ s x > ⨆ y ∈ C, inner ℝ s y) := by\n  sorry",
        "proof": "Set $s := x - p_C(x) \\neq 0$. We write (3.1.3) as\n\\[\n0 \\ge (s,y-x+s) = (s,y) - (s,x) + \\|s\\|^2.\n\\]\nThus we have\n\\[\n(s,x) - \\|s\\|^2 \\ge (s,y)\\qquad\\text{for all }y\\in C,\n\\]\nand our $s$ is a convenient answer for (4.1.1)."
    },
    {
        "statement": "Let $C_1,\\,C_2$ be two nonempty closed convex sets with $C_1\\cap C_2=\\varnothing$. If $C_2$ is bounded, there exists $s\\in\\mathbb{R}^n$ such that\n\\[\n\\sup_{y\\in C_1}\\langle s,y\\rangle \\;<\\; \\min_{y\\in C_2}\\langle s,y\\rangle .\n\\tag{4.1.2}\n\\]",
        "title": "Strict Separation of Convex Sets",
        "label": "cor:CAMA-chap3-4.1.3",
        "lean_tag": [
            "CAMA_chap_3_4_1_3"
        ],
        "lean_formalization": "lemma CAMA_chap_3_4_1_3 {n : ℕ}\n  (C₁ C₂ : Set (EuclideanSpace ℝ (Fin n)))\n  (hC₁ : Set.Nonempty C₁)\n  (hC₁' : IsClosed C₁)\n  (hC₁'' : Convex ℝ C₁)\n  (hC₂ : Set.Nonempty C₂)\n  (hC₂' : IsClosed C₂)\n  (hC₂'' : Convex ℝ C₂)\n  (hI : (C₁ ∩ C₂) = ∅)\n  (h_bounded : Bornology.IsBounded C₂) :\n  ∃ (s : EuclideanSpace ℝ (Fin n)),\n  ⨆ y ∈ C₁, inner ℝ s y < ⨅ y ∈ C₂, inner ℝ s y := by\n  sorry",
        "proof": "The set $C_1-C_2$ is convex (Proposition 1.2.4) and closed (because $C_2$ is compact). To say that $C_1$ and $C_2$ are disjoint is to say that $0\\notin C_1-C_2$, so we have by Theorem 4.1.1 an $s\\in\\mathbb{R}^n$ separating $\\{0\\}$ from $C_1-C_2$:\n\\[\n\\sup\\{\\langle s,y\\rangle : y\\in C_1-C_2\\}<\\langle s,0\\rangle=0.\n\\]\nThis means:\n\\[\n0 \\;>\\; \\sup_{y_1\\in C_1}\\langle s,y_1\\rangle+\\sup_{y_2\\in C_2}\\langle s,-y_2\\rangle\n=\\sup_{y_1\\in C_1}\\langle s,y_1\\rangle-\\inf_{y_2\\in C_2}\\langle s,y_2\\rangle.\n\\]\nBecause $C_2$ is bounded, the last infimum (is a min and) is finite and can be moved to the left-hand side."
    },
    {
        "statement": "If the two nonempty convex sets $C_1$ and $C_2$ satisfy $\\operatorname{ri}C_1\\cap(\\operatorname{ri}C_2)=\\varnothing$, they can be properly separated.",
        "title": "Proper Separation of Convex Sets",
        "label": "thm:CAMA-chap3-4.1.4",
        "lean_tag": [
            "CAMA_chap_3_4_1_4"
        ],
        "lean_formalization": "lemma CAMA_chap_3_4_1_4 {n : ℕ}\n  (C₁ C₂ : Set (EuclideanSpace ℝ (Fin n)))\n  (hC₁ : Set.Nonempty C₁)\n  (hC₁' : Convex ℝ C₁)\n  (hC₂ : Set.Nonempty C₂)\n  (hC₂' : Convex ℝ C₂)\n  (hI : (intrinsicInterior ℝ C₁ ∩ intrinsicInterior ℝ C₂) = ∅) :\n  ∃ (s : EuclideanSpace ℝ (Fin n)),\n  ⨆ y₁ ∈ C₁, inner ℝ s y₁ ≤ ⨅ y₂ ∈ C₂, inner ℝ s y₂ ∧\n  ⨅ y₁ ∈ C₁, inner ℝ s y₁ < ⨆ y₂ ∈ C₂, inner ℝ s y₂ := by\n  sorry",
        "proof": ""
    },
    {
        "statement": "Let $x\\in\\partial C$, where $C\\neq\\varnothing$ is convex in $\\mathbb{R}^n$ (naturally $C\\neq\\mathbb{R}^n$). There exists a hyperplane supporting $C$ at $x$.",
        "title": "",
        "label": "lem:CAMA-chap3-4.2.1",
        "lean_tag": [
            "CAMA_chap_3_4_2_1"
        ],
        "lean_formalization": "lemma CAMA_chap_3_4_2_1 {n : ℕ}\n  (C : Set (EuclideanSpace ℝ (Fin n)))\n  (x : EuclideanSpace ℝ (Fin n))\n  (hC_convex : Convex ℝ C)\n  (hC_nonempty : Set.Nonempty C)\n  (hC_closed : IsClosed C)\n  (hx : x ∈ frontier C) :\n  ∃ (r : ℝ) (s : EuclideanSpace ℝ (Fin n)), SupportingHyperplaneAt s x r C := by\n  sorry",
        "proof": "Because $C$, $\\operatorname{cl}C$ and their complements have the same boundary (remember Remark 2.1.9), a sequence $\\{x_k\\}$ can be found such that\n\\[\nx_k\\notin C\\quad\\text{for }k=1,2,\\dots\\quad\\text{and}\\quad\n\\lim_{k\\to+\\infty}x_k=x.\n\\]\n\nFor each $k$ we have by Theorem 4.1.1 some $s_k$ with $\\|s_k\\|=1$ such that\n\\[\n\\langle s_k,\\; x_k-y\\rangle>0\\quad\\text{for all }y\\in C\\subset\\operatorname{cl}C .\n\\]\nExtract a subsequence if necessary so that $s_k\\to s$ (note: $s\\neq 0$) and pass to the limit to obtain\n\\[\n\\langle s,\\; x-y\\rangle\\ge0\\quad\\text{for all }y\\in C,\n\\]\nwhich is the required result $\\langle s,x\\rangle=r\\ge\\langle s,y\\rangle$ for all $y\\in C$."
    },
    {
        "statement": "Let $S\\subset\\mathbb{R}^n$ and $C:=\\operatorname{co}S$. Any $x\\in C\\cap\\operatorname{bd}C$ can be represented as a convex combination of $n$ elements of $S$.",
        "title": "",
        "label": "prop:CAMA-chap3-4.2.3",
        "lean_tag": [
            "CAMA_chap_3_4_2_3"
        ],
        "lean_formalization": "lemma CAMA_chap_3_4_2_3 {n : ℕ}\n  (S C : Set (EuclideanSpace ℝ (Fin n)))\n  (hC : C = convexHull ℝ S) :\n  ∀ x ∈ C ∩ (frontier C),\n  ∃ (v : Fin n → EuclideanSpace ℝ (Fin n)) (a : EuclideanSpace ℝ (Fin n)),\n  (∀ i, v i ∈ S) ∧ (a ∈ Δκ n) ∧ (x = ∑ i, a i • v i) := by\n  sorry",
        "proof": "Because $x\\in\\operatorname{bd}C$, there is a hyperplane $H_{s,r}$ supporting $C$ at $x$: for some $s\\neq 0$ and $r\\in\\mathbb{R}$,\n\\[\n\\langle s,x\\rangle - r = 0\\qquad\\text{and}\\qquad \\langle s,y\\rangle - r \\le 0\\ \\text{ for all } y\\in C.\n\\tag{4.2.1}\n\\]\nOn the other hand, Carathéodory's Theorem 1.3.6 implies the existence of points $x_1,\\dots,x_{n+1}$ in $S$ and convex multipliers $\\alpha_1,\\dots,\\alpha_{n+1}$ such that $x=\\sum_{i=1}^{n+1}\\alpha_i x_i$; and each $\\alpha_i$ can be assumed positive (otherwise the proof is finished).\n\nSetting successively $y=x_i$ in (4.2.1), we obtain by convex combination\n\\[\n0=\\langle s,x\\rangle - r = \\sum_{i=1}^{n+1}\\alpha_i\\bigl(\\langle s,x_i\\rangle - r\\bigr)\\le 0,\n\\]\nso each $\\langle s,x_i\\rangle - r$ is actually $0$. Each $x_i$ is therefore not only in $S$, but also in $H_{s,r}$, a set whose dimension is $n-1$. It follows that our starting $x$, which is in $\\operatorname{co}\\{x_1,\\dots,x_{n+1}\\}$, can be described as the convex hull of only $n$ among these $x_i$'s."
    },
    {
        "statement": "Let $\\varnothing\\neq C\\subseteq\\mathbb{R}^n$ be convex. The set $C^*$ defined above is the closure of $C$.",
        "title": "",
        "label": "thm:CAMA-chap3-4.2.4",
        "lean_tag": [
            "CAMA_chap_3_4_2_4"
        ],
        "lean_formalization": "lemma CAMA_chap_3_4_2_4 {n : ℕ}\n  (C : Set (EuclideanSpace ℝ (Fin n)))\n  (hC₀ : Convex ℝ C)\n  (hC₁ : Set.Nonempty C)\n  (hC₂ : C ⊂ Set.univ) :\n  closure C = ⋂ v ∈ (I_C C), HyperplaneHalfspace v.1 v.2 := by\n  sorry",
        "proof": "By construction, $C^*\\supseteq\\operatorname{cl}C$. Conversely, take $x\\notin\\operatorname{cl}C$; we can separate $x$ and $\\operatorname{cl}C$: there exists $s_0\\neq 0$ such that\n\\[\n\\langle s_0,x\\rangle>\\sup_{y\\in C}\\langle s_0,y\\rangle=:\\,r_0.\n\\]\nThen $(s_0,r_0)\\in\\Sigma_C$; but $x\\notin H_{s_0,r_0}$, hence $x\\notin C^*$."
    },
    {
        "statement": "The data $(s_j,r_j)\\in\\mathbb{R}^n\\times\\mathbb{R}$ for $j$ in an arbitrary index set $J$ is equivalent to the data of a closed convex set $C$ via the relation\n\\[\nC=\\bigcap_{j\\in J}\\{x\\in\\mathbb{R}^n:\\langle s_j,x\\rangle\\le r_j\\}.\n\\]",
        "title": "",
        "label": "cor:CAMA-chap3-4.2.5",
        "lean_tag": "no-lean-tag",
        "proof": "If $C$ is given, define $\\{(s_j,r_j)\\}_{J}:=\\Sigma_C$ as in (4.2.2). If $\\{(s_j,r_j)\\}_J$ is given, the intersection of the corresponding half-spaces is a closed convex set. Note here that we can define at the same time the whole of $\\mathbb{R}^n$ and the empty sets as two extreme cases."
    },
    {
        "statement": "Let $K$ be a convex cone with polar $K^{\\circ}$; then, the polar $K^{\\circ\\circ}$ of $K^{\\circ}$ is the closure of $K$.",
        "title": "",
        "label": "prop:CAMA-chap3-4.2.7",
        "lean_tag": [
            "CAMA_chap_3_4_2_7"
        ],
        "lean_formalization": "lemma CAMA_chap_3_4_2_7 {n : ℕ}\n  (K : Set (EuclideanSpace ℝ (Fin n)))\n  (hK_convex_cone : IsConvexCone K)\n  (hK_nonempty : Set.Nonempty K) :\n  PolarCone (PolarCone K) = closure K := by\n  sorry",
        "proof": "We exploit Remark 4.1.2: due to its conical character $(\\alpha x\\in K$ if $\\alpha\\in K$ and $\\alpha>0)$, $\\operatorname{cl}K$ has a very special support function:\n\\[\n\\sigma_{\\operatorname{cl}K}(s)=\n\\begin{cases}\n\\langle s,0\\rangle=0 &\\text{if }\\langle s,x\\rangle\\le 0\\text{ for all }x\\in\\operatorname{cl}K,\\\\[4pt]\n+\\infty &\\text{otherwise}.\n\\end{cases}\n\\]\nIn other words, $\\sigma_{\\operatorname{cl}K}$ is $0$ on $K^{\\circ}$, $+\\infty$ elsewhere. Thus, the characterization\n\\[\nx\\in\\operatorname{cl}K \\iff \\langle\\cdot,x\\rangle\\le\\sigma_{\\operatorname{cl}K}(\\cdot)\n\\]\nbecomes\n\\[\nx\\in\\operatorname{cl}K \\iff\n\\begin{cases}\n\\langle s,x\\rangle\\le 0 &\\text{for all }s\\in K^{\\circ}\\\\[4pt]\n(\\langle s,x\\rangle\\text{ arbitrary}) &\\text{for }s\\notin K^{\\circ},\n\\end{cases}\n\\]\nin which we recognize the definition of $K^{\\circ\\circ}$."
    },
    {
        "statement": "Let $b,s_1,\\dots,s_m$ be given in $\\mathbb{R}^n$. The set\n\\[\n\\{x\\in\\mathbb{R}^n:\\ \\langle s_j,x\\rangle\\le 0\\quad\\text{for }j=1,\\dots,m\\}\n\\tag{4.3.1}\n\\]\nis contained in the set\n\\[\n\\{x\\in\\mathbb{R}^n:\\ \\langle b,x\\rangle\\le 0\\}\n\\tag{4.3.2}\n\\]\nif and only if (see Definition 1.4.5 of a conical hull)\n\\[\nb\\in\\operatorname{cone}\\{s_1,\\dots,s_m\\}.\n\\tag{4.3.3}\n\\]",
        "title": "Farkas I",
        "label": "lem:CAMA-chap3-4.3.1",
        "lean_tag": [
            "CAMA_chap_3_4_3_1"
        ],
        "lean_formalization": "lemma CAMA_chap_3_4_3_1 {m n : ℕ}\n  (b : EuclideanSpace ℝ (Fin n))\n  (s : Fin m → EuclideanSpace ℝ (Fin n)) :\n  {x : EuclideanSpace ℝ (Fin n) | ∀ j, inner ℝ (s j) x ≤ 0} ⊆\n  {x : EuclideanSpace ℝ (Fin n) | inner ℝ b x ≤ 0} ↔ b ∈ cone s := by\n  sorry",
        "proof": ""
    },
    {
        "statement": "Let $b,s_1,\\dots,s_m$ be given in $\\mathbb{R}^n$. Then exactly one of the following statements is true.\n\\begin{enumerate}\n\\item[$P:=$] \\eqref{4.3.4} has a solution $\\alpha\\in\\mathbb{R}^n$.\n\\item[$Q:=$] \n\\[\n\\left\\{\n\\begin{aligned}\n&\\langle b,x\\rangle>0,\\\\\n&\\langle s_j,x\\rangle\\le 0\\quad\\text{for }j=1,\\dots,m\n\\end{aligned}\n\\right.\n\\]\nhas a solution $x\\in\\mathbb{R}^n$.\n\\end{enumerate}",
        "title": "Farkas II",
        "label": "lem:CAMA-chap3-4.3.2",
        "lean_tag": [
            "CAMA_chap_3_4_3_2"
        ],
        "lean_formalization": "lemma CAMA_chap_3_4_3_2 {m n : ℕ}\n  (b : EuclideanSpace ℝ (Fin n))\n  (s : Fin m → EuclideanSpace ℝ (Fin n)) :\n  let P := b ∈ cone s\n  let Q := ∃ (x : EuclideanSpace ℝ (Fin n)), (inner ℝ b x > 0) ∧ (∀ j, inner ℝ (s j) x ≤ 0)\n  (P ∨ Q) ∧ ¬(P ∧ Q) := by\n  sorry",
        "proof": ""
    },
    {
        "statement": "Let $s_1,\\dots,s_m$ be given in $\\mathbb{R}^n$. Then the convex cone\n\\[\nK:=\\operatorname{cone}\\{s_1,\\dots,s_m\\}=\\Big\\{\\sum_{j=1}^m\\alpha_j s_j:\\ \\alpha_j\\ge 0\\ \\text{for }j=1,\\dots,m\\Big\\}\n\\]\nis closed.",
        "title": "Farkas III",
        "label": "lem:CAMA-chap3-4.3.3",
        "lean_tag": [
            "CAMA_chap_3_4_3_3"
        ],
        "lean_formalization": "lemma CAMA_chap_3_4_3_3 {m n : ℕ}\n  (s : Fin m → EuclideanSpace ℝ (Fin n)) :\n  IsClosed (cone s) := by\n  sorry",
        "proof": "It is quite similar to that of Carathéodory's Theorem 1.3.6. First, the proof is easy if the $s_j$'s are linearly independent: then, the convergence of\n\\begin{equation}\\label{eq:4.3.5}\nx^k=\\sum_{j=1}^m\\alpha_j^k s_j\\quad\\text{for }k\\to\\infty\n\\end{equation}\nis equivalent to the convergence of each $\\{\\alpha_j^k\\}$ to some $\\alpha_j$, which must be nonnegative if each $\\alpha_j^k$ in \\eqref{eq:4.3.5} is nonnegative.\n\nSuppose, on the contrary, that the system $\\sum_{j=1}^m\\beta_j s_j=0$ has a nonzero solution $\\beta\\in\\mathbb{R}^m$ and assume $\\beta_j<0$ for some $j$ (change $\\beta$ to $-\\beta$ if necessary). As in the proof of Theorem 1.3.6, write each $x\\in K$ as\n\\[\nx=\\sum_{j=1}^m\\alpha_j s_j=\\sum_{j=1}^m[\\alpha_j+t^*(x)\\beta_j]s_j=\\sum_{j\\ne i(x)}\\alpha'_j s_j,\n\\]\nwhere\n\\[\ni(x)\\in\\operatorname{Argmin}_{\\beta_j<0}\\frac{-\\alpha_j}{\\beta_j},\\qquad\nt^*(x):=\\frac{-\\alpha_{i(x)}}{\\beta_{i(x)}},\n\\]\nso that each $\\alpha'_j=\\alpha_j+t^*(x)\\beta_j$ is nonnegative. Letting $x$ vary in $K$, we thus construct\na decomposition\n\\[\nK=\\bigcup\\{K_i:\\; i=1,\\ldots,m\\},\n\\]\nwhere $K_i$ is the conical hull of the $m-1$ generators $s_j,\\; j\\neq i$.\n\nNow, if there is some $i$ such that the generators of $K_i$ are linearly dependent, we repeat the argument for a further decomposition of this $K_i$. After finitely many such operations, we end up with a decomposition of $K$ as a finite union of polyhedral convex cones, each having linearly independent generators. All these cones are therefore closed (first part of the proof), so $K$ is closed as well."
    },
    {
        "statement": "Let be given $(b,r)$ and $(s_j,\\rho_j)$ in $\\mathbb{R}^n\\times\\mathbb{R}$, where $j$ varies in an (arbitrary) index set $J$. Suppose that the system of inequalities\n\\[\n\\langle s_j,x\\rangle \\le \\rho_j\\qquad\\text{for all }j\\in J\n\\tag{4.3.6}\n\\]\nhas a solution $x\\in\\mathbb{R}^n$ (the system is consistent). Then the following two properties are equivalent:\n\n(i) $\\langle b,x\\rangle \\le r$ for all $x$ satisfying (4.3.6);\n\n(ii) $(b,r)$ is in the closed convex conical hull of $S:=\\{(0,1)\\}\\cup\\{(s_j,\\rho_j)\\}_{j\\in J}$.",
        "title": "Generalized Farkas",
        "label": "thm:CAMA-chap3-4.3.4",
        "lean_tag": "no-lean-tag",
        "proof": "[(ii)\\,$\\Rightarrow$\\,(i)] Let first $(b,r)$ be in $K:=\\operatorname{cone}S$. In other words, there exists a finite set $\\{1,\\dots,m\\}\\subset J$ and nonnegative $\\alpha_0,\\alpha_1,\\dots,\\alpha_m$ such that (we adopt the convention $\\sum\\emptyset=0$)\n\\[\nb=\\sum_{j=1}^m\\alpha_j s_j\\qquad\\text{and}\\qquad r=\\alpha_0+\\sum_{j=1}^m\\alpha_j\\rho_j.\n\\]\nFor each $x$ satisfying (4.3.6) we can write\n\\[\n\\langle b,x\\rangle \\le r-\\alpha_0\\le r.\n\\tag{4.3.7}\n\\]\nIf, now, $(b,r)$ is in the closure of $K$, pass to the limit in (4.3.7) to establish the required conclusion (i) for all $(b,r)$ described by (ii).\n\n[(i)\\,$\\Rightarrow$\\,(ii)] If $(b,r)\\notin\\operatorname{cl}K$, separate $(b,r)$ from $\\operatorname{cl}K$: equipping $\\mathbb{R}^n\\times\\mathbb{R}$ with the scalar product\n\\[\n\\langle\\!\\langle (b,r),(d,t)\\rangle\\!\\rangle:=\\langle b,d\\rangle+rt,\n\\]\nthere exists $(d,-t)\\in\\mathbb{R}^n\\times\\mathbb{R}$ such that\n\\[\n\\sup_{(s,\\rho)\\in K}\\; [\\langle s,d\\rangle - \\rho t] < \\langle b,d\\rangle - r t . \\tag{4.3.8}\n\\]\n\nIt follows first that the left-hand supremum is a finite number \\(\\kappa\\). Then the conical character of \\(K\\) implies \\(\\kappa \\le 0\\), because \\(\\alpha\\kappa \\le \\kappa\\) for all \\(\\alpha>0\\); actually \\(\\kappa=0\\) because \\((0,0)\\in K\\). In summary, we have singled out \\((d,t)\\in\\mathbb{R}^n\\times\\mathbb{R}\\) such that\n\\[\nt \\ge 0 \\qquad [\\text{take }(0,1)\\in K]\n\\]\n\\[\n(\\ast)\\qquad \\langle s_j,d\\rangle - \\rho_j t \\le 0\\ \\text{ for all } j\\in J \\qquad [\\text{take }(s_j,\\rho_j)\\in K]\n\\]\n\\[\n(\\ast\\ast)\\qquad \\langle b,d\\rangle - r t > 0. \\qquad [\\text{don't forget (4.3.8)}]\n\\]\n\nNow consider two cases:\n\\begin{itemize}\n\\item If \\(t>0\\), divide \\((\\ast)\\) and \\((\\ast\\ast)\\) by \\(t\\) to exhibit the point \\(x=d/t\\) violating (i).\n\\item If \\(t=0\\), take \\(x_0\\) satisfying (4.3.6). Observe from \\((\\ast)\\) that, for all \\(\\alpha>0\\), the point \\(x(\\alpha)=x_0+\\alpha d\\) satisfies (4.3.6) as well. Yet, let \\(\\alpha\\to +\\infty\\) in\n\\[\n\\langle b,x(\\alpha)\\rangle = \\langle b,x_0\\rangle + \\alpha\\langle b,d\\rangle\n\\]\nto realize from \\((\\ast\\ast)\\) that \\(x(\\alpha)\\) violates (i) if \\(\\alpha\\) is large enough.\n\\end{itemize}\n\nThus we have proved in both cases that \"not (ii) \\(\\Rightarrow\\) not (i)\"."
    },
    {
        "statement": "A direction $d$ is tangent to $S$ at $x\\in S$ if and only if\n\\[\n\\exists (d_k)\\to d,\\ \\exists (t_k)\\downarrow 0\\quad\\text{such that}\\quad x+t_kd_k\\in S\\ \\text{for all }k.\n\\]",
        "title": "",
        "label": "prop:CAMA-chap3-5.1.2",
        "lean_tag": [
            "CAMA_chap_3_5_1_2"
        ],
        "lean_formalization": "lemma CAMA_chap_3_5_1_2 {n : ℕ}\n  (d : EuclideanSpace ℝ (Fin n))\n  (S : Set (EuclideanSpace ℝ (Fin n)))\n  (x : EuclideanSpace ℝ (Fin n)) :\n  IsTangentTo S x d ↔\n  ∃ (dk : ℕ → EuclideanSpace ℝ (Fin n)), ∃ (tk : ℕ → ℝ),\n  (Filter.Tendsto dk Filter.atTop (𝓝 d)) ∧\n  (Filter.Tendsto tk Filter.atTop (𝓝 0)) ∧\n  (∀ i, tk i > 0) ∧\n  (∀ k, x + (tk k) • (dk k) ∈ S) := by\n  sorry",
        "proof": ""
    },
    {
        "statement": "The tangent cone is closed.",
        "title": "",
        "label": "prop:CAMA-chap3-5.1.3",
        "lean_tag": [
            "CAMA_chap_3_5_1_3"
        ],
        "lean_formalization": "lemma CAMA_chap_3_5_1_3 {n : ℕ}\n  (S : Set (EuclideanSpace ℝ (Fin n)))\n  (x : EuclideanSpace ℝ (Fin n)) :\n  IsClosed (TangentCone x S) := by\n  sorry",
        "proof": "Let $\\{d_\\ell\\}\\subset T_S(x)$ be converging to $d$; for each $\\ell$ take sequences $\\{x_{\\ell,k}\\}_k$ and $\\{t_{\\ell,k}\\}_k$ associated with $d_\\ell$ in the sense of Definition 5.1.1. Fix $\\ell>0$: we can find $k_\\ell$ such that\n\\[\n\\left\\|\\frac{x_{\\ell,k_\\ell}-x}{t_{\\ell,k_\\ell}}-d_\\ell\\right\\|\\le\\frac{1}{\\ell}.\n\\]\nLetting $\\ell\\to\\infty$, we then obtain the sequences $\\{x_\\ell,t_\\ell\\}_\\ell$ and $\\{t_\\ell,k_\\ell\\}_\\ell$ which define $d$ as an element of $T_S(x)$."
    },
    {
        "statement": "The tangent cone to $C$ at $x$ is the closure of the cone generated by $C-\\{x\\}$:\n\\begin{align}\nT_C(x)\n&=\\overline{\\operatorname{cone}}(C-x)=\\overline{\\operatorname{cl}}\\mathbb{R}^+(C-x)\\nonumber\\\\\n&=\\overline{\\{d\\in\\mathbb{R}^n:\\;d=\\alpha(y-x),\\;y\\in C,\\;\\alpha\\ge0\\}}.\n\\tag{5.2.1}\n\\end{align}",
        "title": "",
        "label": "prop:CAMA-chap3-5.2.1",
        "lean_tag": [
            "CAMA_chap_3_5_2_1"
        ],
        "lean_formalization": "lemma CAMA_chap_3_5_2_1 {n : ℕ}\n  (C : Set (EuclideanSpace ℝ (Fin n)))\n  (x : EuclideanSpace ℝ (Fin n))\n  (hx_inC : x ∈ C)\n  (hC_closed : IsClosed C)\n  (hC_convex : Convex ℝ C) :\n  TangentCone x C = closure (GenCone (translate_set C x)) := by\n  sorry",
        "proof": "We have just said that $C-\\{x\\}\\subset T_C(x)$. Because $T_C(x)$ is a closed cone (Proposition 5.1.3), it immediately follows that $\\overline{\\mathbb{R}^+}(C-x)\\subset T_C(x)$. Conversely, for $d\\in T_C(x)$, take $\\{x_k\\}$ and $\\{t_k\\}$ as in the definition (5.1.1): the point $(x_k-x)/t_k$ is in $\\mathbb{R}^+(C-x)$, hence its limit $d$ is in the closure of this latter set. \\qedhere"
    },
    {
        "statement": "The normal cone is the polar of the tangent cone.",
        "title": "",
        "label": "prop:CAMA-chap3-5.2.4",
        "lean_tag": [
            "CAMA_chap_3_5_2_4"
        ],
        "lean_formalization": "lemma CAMA_chap_3_5_2_4 {n : ℕ}\n  (C : Set (EuclideanSpace ℝ (Fin n)))\n  (x : EuclideanSpace ℝ (Fin n))\n  (hX_inC : x ∈ C)\n  (hC_closed : IsClosed C)\n  (hC_convex : Convex ℝ C) :\n  NormalCone x C = PolarCone (TangentCone x C) := by\n  sorry",
        "proof": "If $\\langle s,d\\rangle\\le 0$ for all $d\\in C-x$, the same holds for all $d\\in\\mathbb R^+(C-x)$, as well as for all $d$ in the closure $T_C(x)$ of the latter. Thus, $N_C(x)\\subset[T_C(x)]^\\circ$.\n\nConversely, take $s$ arbitrary in $[T_C(x)]^\\circ$. The relation $\\langle s,d\\rangle\\le 0$, which holds for all $d\\in T_C(x)$, a fortiori holds for all $d\\in C-x\\subset T_C(x)$; this is just (5.2.2). \\qedhere"
    },
    {
        "statement": "The tangent cone is the polar of the normal cone:\n\\[\nT_C(x)=\\{d\\in\\mathbb{R}^n:\\ \\langle s,d\\rangle\\le 0\\ \\text{for all }s\\in N_C(x)\\}.\n\\]",
        "title": "",
        "label": "cor:CAMA-chap3-5.2.5",
        "lean_tag": [
            "CAMA_chap_3_5_2_5"
        ],
        "lean_formalization": "lemma CAMA_chap_3_5_2_5 {n : ℕ}\n  (C : Set (EuclideanSpace ℝ (Fin n)))\n  (x : EuclideanSpace ℝ (Fin n))\n  (hx_inC : x ∈ C)\n  (hC_closed : IsClosed C)\n  (hC_convex : Convex ℝ C) :\n  TangentCone x C = PolarCone (NormalCone x C) := by\n  sorry",
        "proof": ""
    },
    {
        "statement": "Here, the $C$'s are nonempty closed convex sets.\n\\begin{enumerate}[(i)]\n\\item For $x\\in C_1\\cap C_2$, there holds\n\\[\nT_{C_1\\cap C_2}(x)\\subset T_{C_1}(x)\\cap T_{C_2}(x)\n\\qquad\\text{and}\\qquad\nN_{C_1\\cap C_2}(x)\\supset N_{C_1}(x)+N_{C_2}(x).\n\\]\n\n\\item With $C_i\\subset\\mathbb R^{n_i}$, $i=1,2$ and $(x_1,x_2)\\in C_1\\times C_2$,\n\\[\nT_{C_1\\times C_2}(x_1,x_2)=T_{C_1}(x_1)\\times T_{C_2}(x_2),\n\\qquad\nN_{C_1\\times C_2}(x_1,x_2)=N_{C_1}(x_1)\\times N_{C_2}(x_2).\n\\]\n\n\\item With an affine mapping $A(x)=y_0+A_0x$ ($A_0$ linear) and $x\\in C$,\n\\[\nT_{A(C)}[A(x)]=\\operatorname{cl}[A_0T_C(x)]\n\\qquad\\text{and}\\qquad\nN_{A(C)}[A(x)]=A_0^{-*}[N_C(x)].\n\\]\n\n\\item In particular (start from (ii), (iii) and proceed as when proving (1.2.2)):\n\\[\nT_{C_1+C_2}(x_1+x_2)=\\operatorname{cl}[T_{C_1}(x_1)+T_{C_2}(x_2)],\n\\qquad\nN_{C_1+C_2}(x_1+x_2)=N_{C_1}(x_1)\\cap N_{C_2}(x_2).\n\\]\n\\end{enumerate}",
        "title": "",
        "label": "prop:CAMA-chap3-5.3.1",
        "lean_tag": [
            "CAMA_chap_3_5_3_1"
        ],
        "lean_formalization": "lemma CAMA_chap_3_5_3_1 {n : ℕ}\n  (x : EuclideanSpace ℝ (Fin n))\n  (C₁ : Set (EuclideanSpace ℝ (Fin n)))\n  (C₂ : Set (EuclideanSpace ℝ (Fin n)))\n  (hC₁₀ : Set.Nonempty C₁)\n  (hC₁₁ : IsClosed C₁)\n  (hC₁₂ : Convex ℝ C₁)\n  (hC₂₀ : Set.Nonempty C₂)\n  (hC₂₁ : IsClosed C₂)\n  (hC₂₂ : Convex ℝ C₂) :\n  x ∈ C₁ ∩ C₂ → TangentCone x (C₁ ∩ C₂) ⊆ TangentCone x C₁ ∩ TangentCone x C₂ ∧\n  NormalCone x (C₁ ∩ C₂) ⊇ set_add (NormalCone x C₁) (NormalCone x C₂) := by\n  sorry",
        "proof": ""
    },
    {
        "statement": "For $x\\in C$ and $s\\in\\mathbb{R}^n$, the following properties are equivalent:\n\\begin{enumerate}\n\\item[(i)] $s\\in N_C(x)$;\n\\item[(ii)] $x$ is in the exposed face $F_C(s)$: $\\langle s,x\\rangle=\\max_{y\\in C}\\langle s,y\\rangle$;\n\\item[(iii)] $x=p_C(x+s)$.\n\\end{enumerate}",
        "title": "",
        "label": "prop:CAMA-chap3-5.3.3",
        "lean_tag": [
            "CAMA_chap_3_5_3_3"
        ],
        "lean_formalization": "lemma CAMA_chap_3_5_3_3 {n : ℕ}\n  (x : EuclideanSpace ℝ (Fin n))\n  (C : Set (EuclideanSpace ℝ (Fin n)))\n  (s : EuclideanSpace ℝ (Fin n))\n  (hC_nonempty : Set.Nonempty C)\n  (hC_closed : IsClosed C)\n  (hC_convex : Convex ℝ C)\n  (hx_in_C : x ∈ C) :\n  List.TFAE [\n    s ∈ NormalCone x C,\n    (inner ℝ s x) = ⨆ y ∈ C, inner ℝ s y,\n    x = pC (x + s) C hC_closed hC_convex hC_nonempty\n  ] := by\n  sorry\n",
        "proof": "Nothing really new: everything comes from the definitions of normal cones, supporting hyperplanes, exposed faces, and the characteristic property (3.1.3) of the projection operator."
    },
    {
        "statement": "The function $f$ is strongly convex on $C$ with modulus $c$ if and only if the function $f-\\tfrac{1}{2}c\\|\\cdot\\|^{2}$ is convex on $C$.",
        "title": "",
        "label": "prop:FCA-chapB-1.1.2",
        "lean_tag": [
            "FCA_chap_B_1_1_2"
        ],
        "lean_formalization": "lemma FCA_chap_B_1_1_2 {n : ℕ}\n  (c : ℝ) (C : Set (EuclideanSpace ℝ (Fin n))) (f : EuclideanSpace ℝ (Fin n) → ℝ)\n  (hC_nonempty : Set.Nonempty C) (hC_convex : Convex ℝ C)\n  : StrongConvexOn C c f ↔ ConvexOn ℝ C (fun x => f x - (c/2) * ‖x‖^2)\n  := by sorry",
        "proof": "Use direct calculations in the definition (1.1.1) of convexity applied to the function $f-\\tfrac{1}{2}c\\|\\cdot\\|^{2}$, namely:\n\\[\nf(\\alpha x+(1-\\alpha)x')-\\tfrac{1}{2}c\\|\\alpha x+(1-\\alpha)x'\\|^{2}\\le\n\\]\n\\[\n\\le \\alpha f(x)+(1-\\alpha)f(x')-\\tfrac{1}{2}c\\big[\\alpha\\|x\\|^{2}+(1-\\alpha)\\|x'\\|^{2}\\big].\n\\]"
    },
    {
        "statement": "Let $f:\\mathbb{R}^n\\to\\mathbb{R}\\cup\\{+\\infty\\}$ be not identically equal to $+\\infty$. The three properties below are equivalent:\n\\begin{enumerate}\n\\item[(i)] $f$ is convex in the sense of Definition 1.1.3;\n\\item[(ii)] its epigraph is a convex set in $\\mathbb{R}^n\\times\\mathbb{R}$;\n\\item[(iii)] its strict epigraph is a convex set in $\\mathbb{R}^n\\times\\mathbb{R}$.\n\\end{enumerate}",
        "title": "",
        "label": "prop:FCA-chapB-1.1.6",
        "lean_tag": [
            "FCA_chap_B_1_1_6"
        ],
        "lean_formalization": "lemma FCA_chap_B_1_1_6 {n : ℕ}\n  (f : EuclideanSpace ℝ (Fin n) → WithTop ℝ) (hf_nonemptyDomain : ∃ x, f x < ⊤)\n  : List.TFAE [\n    ConvexOn ℝ Set.univ f,\n    Convex ℝ (epigraph (liftWithToptoEReal f)),\n    Convex ℝ (strictEpigraph f)\n  ]\n  := by sorry",
        "proof": "Left as an exercise."
    },
    {
        "statement": "(Inequality of Jensen) Let $f\\in\\operatorname{Conv}\\mathbb{R}^n$. Then, for all collections $\\{x_1,\\dots,x_k\\}$ of points in $\\dom f$ and all $\\alpha=(\\alpha_1,\\dots,\\alpha_k)$ in the unit simplex of $\\mathbb{R}^k$, there holds (inequality of Jensen in summation form)\n\\[\nf\\!\\bigg(\\sum_{i=1}^k \\alpha_i x_i\\bigg)\\le \\sum_{i=1}^k \\alpha_i f(x_i).\n\\]",
        "title": "",
        "label": "thm:FCA-chapB-1.1.8",
        "lean_tag": [
            "FCA_chap_B_1_1_8"
        ],
        "lean_formalization": "lemma FCA_chap_B_1_1_8 {n : ℕ}\n  (k : ℕ) (f : EuclideanSpace ℝ (Fin n) → WithTop ℝ)\n  (hC_convex : InConvRn f)\n  : ∀ (x : Fin k → EuclideanSpace ℝ (Fin n)), ∀ (α : EuclideanSpace ℝ (Fin k)),\n    α ∈ Δκ k → f (∑ i, (α i) • (x i)) ≤ ∑ i, (α i) • f (x i)\n  := by sorry",
        "proof": "The $k$ points $(x_i,f(x_i))\\in\\mathbb{R}^n\\times\\mathbb{R}$ are clearly in $\\epi f$, a convex set. Their convex combination\n\\[\n\\sum_i \\alpha_i (x_i,f(x_i)) = \\Big(\\sum_i \\alpha_i x_i,\\sum_i \\alpha_i f(x_i)\\Big)\n\\]\nis also in $\\epi f$ (Proposition A.1.3.3). This is just the claimed inequality."
    },
    {
        "statement": "Let $f\\in\\Conv\\R^n$.  The relative interior of $\\operatorname{epi}f$ is the union over $x\\in\\operatorname{ri}\\,\\operatorname{dom}f$ of the open half-lines with bottom endpoints at $f(x)$:\n\\[\n\\operatorname{ri}\\operatorname{epi}f=\\{(x,r)\\in\\R^n\\times\\R : x\\in\\operatorname{ri}\\operatorname{dom}f,\\ r>f(x)\\}.\n\\]",
        "title": "",
        "label": "prop:FCA-chapB-1.1.9",
        "lean_tag": [
            "FCA_chap_B_1_1_9"
        ],
        "lean_formalization": "lemma FCA_chap_B_1_1_9 {n : ℕ}\n  (f : EuclideanSpace ℝ (Fin n) → WithTop ℝ)\n  (hf_convex : InConvRn f)\n  : intrinsicInterior ℝ (epigraph (liftWithToptoEReal f)) =\n    {p : EuclideanSpace ℝ (Fin n) × ℝ | p.1 ∈ intrinsicInterior ℝ (effDom (liftWithToptoEReal f)) ∧ p.2 > f p.1}\n  := by sorry",
        "proof": "Since $\\operatorname{dom}f$ is the image of $\\operatorname{epi}f$ under the linear mapping ``projection onto $\\R^n$'', Proposition A.2.1.12 tells us that\n\\[\n\\operatorname{ri}\\operatorname{dom}f\\ \\text{is the projection onto }\\R^n\\ \\text{of }\\operatorname{ri}\\operatorname{epi}f.\n\\tag{1.1.5}\n\\]\nNow take $x$ arbitrary in $\\operatorname{ri}\\operatorname{dom}f$.  The subset of $\\operatorname{ri}\\operatorname{epi}f$ that is projected onto $x$ is just $((\\{x\\}\\times\\R)\\cap\\operatorname{ri}\\operatorname{epi}f)$, which in turn is $\\operatorname{ri}((\\{x\\}\\times\\R)\\cap\\operatorname{epi}f)$ (use Proposition A.2.1.10).  This latter set is clearly $\\{x\\}\\times(f(x),+\\infty)$.\n\nIn summary, we have proved that, for $x\\in\\operatorname{ri}\\operatorname{dom}f$, $(x,r)\\in\\operatorname{ri}\\operatorname{epi}f$ if and only if $r>f(x)$.  Together with (1.1.5), this proves our claim."
    },
    {
        "statement": "Any $f\\in\\operatorname{Conv}\\mathbb{R}^n$ is minorized by some affine function.  More precisely: for any $x_0\\in\\operatorname{ri}\\operatorname{dom}f$, there is $s$ in the subspace parallel to $\\operatorname{aff}\\operatorname{dom}f$ such that\n\\[\nf(x)\\ge f(x_0)+\\langle s,\\,x-x_0\\rangle\\quad\\text{for all }x\\in\\mathbb{R}^n.\n\\]",
        "title": "",
        "label": "thm:FCA-chapB-1.2.1",
        "lean_tag": [
            "FCA_chap_B_1_2_1"
        ],
        "lean_formalization": "lemma FCA_chap_B_1_2_1 {n : ℕ}\n  (f : EuclideanSpace ℝ (Fin n) → WithTop ℝ)\n  (hf_convex : InConvRn f)\n  : ∀ (x₀ : EuclideanSpace ℝ (Fin n)), (x₀ ∈ intrinsicInterior ℝ (effDom (liftWithToptoEReal f))) →\n    ∃ (s : EuclideanSpace ℝ (Fin n)), ∀ (x : EuclideanSpace ℝ (Fin n)),\n    (s ∈ parallelSubspace (affineHull (effDom (liftWithToptoEReal f)))) ∧ (f x ≥ f x₀ + inner ℝ s (x - x₀))\n  := by sorry",
        "proof": ""
    },
    {
        "statement": "For $f:\\mathbb{R}^n\\to\\mathbb{R}\\cup\\{+\\infty\\}$, the following three properties are equivalent:\n\\begin{enumerate}\n\\item[(i)] $f$ is lower semi-continuous on $\\mathbb{R}^n$;\n\\item[(ii)] $\\operatorname{epi} f$ is a closed set in $\\mathbb{R}^n\\times\\mathbb{R}$;\n\\item[(iii)] the sublevel-sets $S_r(f)$ are closed (possibly empty) for all $r\\in\\mathbb{R}$.\n\\end{enumerate}",
        "title": "",
        "label": "prop:FCA-chapB-1.2.2",
        "lean_tag": [
            "FCA_chap_B_1_2_2"
        ],
        "lean_formalization": "lemma FCA_chap_B_1_2_2 {n : ℕ}\n  (f : EuclideanSpace ℝ (Fin n) → WithTop ℝ)\n  : List.TFAE [\n      LowerSemicontinuousOn f Set.univ,\n      IsClosed (epigraph (liftWithToptoEReal f)),\n      ∀ (r : ℝ), IsClosed (sublevelSet r f)\n  ]\n  := by sorry",
        "proof": "[(i) $\\Rightarrow$ (ii)] Let $(y_k,r_k)_k$ be a sequence of $\\operatorname{epi} f$ converging to $(x,r)$ for $k\\to+\\infty$. Since $f(y_k)\\le r_k$ for all $k$, the l.s.c.\\ relation (1.2.3) readily gives\n\\[\nr=\\lim r_k \\ge \\liminf f(y_k)\\ge \\liminf_{y\\to x} f(y)\\ge f(x),\n\\]\ni.e.\\ $(x,r)\\in\\operatorname{epi} f$.\n\n[(ii) $\\Rightarrow$ (iii)] Construct the sublevel-sets $S_r(f)$ as in Remark 1.1.7: the closed sets $\\operatorname{epi} f$ and $\\mathbb{R}^n\\times\\{r\\}$ have a closed intersection.\n\n[(iii) $\\Rightarrow$ (i)] Suppose $f$ is not lower semi-continuous at some $x$: there is a (sub)sequence $(y_k)$ converging to $x$ such that $f(y_k)$ converges to $\\rho<f(x)\\le +\\infty$. Pick $r\\in[\\rho,f(x))$: for $k$ large enough, $f(y_k)\\le r< f(x)$; hence $S_r(f)$ contains the tail of $(y_k)$ but not its limit $x$. Consequently, this $S_r(f)$ is not closed."
    },
    {
        "statement": "Let $f\\in\\operatorname{Conv}\\mathbb{R}^n$ and $x'\\in\\operatorname{ri}\\operatorname{dom}f$. There holds (in $\\mathbb{R}\\cup\\{+\\infty\\}$)\n\\[\n\\operatorname{cl}f(x)=\\lim_{t\\downarrow 0}f\\big(x+t(x'-x)\\big)\\quad\\text{for all }x\\in\\mathbb{R}^n.\n\\]",
        "title": "",
        "label": "prop:FCA-chapB-1.2.5",
        "lean_tag": [
            "FCA_chap_B_1_2_5"
        ],
        "lean_formalization": "lemma FCA_chap_B_1_2_5 {n : ℕ}\n  (f : EuclideanSpace ℝ (Fin n) → WithTop ℝ) (x₀ : EuclideanSpace ℝ (Fin n))\n  (hf_convex : InConvRn f) (hx : x₀ ∈ intrinsicInterior ℝ (effDom (liftWithToptoEReal f)))\n  : ∀ x, Filter.Tendsto (fun t => f (x + t • (x₀ - x))) (𝓝[>] 0) (𝓝 (lscHull f x))\n  := by sorry",
        "proof": "Since $x_t:=x+t(x'-x)\\to x$ when $t\\downarrow 0$, we certainly have\n\\[\n(\\operatorname{cl}f)(x)\\le\\liminf_{t\\downarrow 0}f\\big(x+t(x'-x)\\big).\n\\]\n\nWe will prove the converse inequality by showing that\n\\[\n\\limsup_{t\\downarrow 0}f\\big(x+t(x'-x)\\big)\\le r\\quad\\text{for all }r\\ge(\\operatorname{cl}f)(x).\n\\]\n\n(Non-existence of such an $r$ means that $\\operatorname{cl}f(x)=+\\infty$, the proof is finished.)\n\nThus let $(x,r)\\in\\operatorname{epi}(\\operatorname{cl}f)=\\operatorname{cl}(\\operatorname{epi}f)$. Pick $r'>f(x')$, hence $(x',r')\\in\\operatorname{epi}f$ (Proposition 1.1.9). Applying Lemma A.2.1.6 to the convex set $\\operatorname{epi}f$, we see that\n\\[\nt(x',r')+(1-t)(x,r)\\in\\operatorname{epi}f\\subset\\operatorname{epi}f\\quad\\text{for all }t\\in]0,1].\n\\]\n\nThis just means\n\\[\nf\\big(x+t(x'-x)\\big)\\le tr'+(1-t)r\\quad\\text{for all }t\\in]0,1]\n\\]\nand our required inequality follows by letting $t\\downarrow 0$."
    },
    {
        "statement": "For $f\\in\\Conv\\R^n$, there holds\n\\[\n\\operatorname{cl} f\\in\\Conv\\R^n;\n\\tag{1.2.7}\n\\]\n\\[\n\\operatorname{cl} f\\text{ and }f\\text{ coincide on the relative interior of }\\dom f.\n\\tag{1.2.8}\n\\]",
        "title": "",
        "label": "prop:FCA-chapB-1.2.6",
        "lean_tag": [
            "FCA_chap_B_1_2_6"
        ],
        "lean_formalization": "lemma FCA_chap_B_1_2_6 {n : ℕ}\n  (f : EuclideanSpace ℝ (Fin n) → WithTop ℝ) (hf_convex : InConvRn f)\n  : let cl_f := lscHull f\n    (InConvRn cl_f) ∧ (∀ x ∈ intrinsicInterior ℝ (effDom (liftWithToptoEReal f)), cl_f x = f x)\n  := by sorry",
        "proof": "We already know from Proposition A.1.2.6 that $\\epi\\operatorname{cl} f=\\operatorname{cl}\\epi f$ is a convex set; also $\\operatorname{cl} f\\le f\\not\\equiv+\\infty$; finally, Proposition 1.2.1 guarantees in the relation of definition (1.2.4) that $\\operatorname{cl} f(x)>-\\infty$ for all $x$: (1.2.7) does hold.\n\nOn the other hand, suppose $x\\in\\ri\\dom f$. Then the one-dimensional function $\\varphi(t)=f(x+td)$ is continuous at $t=0$ (Theorem 0.6.2); it follows from Proposition 1.2.5 that $\\operatorname{cl} f$ coincides with $f$ on $\\ri\\dom f$; besides, $\\operatorname{cl} f(x)$ is obviously equal to $f(x)=+\\infty$ for all $x\\notin\\cl\\dom f$. Altogether, (1.2.8) is true."
    },
    {
        "statement": "The closure of $f\\in\\Conv\\R^n$ is the supremum of all affine functions minorizing $f$:\n\\[\n\\cl f(x)=\\sup_{(s,b)\\in\\R^n\\times\\R}\\{\\langle s,x\\rangle-b:\\langle s,y\\rangle-b\\le f(y)\\text{ for all }y\\in\\R^n\\}.\n\\tag{1.2.9}\n\\]",
        "title": "",
        "label": "thm:FCA-chapB-1.2.8",
        "lean_tag": [
            "FCA_chap_B_1_2_8"
        ],
        "lean_formalization": "lemma FCA_chap_B_1_2_8 {n : ℕ}\n  (f : EuclideanSpace ℝ (Fin n) → WithTop ℝ) (hf_convex : InConvRn f)\n  : let cl_f := lscHull f\n    ∀ x, cl_f x = sSup {v : ℝ | ∃ (z : EuclideanSpace ℝ (Fin n) × ℝ),\n                                (v = inner ℝ z.1 x - z.2) ∧ (∀ y, (inner ℝ z.1 y) - z.2 ≤ f y)}\n  := by sorry",
        "proof": "A closed half-space containing $\\epi f$ is characterized by a nonzero vector $(s,\\alpha)\\in\\R^n\\times\\R$ and a real number $b$ such that\n\\[\n\\langle s,x\\rangle+\\alpha r\\le b\\qquad\\text{for all }(x,r)\\in\\epi f\n\\tag{1.2.10}\n\\]\n(we equip the graph-space $\\R^n\\times\\R$ with the scalar product of a product-space). Let us denote by $\\Sigma\\subset\\R^n\\times\\R\\times\\R$ the index-set of such triples $\\sigma=(s,\\alpha,b)$, with corresponding half-space\n\\[\nH^-_{\\sigma}:=\\{(x,r):\\langle s,x\\rangle+\\alpha r\\le b\\}.\n\\tag{1.2.11}\n\\]\n\nIn other words, $\\operatorname{epi}(\\overline{cl}\\,f)=\\overline{\\operatorname{epi}f}=\\bigcap_{\\sigma\\in\\Sigma}H^-_{\\sigma}$.\n\nBecause of the particular nature of an epigraph, (1.2.10) implies $\\alpha\\le 0$ (let $r\\to +\\infty$) and, by positive homogeneity, the values $\\alpha=0$ and $\\alpha=-1$ suffice: $\\Sigma$ can be partitioned in\n\n\\[\n\\Sigma_1 := \\{(s,-1,b):(1.2.10)\\ \\text{holds with }\\alpha=-1\\}\n\\]\n\nand\n\n\\[\n\\Sigma_0 := \\{(s,0,b):(1.2.10)\\ \\text{holds with }\\alpha=0\\}.\n\\]\n\nIndeed, $\\Sigma_1$ corresponds to affine functions minorizing $f$ (Proposition 1.2.1 tells us that $\\Sigma_1\\neq\\varnothing$) and $\\Sigma_0$ to closed half-spaces of $\\mathbb{R}^{n}$ containing $\\operatorname{dom}f$ (note that $\\Sigma_0=\\varnothing$ if $\\operatorname{dom}f=\\mathbb{R}^n$).\n\nWe have to prove that, even when $\\Sigma_0\\neq\\varnothing$, intersecting the half-spaces $H^-_{\\sigma}$ over $\\Sigma$ or over $\\Sigma_1$ produces the same set, namely $\\operatorname{epi}f$. For this we take arbitrary $\\sigma_0=(s_0,0,b_0)\\in\\Sigma_0$ and $\\sigma_1=(s_1,-1,b_1)\\in\\Sigma_1$, we set\n\n\\[\n\\sigma(t):=(s_1+ts_0,-1,b_1+t b_0)\\in\\Sigma_1\\quad\\text{for all }t\\ge 0,\n\\]\n\nand we prove (see Fig. 1.2.1)\n\n\\[\nH^-_{\\sigma_0}\\cap H^-_{\\sigma_1}=\\bigcap_{t\\ge 0}H^-_{\\sigma(t)}=:\\,H^-.\n\\]\n\nFig. 1.2.1. Closing a convex epigraph\n\nIt results directly from the definition (1.2.11) that an $(x,r)$ in $H^-_{\\sigma_0}\\cap H^-_{\\sigma_1}$ satisfies\n\n\\[\n\\langle s_1+ts_0,x\\rangle-(b_1+tb_0)\\le r\\quad\\text{for all }t\\ge 0,\\tag{1.2.12}\n\\]\n\ni.e.\\ $(x,r)\\in H^-$. Conversely, take $(x,r)\\in H^-$. Set $t=0$ in (1.2.12) to see that $(x,r)\\in H^-_{\\sigma_1}$. Also, divide by $t>0$ and let $t\\to+\\infty$ to see that $(x,r)\\in H^-_{\\sigma_0}$. The proof is complete."
    },
    {
        "statement": "Let $C$ be a nonempty subset of $\\mathbb{R}^n\\times\\mathbb{R}$ satisfying \\eqref{1.3.2}, and let its lower-bound function $\\ell_C$ be defined by \\eqref{1.3.5}.\n\\begin{enumerate}[(i)]\n\\item If $C$ is convex, then $\\ell_C\\in\\operatorname{Conv}\\mathbb{R}^n$;\n\\item If $C$ is closed convex, then $\\ell_C\\in\\overline{\\operatorname{Conv}}\\mathbb{R}^n$.\n\\end{enumerate}",
        "title": "",
        "label": "thm:FCA-chapB-1.3.1",
        "lean_tag": [
            "FCA_chap_B_1_3_1"
        ],
        "lean_formalization": "lemma FCA_chap_B_1_3_1 {n : ℕ}\n  (C : Set (EuclideanSpace ℝ (Fin n) × ℝ))\n  (hC_nonempty : Nonempty C) (hC_minorized : ∀ x, minorizedAt C x)\n  : (Convex ℝ C → InConvRn (lowerBoundFunction C)) ∧\n    (Convex ℝ C ∧ IsClosed C → InClosedConvRn (lowerBoundFunction C))\n  := by sorry",
        "proof": "We use the analytical definition (1.1.1). Take arbitrary $\\varepsilon>0$, $\\alpha\\in]0,1[$ and $(x_i,r_i)\\in C$ such that $r_i\\le \\ell_C(x_i)+\\varepsilon$ for $i=1,2$.\n\nWhen $C$ is convex, $(\\alpha x_1+(1-\\alpha)x_2,\\alpha r_1+(1-\\alpha)r_2)\\in C$, hence\n\\[\n\\ell_C(\\alpha x_1+(1-\\alpha)x_2)\\le \\alpha r_1+(1-\\alpha)r_2 \\le \\alpha\\ell_C(x_1)+(1-\\alpha)\\ell_C(x_2)+\\varepsilon.\n\\]\n\nThe convexity of $\\ell_C$ follows, since $\\varepsilon>0$ was arbitrary; (i) is proved.\n\nNow take a sequence $(x_k,\\rho_k)_k\\subset\\operatorname{epi}\\ell_C$ converging to $(x,\\rho)$; we have to prove $\\ell_C(x)\\le \\rho$ (cf.\\ Proposition 1.2.2). By definition of $\\ell_C(x_k)$, we can select, for each positive integer $k$, a real number $r_k$ such that $(x_k,r_k)\\in C$ and\n\\begin{equation}\\label{eq:1.3.7}\n\\ell_C(x_k)\\le r_k\\le \\ell_C(x_k)+\\tfrac{1}{k}\\le \\rho_k+\\tfrac{1}{k}.\n\\end{equation}\n\nWe deduce first that $(r_k)$ is bounded from above. Also, when $\\ell_C$ is convex, Proposition 1.2.1 implies the existence of an affine function minorizing $\\ell_C$: $(r_k)$ is bounded from below.\n\nExtracting a subsequence if necessary, we may assume $r_k\\to r$. When $C$ is closed, $(x,r)\\in C$, hence $\\ell_C(x)\\le r$; but pass to the limit in \\eqref{eq:1.3.7} to see that $r\\le \\rho$; the proof is complete."
    },
    {
        "statement": "Let $f_1,\\dots,f_m$ be in $\\Conv\\R^n$ [resp.\\ in $\\overline{\\Conv}\\R^n$], let $t_1,\\dots,t_m$ be positive numbers, and assume that there is a point where all the $f_j$'s are finite. Then the function $f:=\\sum_{j=1}^m t_j f_j$ is in $\\Conv\\R^n$ [resp.\\ in $\\overline{\\Conv}\\R^n$].",
        "title": "",
        "label": "thm:FCA-chapB-2.1.1",
        "lean_tag": [
            "FCA_chap_B_2_1_1"
        ],
        "lean_formalization": "lemma FCA_chap_B_2_1_1 {n : ℕ} {m : ℕ}\n  (f : Fin m → (EuclideanSpace ℝ (Fin n) → WithTop ℝ)) (t : Fin m → ℝ)\n  (ht_positive : ∀ i, (t i) > 0)\n  : let g := fun x => ∑ i, (t i) * ((f i) x)\n    (∀ i, InConvRn (f i) → InConvRn g) ∧\n    (∀ i, InClosedConvRn (f i) → InClosedConvRn g)\n  := by sorry",
        "proof": "The convexity of $f$ is readily proved from the relation of definition (1.1.1). As for its closedness, start from\n\\[\n\\liminf_{y\\to x} t_j f_j(y)=t_j\\liminf_{y\\to x} f_j(y)\\ge t_j f_j(x)\n\\]\n(valid for $t_j>0$ and $f_j$ closed); then note that the lim inf of a sum is not smaller than the sum of lim inf's."
    },
    {
        "statement": "Let $\\{f_j\\}_{j\\in J}$ be an arbitrary family of convex [resp.\\ closed convex] functions. If there exists $x_0$ such that $\\sup_{j} f_j(x_0) < +\\infty$, then their pointwise supremum $f := \\sup\\{f_j : j\\in J\\}$ is in $\\mathrm{Conv}\\,\\mathbb{R}^n$ [resp.\\ in $\\mathrm{Conv}_\\mathrm{cl}\\,\\mathbb{R}^n$].",
        "title": "",
        "label": "prop:FCA-chapB-2.1.2",
        "lean_tag": [
            "FCA_chap_B_2_1_2"
        ],
        "lean_formalization": "lemma FCA_chap_B_2_1_2 {n : ℕ}\n  (J : Set ℕ) (f : ℕ → (EuclideanSpace ℝ (Fin n) → WithTop ℝ))\n  (hx₀ : ∃ x₀, sSup {y | ∃ j, (j ∈ J) ∧ ((f j) x₀ = y)} < ⊤)\n  : let g := fun x => sSup {y | ∃ j, (j ∈ J) ∧ ((f j) x = y)}\n    (∀ j ∈ J, ConvexOn ℝ Set.univ (f j) → InConvRn g) ∧\n    (∀ j ∈ J, ConvexOn ℝ Set.univ (f j) ∧ ∀ j ∈ J, ∀ x, (lscHull (f j)) x = (f j) x → InClosedConvRn g)\n  := by sorry",
        "proof": "The key property is that a supremum of functions corresponds to an intersection of epigraphs: $\\operatorname{epi} f = \\bigcap_{j\\in J}\\operatorname{epi} f_j$, which conserves convexity and closedness. The only needed restriction is nonemptiness of this intersection."
    },
    {
        "statement": "Let $f\\in\\operatorname{Conv}\\mathbb{R}^n$ [resp.\\ $\\overline{\\operatorname{Conv}}\\mathbb{R}^n$] and let $A$ be an affine mapping from $\\mathbb{R}^m$ to $\\mathbb{R}^n$ such that $\\operatorname{Im}A\\cap\\operatorname{dom}f\\neq\\varnothing$.  Then the function\n\\[\nf\\circ A : \\mathbb{R}^m \\supseteq x\\mapsto (f\\circ A)(x)=f(A(x))\n\\]\nis in $\\operatorname{Conv}\\mathbb{R}^m$ [resp.\\ $\\overline{\\operatorname{Conv}}\\mathbb{R}^m$].",
        "title": "",
        "label": "prop:FCA-chapB-2.1.4",
        "lean_tag": [
            "FCA_chap_B_2_1_4"
        ],
        "lean_formalization": "lemma FCA_chap_B_2_1_4 {m n : ℕ}\n  (f : EuclideanSpace ℝ (Fin n) → WithTop ℝ)\n  (A : AffineMap ℝ (EuclideanSpace ℝ (Fin m)) (EuclideanSpace ℝ (Fin n)))\n  (hf_nonempty : (Set.range A) ∩ (effDom (liftWithToptoEReal f)) ≠ ∅)\n  : let g := fun x => f (A x)\n    (InConvRn f → InConvRn g) ∧\n    (InClosedConvRn f → InClosedConvRn g)\n  := by sorry",
        "proof": "Clearly $(f\\circ A)(x)>-\\infty$ for all $x$; besides, there exists by assumption $y=A(x)\\in\\mathbb{R}^n$ such that $f(y)<+\\infty$. To check convexity, it suffices to plug the relation\n\\[\nA(\\alpha x+(1-\\alpha)x')=\\alpha A(x)+(1-\\alpha)A(x')\n\\]\ninto the analytical definition (1.1.1) of convexity. As for closedness, it comes readily from the continuity of $A$ when $f$ is itself closed."
    },
    {
        "statement": "If $f\\in\\operatorname{Conv}\\mathbb{R}^n$, its perspective $\\tilde f$ is in $\\operatorname{Conv}\\mathbb{R}^{n+1}$.",
        "title": "",
        "label": "prop:FCA-chapB-2.2.1",
        "lean_tag": [
            "FCA_chap_B_2_2_1"
        ],
        "lean_formalization": "lemma FCA_chap_B_2_2_1 {m n : ℕ}\n  (f : EuclideanSpace ℝ (Fin n) → WithTop ℝ) (hf_convex : InConvRn f)\n  : InConvRn (fun x => (perspective f) (x 0) (Matrix.vecTail x))\n  := by sorry",
        "proof": "Here also, it is better to look at $\\tilde f$ with ``geometric glasses'':\n\\[\n\\operatorname{epi}\\tilde f=\\{(u,x,r)\\in\\mathbb{R}_+^\\times\\times\\mathbb{R}^n\\times\\mathbb{R}:\\; f(x/u)\\le r/u\\}\n=\\{u(1,x',r'):\\;u>0,(x',r')\\in\\operatorname{epi}f\\}\n\\]\n\\[\n=\\bigcup_{u>0}u(\\{1\\}\\times\\operatorname{epi}f)=\\mathbb{R}_+^\\times(\\{1\\}\\times\\operatorname{epi}f)\n\\]\nand $\\operatorname{epi}\\tilde f$ is therefore a convex cone."
    },
    {
        "statement": "Let $f\\in\\operatorname{Conv}\\mathbb{R}^n$ and let $x'\\in\\operatorname{ri}\\dom f$. Then the closure $\\overline{f}$ of its perspective is given as follows:\n\\[\n(\\cl\\tilde f)(u,x)=\n\\begin{cases}\nu f(x/u) & \\text{if } u>0,\\\\[4pt]\n\\lim_{\\alpha\\downarrow0}\\alpha f(x'-x+\\tfrac{x}{\\alpha}) & \\text{if } u=0,\\\\[4pt]\n+\\infty & \\text{if } u<0.\n\\end{cases}\n\\]",
        "title": "",
        "label": "prop:FCA-chapB-2.2.2",
        "lean_tag": [
            "FCA_chap_B_2_2_2"
        ],
        "lean_formalization": "lemma FCA_chap_B_2_2_2 {m n : ℕ} [NeZero n]\n  (f : EuclideanSpace ℝ (Fin n) → WithTop ℝ) (x' : EuclideanSpace ℝ (Fin n))\n  (hf_convex : InClosedConvRn f) (hx : x' ∈ intrinsicInterior ℝ (effDom (liftWithToptoEReal f)))\n  : let pers := fun z => (perspective f) (z 0) (Matrix.vecTail z)\n    let x : EuclideanSpace ℝ (Fin (n + 1)) := (Matrix.vecCons 1 x')\n  ∀ (z : EuclideanSpace ℝ (Fin (n + 1))), (z 0 ≥ 0) →\n    Filter.Tendsto (fun α => pers (z + α • (x - z))) (𝓝[>] 0) (𝓝 (lscHull pers z))\n  := by sorry",
        "proof": "Suppose first $u<0$. For any $x$, it is clear that $(u,x)$ is outside $\\cl\\dom\\tilde f$ and, in view of (1.2.8), $\\cl\\tilde f(u,x)=+\\infty$.\n\nNow let $u\\ge 0$. Using (2.2.1), the assumption on $x'$ and the results of \\S A.2.1, we see that $(1,x')\\in\\ri\\dom\\tilde f$, so Proposition 1.2.5 allows us to write\n\\[\n(\\cl\\tilde f)(u,x)=\\lim_{\\alpha\\downarrow0}\\tilde f\\bigl((u,x)+\\alpha[(1,x')-(u,x)]\\bigr)\n\\]\n\\[\n\\qquad=\\lim_{\\alpha\\downarrow0}[u+\\alpha(1-u)]\\,f\\!\\Bigl(\\frac{x+\\alpha(x'-x)}{u+\\alpha(1-u)}\\Bigr).\n\\]\n\nIf $u=1$, this reads $\\cl\\tilde f(1,x)=\\cl f(x)=f(x)$ (because $f$ is closed); if $u=0$, we just obtain our claimed relation."
    },
    {
        "statement": "Let the functions $f_1$ and $f_2$ be in $\\operatorname{Conv}\\mathbb{R}^n$. Suppose that they have a common affine minorant: for some $(s,b)\\in\\mathbb{R}^n\\times\\mathbb{R}$,\n\\[\nf_j(x)\\ge\\langle s,x\\rangle - b\\qquad\\text{for }j=1,2\\text{ and all }x\\in\\mathbb{R}^n.\n\\]\nThen their infimal convolution is also in $\\operatorname{Conv}\\mathbb{R}^n$.",
        "title": "",
        "label": "prop:FCA-chapB-2.3.3",
        "lean_tag": [
            "FCA_chap_B_2_3_3"
        ],
        "lean_formalization": "lemma FCA_chap_B_2_3_3 {m n : ℕ}\n  (f₁ : EuclideanSpace ℝ (Fin n) → WithTop ℝ) (f₂ : EuclideanSpace ℝ (Fin n) → WithTop ℝ)\n  (hf_convex : InConvRn f₁ ∧ InConvRn f₂)\n  (h_common_affine_minorant : ∃ (s : EuclideanSpace ℝ (Fin n)) (b : ℝ),\n                              (∀ x, f₁ x ≥ (inner ℝ s x) - b) ∧ (∀ x, f₂ x ≥ (inner ℝ s x) - b))\n  : InProperConvRn (infimalConv f₁ f₂)\n  := by sorry",
        "proof": "For arbitrary $x\\in\\mathbb{R}^n$ and $x_1,x_2$ such that $x_1+x_2=x$, we have by assumption\n\\[\nf_1(x_1)+f_2(x_2)\\ge\\langle s,x\\rangle -2b>- \\infty,\n\\]\nand this inequality extends to the infimal value $(f_1\\infconv f_2)(x)$.\n\nOn the other hand, it suffices to choose particular values $x_j\\in\\dom f_j$, $j=1,2$, to obtain the point $x_1+x_2\\in\\dom(f_1\\infconv f_2)$. Finally, the convexity of $f_1\\infconv f_2$ results from the convexity of a lower-bound function, as seen in \\S 1.3(g)."
    },
    {
        "statement": "Let $g$ of Definition 2.4.1 be in $\\mathrm{Conv}\\,\\mathbb{R}^m$.  Assume also that, for all $x\\in\\mathbb{R}^n$, $g$ is bounded below on the inverse image $A^{-1}(x)=\\{y\\in\\mathbb{R}^m:Ay=x\\}$.  Then $Ag\\in\\mathrm{Conv}\\,\\mathbb{R}^n$.",
        "title": "",
        "label": "thm:FCA-chapB-2.4.2",
        "lean_tag": [
            "FCA_chap_B_2_4_2"
        ],
        "lean_formalization": "lemma FCA_chap_B_2_4_2 {m n : ℕ}\n  (A : (EuclideanSpace ℝ (Fin m)) →ₗ[ℝ] (EuclideanSpace ℝ (Fin n)))\n  (g : EuclideanSpace ℝ (Fin m) → WithBot (WithTop ℝ))\n  (hg_convex : InProperConvRn g) (hg_bounded : ∀ x, sInf (Set.image g {y | A y = x}) > ⊥)\n  : InProperConvRn (imageFun A g)\n  := by sorry",
        "proof": "By assumption, $Ag$ is nowhere $-\\infty$; also, $(Ag)(x)<+\\infty$ whenever $x=Ay$, with $y\\in\\dom g$.  Now consider the extended operator\n\\[\nA':\\mathbb{R}^m\\times\\mathbb{R}\\rightrightarrows(y,r)\\mapsto A'(y,r):=(Ay,r)\\in\\mathbb{R}^n\\times\\mathbb{R}.\n\\]\nThe set $A'(\\epi g)=:C$ is convex in $\\mathbb{R}^n\\times\\mathbb{R}$; let us compute its lower-bound function (1.3.5): for given $x\\in\\mathbb{R}^n$,\n\\[\n\\inf_{r}\\{r : (x,r)\\in C\\} = \\inf_{y,r}\\{r : Ay = x \\ \\text{and}\\ g(y)\\le r\\}\n= \\inf_{y}\\{g(y) : Ay = x\\} = (Ag)(x),\n\\]\nand this proves the convexity of $Ag=\\ell_C$."
    },
    {
        "statement": "Let (2.4.1) have the following form: $U=\\mathbb{R}^p$; $\\varphi\\in\\Conv\\mathbb{R}^p$; \n$X=\\mathbb{R}^n$ is equipped with the canonical basis; the mapping $c$ has its components \n$c_j\\in\\Conv\\mathbb{R}^p$ for $j=1,\\dots,n$. Suppose also that the optimal value is $>-\\infty$ for \nall $x\\in\\mathbb{R}^n$, and that\n\\[\n\\dom\\varphi\\cap\\dom c_1\\cap\\cdots\\cap\\dom c_n\\neq\\varnothing.\n\\tag{2.4.4}\n\\]\nThen the value function\n\\[\nv_{\\varphi,c}(x):=\\inf\\{\\varphi(u):c_j(u)\\le x_j,\\ \\text{for }j=1,\\dots,n\\}\n\\]\nlies in $\\Conv\\mathbb{R}^n$.",
        "title": "",
        "label": "thm:FCA-chapB-2.4.3",
        "lean_tag": [
            "FCA_chap_B_2_4_3"
        ],
        "lean_formalization": "lemma FCA_chap_B_2_4_3 {p n : ℕ}\n  (phi : EuclideanSpace ℝ (Fin p) → WithBot (WithTop ℝ))\n  (c : (Fin n) → (EuclideanSpace ℝ (Fin p) → WithBot (WithTop ℝ)))\n  (h_phi_convex : InProperConvRn phi) (hc_convex : ∀ j, InProperConvRn (c j))\n  (h_nonempty_dom : Set.Nonempty (effDom phi ∩ (⋂ j, effDom (c j))))\n  (h_noninf_val : ∀ x, (valueFun phi c x) > ⊥)\n  : InProperConvRn (valueFun phi c)\n  := by sorry",
        "proof": "Note first that we have assumed $v_{\\varphi,c}(x)>-\\infty$ for all $x$. Take $u_0$ in the set (2.4.4) and set $M:=\\max_j c_j(u_0)$; then take $x_0:=(M,\\dots,M)\\in\\mathbb{R}^n$, so that $v_{\\varphi,c}(x_0)\\le\\varphi(u_0)<+\\infty$. Knowing that $v_{\\varphi,c}$ is an image-function, we just have to prove the convexity of the set (2.4.3); but this in turn comes immediately from the convexity of each $c_j$."
    },
    {
        "statement": "With the above notation, suppose that $g$ is bounded below on the set $\\{x\\}\\times\\mathbb{R}^m$, for all $x\\in\\mathbb{R}^n$. Then the marginal function $\\gamma$ lies in $\\operatorname{Conv}\\mathbb{R}^n$.",
        "title": "",
        "label": "cor:FCA-chapB-2.4.5",
        "lean_tag": [
            "FCA_chap_B_2_4_5"
        ],
        "lean_formalization": "lemma FCA_chap_B_2_4_5 {n m : ℕ}\n  (g : EuclideanSpace ℝ (Fin (n + m)) → WithBot (WithTop ℝ))\n  (hg_convex : InProperConvRn g)\n  (hg_bounded : ∀ (x : EuclideanSpace ℝ (Fin n)), sInf (Set.image g {z : EuclideanSpace ℝ (Fin (n + m)) | ∃ (y : EuclideanSpace ℝ (Fin m)), z = Fin.append x y}) > ⊥)\n  : InProperConvRn (marginalFun g)\n  := by sorry",
        "proof": "The marginal function $\\gamma$ is the image of $g$ under the the linear operator $A$ projecting each $(x,y)\\in\\mathbb{R}^n\\times\\mathbb{R}^m$ onto $x\\in\\mathbb{R}^n$: $A(x,y)=x$. \\qquad \\qedhere"
    },
    {
        "statement": "Let $g:\\mathbb{R}^n\\to\\mathbb{R}\\cup\\{+\\infty\\}$, not identically $+\\infty$, be minorized by some affine function: for some $(s,b)\\in\\mathbb{R}^n\\times\\mathbb{R}$,\n\\[\ng(x)\\ge\\langle s,x\\rangle - b\\quad\\text{for all }x\\in\\mathbb{R}^n .\n\\tag{2.5.1}\n\\]\nThen, the following three functions \\(f_1, f_2\\) and \\(f_3\\) are convex and coincide on \\(\\mathbb{R}^n\\):\n\\[\n\\begin{aligned}\nf_1(x) &:= \\inf\\{r : (x,r)\\in\\operatorname{co}\\,\\operatorname{epi} g\\},\\\\\nf_2(x) &:= \\sup\\{h(x): h\\in\\operatorname{Conv}\\mathbb{R}^n,\\ h\\le g\\},\\\\\nf_3(x) &:= \\inf\\Big\\{\\sum_{j=1}^k \\alpha_j g(x_j): k=1,2,\\dots\\\\\n&\\qquad\\qquad\\qquad \\alpha\\in\\Delta_k,\\ x_j\\in\\operatorname{dom} g,\\ \\sum_{j=1}^k \\alpha_j x_j = x\\Big\\}.\n\\end{aligned}\n\\tag{2.5.2}\n\\]",
        "title": "",
        "label": "prop:FCA-chapB-2.5.1",
        "lean_tag": [
            "FCA_chap_B_2_5_1"
        ],
        "lean_formalization": "lemma FCA_chap_B_2_5_1 {n : ℕ}\n  (g : EuclideanSpace ℝ (Fin n) → WithTop ℝ)\n  (hg_minorized : ∃ (s : EuclideanSpace ℝ (Fin n)) (b : ℝ), ∀ x, g x ≥ inner ℝ s x - b)\n  : let f₁ := fun x => sInf {r : WithTop ℝ | ∃ z ∈ convexHull ℝ (epigraph (liftWithToptoEReal g)), z.1 = x ∧ z.2 = r}\n    let f₂ := fun x => sSup {z : WithTop ℝ | ∃ (h : EuclideanSpace ℝ (Fin n) → WithTop ℝ),\n                                               (InConvRn h) ∧ (Minorizes h g) ∧ (z = h x)}\n    let f₃ := fun x => sInf (⋃ k, {z | ∃ (α : EuclideanSpace ℝ (Fin k))\n                                          (x₀ : (Fin k) → EuclideanSpace ℝ (Fin n)),\n                                          (α ∈ Δκ k) ∧ (∀ j, x₀ j ∈ effDom (liftWithToptoEReal g)) ∧\n                                          (x = ∑ j, (α j) • (x₀ j)) ∧\n                                          (z = ∑ j, (α j) • g (x₀ j))})\n    (InConvRn f₁) ∧ (InConvRn f₂) ∧ (InConvRn f₃) ∧\n    (∀ x, (f₁ x = f₂ x) ∧ (f₂ x = f₃ x))\n  := by sorry",
        "proof": "We denote by \\(\\Gamma\\) the family of convex functions minorizing \\(g\\). By assumption, \\(\\Gamma\\neq\\varnothing\\); then the convexity of \\(f_1\\) results from \\S1.3(g).\n\n[\\(f_2\\le f_1\\)] Consider the epigraph of any \\(h\\in\\Gamma\\): its lower-bound function \\(\\ell_{\\mathrm{epi}\\,h}\\) is \\(h\\) itself; besides, it contains \\(\\mathrm{epi}\\,g\\), and \\(\\operatorname{co}(\\mathrm{epi}\\,g)\\) as well (see Proposition A.1.3.4). In a word, there holds \\(h=\\ell_{\\mathrm{epi}\\,h}\\le \\ell_{\\operatorname{co}\\,\\mathrm{epi}\\,g}=f_1\\) and we conclude \\(f_2\\le f_1\\) since \\(h\\) was arbitrary in \\(\\Gamma\\).\n\n[\\(f_3\\le f_2\\)] We have to prove \\(f_3\\in\\Gamma\\), and the result will follow by definition of \\(f_2\\); clearly \\(f_3\\le g\\) (take \\(\\alpha\\in\\Delta_1\\)!), so it suffices to establish \\(f_3\\le\\operatorname{Conv}\\mathbb{R}^n\\). First, with \\((s,b)\\) of (2.5.1) and all \\(x,\\ \\{x_j\\}\\) and \\(\\{\\alpha_j\\}\\) as described by (2.5.2),\n\\[\n\\sum_{j=1}^k \\alpha_j g(x_j) \\ge \\sum_{j=1}^k \\alpha_j(\\langle s,x_j\\rangle - b) = \\langle s,x\\rangle - b;\n\\]\nhence \\(f_3\\) is minorized by the affine function \\(\\langle s,\\cdot\\rangle - b\\). Now, take two points \\((x,r)\\) and \\((x',r')\\) in the strict epigraph of \\(f_3\\). By definition of \\(f_3\\), there are \\(k,\\ \\{\\alpha_j\\},\\ \\{x_j\\}\\) as described in (2.5.2), and likewise \\(k',\\ \\{\\alpha'_j\\},\\ \\{x'_j\\}\\), such that \\(\\sum_{j=1}^k \\alpha_j g(x_j)<r\\) and likewise \\(\\sum_{j=1}^{k'} \\alpha'_j g(x'_j)<r'\\).\n\nFor arbitrary \\(t\\in]0,1[\\), we obtain by convex combination\n\\[\n\\sum_{j=1}^k t\\alpha_j g(x_j) + \\sum_{j=1}^{k'} (1-t)\\alpha'_j g(x'_j) < tr + (1-t)r'.\n\\]\n\nObserve that\n\\[\n\\sum_{j=1}^k t\\alpha_j x_j + \\sum_{j=1}^{k'} (1-t)\\alpha'_j x'_j = tx + (1-t)x',\n\\]\ni.e. we have in the lefthand side a convex decomposition of \\(tx+(1-t)x'\\) in \\(k+k'\\) elements; therefore, by definition of \\(f_3\\):\n\\[\nf_3(tx+(1-t)x') \\le \\sum_{j=1}^k t\\alpha_j g(x_j) + \\sum_{j=1}^{k'} (1-t)\\alpha'_j g(x'_j)\n\\]\nand we have proved that \\(\\mathrm{epi}_s f_3\\) is a convex set: \\(f_3\\) is convex.\n\nLet \\(x\\in\\mathbb{R}^n\\) and take an arbitrary convex decomposition \\(x=\\sum_{j=1}^k \\alpha_j x_j\\),\nwith \\(\\alpha_j\\) and \\(x_j\\) as described in (2.5.2). Since \\((x_j,g(x_j))\\in\\operatorname{epi} g\\) for \\(j=1,\\ldots,k\\),\n\\[\n\\biggl(x,\\sum_{j=1}^k \\alpha_j g(x_j)\\biggr)\\in\\operatorname{co}\\operatorname{epi} g\n\\]\nand this implies \\(f_1(x)\\le\\sum_{j=1}^k \\alpha_j g(x_j)\\) by definition of \\(f_1\\). Because the decomposition of \\(x\\) was arbitrary within (2.5.2), this implies \\(f_1(x)\\le f_3(x)\\)."
    },
    {
        "statement": "Let $g$ satisfy the hypotheses of Proposition 2.5.1. Then the three functions below\n\\[\n\\bar f_1(x):=\\inf\\{r:(x,r)\\in\\overline{\\operatorname{epi} g}\\},\n\\qquad\n\\bar f_2(x):=\\sup\\{h(x):\\ h\\in\\operatorname{Conv}\\mathbb{R}^n,\\ h\\le g\\},\n\\]\n\\[\n\\bar f_3(x):=\\sup\\{\\langle s,x\\rangle-b:\\ \\langle s,y\\rangle-b\\le g(y)\\text{ for all }y\\in\\mathbb{R}^n\\}\n\\]\nare closed, convex, and coincide on $\\mathbb{R}^n$ with the closure of the function constructed in Proposition 2.5.1.",
        "title": "",
        "label": "prop:FCA-chapB-2.5.2",
        "lean_tag": [
            "FCA_chap_B_2_5_2"
        ],
        "lean_formalization": "lemma FCA_chap_B_2_5_2 {n : ℕ}\n  (g : EuclideanSpace ℝ (Fin n) → WithTop ℝ)\n  (hg_minorized : ∃ (s : EuclideanSpace ℝ (Fin n)) (b : ℝ), ∀ x, g x ≥ inner ℝ s x - b)\n  : let f₁ := fun x => sInf {r : WithTop ℝ | ∃ z ∈ closure (convexHull ℝ (epigraph (liftWithToptoEReal g))), z.1 = x ∧ z.2 = r}\n    let f₂ := fun x => sSup {z : WithTop ℝ | ∃ (h : EuclideanSpace ℝ (Fin n) → WithTop ℝ),\n                                               (InClosedConvRn h) ∧ (Minorizes h g) ∧ (z = h x)}\n    let f₃ := fun x => sSup {z : WithTop ℝ | ∃ (s : EuclideanSpace ℝ (Fin n)) (b : ℝ),\n                                               (∀ y, inner ℝ s y - b ≤ g y) ∧ (z = inner ℝ s x - b)}\n    (InClosedConvRn f₁) ∧ (InClosedConvRn f₂) ∧ (InClosedConvRn f₃) ∧\n    (∀ x, (f₁ x = f₂ x) ∧ (f₂ x = f₃ x))\n  := by sorry\n",
        "proof": ""
    },
    {
        "statement": "Let $g_1,\\dots,g_m$ be in $\\operatorname{Conv}\\mathbb{R}^n$, all minorized by the same affine function. Then the convex hull of their infimum is the function\n\\[\n\\mathbb{R}^n \\ni x \\mapsto [\\operatorname{co}(\\min_j g_j)](x)=\n\\inf\\Big\\{\\sum_{j=1}^m \\alpha_j g_j(x_j)\\;:\\;\\alpha\\in\\Delta_m,\\;x_j\\in\\operatorname{dom} g_j,\\;\\sum_{j=1}^m\\alpha_j x_j=x\\Big\\}.\n\\tag{2.5.3}\n\\]",
        "title": "",
        "label": "thm:FCA-chapB-2.5.4",
        "lean_tag": "no-lean-tag",
        "proof": "Apply Example A.1.3.5 to the convex sets $C_j=\\operatorname{epi} g_j$."
    },
    {
        "statement": "Let $f\\in\\operatorname{Conv}\\mathbb{R}^n$ and suppose there are $x_0,\\ \\delta,\\ m$ and $M$ such that\n\\[\nm\\le f(x)\\le M\\quad\\text{for all }x\\in B(x_0,2\\delta).\n\\]\nThen $f$ is Lipschitzian on $B(x_0,\\delta)$; more precisely: for all $y$ and $y'$ in $B(x_0,\\delta)$,\n\\begin{equation}\n\\label{eq:3.1.1}\n|f(y)-f(y')|\\le\\frac{M-m}{\\delta}\\|y-y'\\|.\n\\end{equation}",
        "title": "",
        "label": "lem:FCA-chapB-3.1.1",
        "lean_tag": "no-lean-tag",
        "proof": "Look at Fig.\\ 3.1.1: with two different $y$ and $y'$ in $B(x_0,\\delta)$, take\n\\[\ny'':=y'+\\delta\\,\\frac{y'-y}{\\|y'-y\\|}\\in B(x_0,2\\delta);\n\\]\n\nby construction, $y'$ lies on the segment $[y,y'']$, namely\n\\[\ny'=\\frac{\\|y'-y\\|}{\\delta+\\|y'-y\\|}\\,y''+\\frac{\\delta}{\\delta+\\|y'-y\\|}\\,y.\n\\]\nApplying the convexity of $f$ and using the postulated bounds, we obtain\n\\[\nf(y')-f(y)\\le\\frac{\\|y'-y\\|}{\\delta+\\|y'-y\\|}\\bigl[f(y'')-f(y)\\bigr]\\le\\frac{1}{\\delta}\\|y'-y\\|(M-m).\n\\]\nThen, it suffices to exchange $y$ and $y'$ to prove (3.1.1)."
    },
    {
        "statement": "With $f\\in\\operatorname{Conv}\\mathbb{R}^n$, let $S$ be a convex compact subset of \\(\\operatorname{ri}\\dom f\\). Then there exists \\(L=L(S)\\ge 0\\) such that\n\\[\n|f(x)-f(x')|\\le L\\|x-x'\\|\\quad\\text{for all }x\\text{ and }x'\\text{ in }S.\n\\tag{3.1.2}\n\\]",
        "title": "",
        "label": "thm:FCA-chapB-3.1.2",
        "lean_tag": "no-lean-tag",
        "proof": "[Preliminaries] First of all, our statement ignores \\(x\\)-values outside the affine hull of the convex set \\(\\dom f\\). Instead of \\(\\mathbb{R}^n\\), it can be formulated in \\(\\mathbb{R}^d\\), where \\(d\\) is the dimension of \\(\\dom f\\); alternatively, we may assume \\(\\ri\\dom f=\\int\\dom f\\), which will simplify the writing.\n\nMake this assumption and let \\(x_0\\in S\\). We will prove that there are \\(\\delta=\\delta(x_0)>0\\) and \\(L=L(x_0,\\delta)\\) such that the ball \\(B(x_0,\\delta)\\) is included in \\(\\int\\dom f\\) and\n\\[\n|f(y)-f(y')|\\le L\\|y-y'\\|\\quad\\text{for all }y\\text{ and }y'\\text{ in }B(x_0,\\delta).\n\\tag{3.1.3}\n\\]\n\nIf this holds for all \\(x_0\\in S\\), the corresponding balls \\(B(x_0,\\delta)\\) will provide a covering of the compact set \\(S\\), from which we will extract a finite covering \\((x_1,\\delta_1,L_1),\\ldots,(x_k,\\delta_k,L_k)\\). With these balls, we will divide an arbitrary segment \\([x,x']\\) of the convex set \\(S\\) into finitely many subsegments, of endpoints \\(y_0:=x,\\ldots,y_i,\\ldots,y_\\ell:=x'\\). Ordering properly the \\(y_i\\)'s, we will have \\(\\|x-x'\\|=\\sum_{i=1}^\\ell\\|y_i-y_{i-1}\\|\\); furthermore, \\(f\\) will be Lipschitzian on each \\([y_{i-1},y_i]\\) with the common constant \\(L:=\\max\\{L_1,\\ldots,L_k\\}\\). The required Lipschitz property (3.1.2) will follow.\n\n[Main Step] To establish (3.1.3), we use Lemma 3.1.1, which requires boundedness of $f$ in the neighborhood of $x_0$. For this, we construct as in the proof of Theorem A.2.1.3 (see Fig.\\ A.2.1.1) a simplex $\\Delta=\\operatorname{co}\\{v_0,\\dots,v_n\\}\\cap\\operatorname{dom}f$ having $x_0$ in its interior: we can take $\\delta>0$ such that $B(x_0,2\\delta)\\subset\\Delta$.\n\nThen any $y\\in B(x_0,2\\delta)$ can be written: $y=\\sum_{i=0}^n\\alpha_i v_i$ with $\\alpha\\in\\Delta_{n+1}$, so that the convexity of $f$ gives\n\\[\nf(y)\\le\\sum_{i=0}^n\\alpha_i f(v_i)\\le\\max\\{f(v_0),\\dots,f(v_n)\\} =: M .\n\\]\n\nOn the other hand, Proposition 1.2.1 tells us that $f$ is bounded from below, say by $m$, on this very same $B(x_0,2\\delta)$. Our claim is proved: we have singled out $\\delta>0$ such that $m\\le f(y)\\le M$ for all $y\\in B(x_0,2\\delta)$."
    },
    {
        "statement": "Let the convex functions $f_k:\\mathbb{R}^n\\to\\mathbb{R}$ converge pointwise for $k\\to+\\infty$ to $f:\\mathbb{R}^n\\to\\mathbb{R}$. Then $f$ is convex and, for each compact set $S$, the convergence of $f_k$ to $f$ is uniform on $S$.",
        "title": "",
        "label": "thm:FCA-chapB-3.1.4",
        "lean_tag": "no-lean-tag",
        "proof": "Convexity of $f$ is trivial: pass to the limit in the definition (1.1.1) itself. For uniformity, we want to use Lemma 3.1.1, so we need to bound $f_k$ on $S$ independently of $k$; thus, let $r>0$ be such that $S\\subset B(0,r)$.\n\n[Step 1] First the function $g:=\\sup_k f_k$ is convex, and $g(x)<+\\infty$ for all $x$ because the convergent sequence $(f_k(x))_k$ is certainly bounded. Hence, $g$ is continuous and therefore bounded, say by $M$, on the compact set $B(0,2r)$:\n\\[\nf_k(x)\\le g(x)\\le M\\qquad\\text{for all $k$ and all $x\\in B(0,2r)$.}\n\\]\n\nSecond, the convergent sequence $(f_k(0))_k$ is bounded from below:\n\\[\n\\mu\\le f_k(0)\\qquad\\text{for all $k$.}\n\\]\n\nThen, for $x\\in B(0,2r)$ and all $k$, use convexity on $[-x,x]\\subset B(0,2r)$:\n\\[\n2\\mu\\le 2f_k(0)\\le f_k(x)+f_k(-x)\\le f_k(x)+M,\n\\]\ni.e.\\ the $f_k$'s are bounded from below, independently of $k$. Thus, we are within the conditions of Lemma 3.1.1: there is some $L$ (independent of $k$) such that\n\\[\n|f_k(y)-f_k(y')| \\le L\\|y-y'\\| \\quad\\text{for all }k\\text{ and all }y,y'\\in B(0,r). \\tag{3.1.5}\n\\]\n\nNaturally, the same Lipschitz property is transmitted to the limiting function $f$.\n\n[Step 2] Now fix $\\varepsilon>0$. Cover $S$ by the balls $B(x,\\varepsilon)$ for $x$ describing $S$, and extract a finite covering $S\\subset B(x_1,\\varepsilon)\\cup\\cdots\\cup B(x_m,\\varepsilon)$. With $x$ arbitrary in $S$, take an $x_i$ such that $x\\in B(x_i,\\varepsilon)$. There is $k_{i,\\varepsilon}$ such that, for all $k\\ge k_{i,\\varepsilon}$,\n\\[\n|f_k(x)-f(x)| \\le |f_k(x)-f_k(x_i)|+|f_k(x_i)-f(x_i)|+|f(x_i)-f(x)| \\le (2L+1)\\varepsilon\n\\]\nwhere we have also used (3.1.5), knowing that $x$ and $x_i$ are in $S\\subset B(0,r)$. The above inequality is then valid uniformly in $x$, providing that\n\\[\nk\\ge \\max\\{k_{1,\\varepsilon},\\ldots,k_{m,\\varepsilon}\\} =: k_\\varepsilon.\n\\]"
    },
    {
        "statement": "For $f\\in\\overline{\\operatorname{Conv}}\\mathbb{R}^n$, the asymptotic cone of $\\operatorname{epi} f$ is the epigraph of the function $f'_{\\infty}\\in\\overline{\\operatorname{Conv}}\\mathbb{R}^n$ defined by\n\\[\nd\\mapsto f'_{\\infty}(d):=\\sup_{t>0}\\frac{f(x_0+td)-f(x_0)}{t}\n\\;=\\;\n\\lim_{t\\to+\\infty}\\frac{f(x_0+td)-f(x_0)}{t},\n\\tag{3.2.2}\n\\]\nwhere $x_0$ is arbitrary in $\\operatorname{dom} f$.",
        "title": "",
        "label": "prop:FCA-chapB-3.2.1",
        "lean_tag": "no-lean-tag",
        "proof": "Since $(x_0,f(x_0))\\in\\operatorname{epi} f$, (3.2.1) tells us that $(d,\\rho)\\in(\\operatorname{epi} f)_{\\infty}$ if and only if $f(x_0+td)\\le f(x_0)+t\\rho$ for all $t>0$, which means\n\n\\[\n\\sup_{t>0}\\frac{f(x_0+td)-f(x_0)}{t}\\le \\rho. \\tag{3.2.3}\n\\]\n\nIn other words, $(\\operatorname{epi} f)_\\infty$ is the epigraph of the function whose value at $d$ is the left hand side of (3.2.3); and this is true no matter how $x_0$ has been chosen in \\(\\operatorname{dom} f\\). The rest follows from the fact that the difference quotient in (3.2.3) is closed convex in \\(d\\), and increasing in \\(t\\) (the function \\(t\\mapsto f(x_0+td)\\) is convex and enjoys the property of increasing slopes, namely Proposition 0.6.1). \\(\\square\\)"
    },
    {
        "statement": "A function $f\\in\\operatorname{Conv}\\mathbb{R}^n$ is Lipschitzian on the whole of $\\mathbb{R}^n$ if and only if $f'_\\infty$ is finite on the whole of $\\mathbb{R}^n$. The best Lipschitz constant for $f$ is then\n\\[\n\\sup\\{f'_\\infty(d):\\|d\\|=1\\}.\n\\tag{3.2.4}\n\\]",
        "title": "",
        "label": "prop:FCA-chapB-3.2.6",
        "lean_tag": "no-lean-tag",
        "proof": "When the (convex) function $f'_\\infty$ is finite-valued, it is continuous (\\S3.1) and therefore bounded on the compact unit sphere:\n\\[\n\\sup\\{f'_\\infty(d):\\|d\\|=1\\} =: L < +\\infty,\n\\]\nwhich implies by positive homogeneity\n\\[\nf'_\\infty(d) \\le L\\|d\\|\\quad\\text{for all }d\\in\\mathbb{R}^n.\n\\]\nNow use the definition (3.2.2) of $f'_\\infty$:\n\\[\nf(x+d)-f(x)\\le L\\|d\\|\\quad\\text{for all }x\\in\\operatorname{dom}f\\text{ and }d\\in\\mathbb{R}^n;\n\\]\nthus, $\\operatorname{dom}f$ is the whole space ($f(x+d)<+\\infty$ for all $d$) and we do obtain that $L$ is a global Lipschitz constant for $f$.\n\nConversely, let $f$ have a global Lipschitz constant $L$. Pick $x_0\\in\\operatorname{dom}f$ and plug the inequality\n\\[\nf(x_0+td)-f(x_0)\\le Lt\\|d\\|\\quad\\text{for all }t>0\\text{ and }d\\in\\mathbb{R}^n\n\\]\ninto the definition (3.2.2) of $f'_\\infty$ to obtain $f'_\\infty(d)\\le L\\|d\\|$ for all $d\\in\\mathbb{R}^n$.\n\nIt follows that $f'_\\infty$ is finite everywhere, and the value (3.2.4) does not exceed $L$."
    },
    {
        "statement": "\\begin{enumerate}\n\\item Let $f_1,\\dots,f_m$ be $m$ functions of $\\Conv\\R^n$, and $t_1,\\dots,t_m$ be positive numbers. Assume that there is $x_0$ at which each $f_j$ is finite. Then,\n\\[\n\\text{for } f:=\\sum_{j=1}^m t_j f_j,\\qquad\\text{we have } f'_\\infty=\\sum_{j=1}^m t_j(f_j)'_\\infty.\n\\]\n\n\\item Let $\\{f_j\\}_{j\\in J}$ be a family of functions in $\\Conv\\R^n$. Assume that there is $x_0$ at which $\\sup_{j\\in J} f_j(x_0)<+\\infty$. Then,\n\\[\n\\text{for } f:=\\sup_{j\\in J} f_j,\\qquad\\text{we have } f'_\\infty=\\sup_{j\\in J}(f_j)'_\\infty.\n\\]\n\n\\item Let $A:\\R^n\\to\\R^m$ be affine with linear part $A_0$, and let $f\\in\\Conv\\R^m$. Assume that $A(\\R^n)\\cap\\dom f\\neq\\emptyset$. Then $(f\\circ A)'_\\infty=f'_\\infty\\circ A_0$.\n\\end{enumerate}",
        "title": "3.2.8",
        "label": "thm:FCA-chapB-3.2.8",
        "lean_tag": "no-lean-tag",
        "proof": ""
    },
    {
        "statement": "Let $f$ be a function differentiable on an open set $\\Omega\\subset\\mathbb{R}^n$, and let $C$ be a convex subset of $\\Omega$. Then\n\\begin{enumerate}\n\\item $f$ is convex on $C$ if and only if\n\\[\nf(x)\\ge f(x_0)+\\langle\\nabla f(x_0),\\,x-x_0\\rangle\\quad\\text{for all }(x_0,x)\\in C\\times C;\n\\tag{4.1.1}\n\\]\n\n\\item $f$ is strictly convex on $C$ if and only if strict inequality holds in (4.1.1) whenever $x\\neq x_0$;\n\n\\item $f$ is strongly convex with modulus $c$ on $C$ if and only if, for all $(x_0,x)\\in C\\times C$,\n\\[\nf(x)\\ge f(x_0)+\\langle\\nabla f(x_0),x-x_0\\rangle+\\tfrac{1}{2}c\\|x-x_0\\|^2.\n\\tag{4.1.2}\n\\]\n\\end{enumerate}",
        "title": "",
        "label": "thm:FCA-chapB-4.1.1",
        "lean_tag": "no-lean-tag",
        "proof": "[(i)] Let $f$ be convex on $C$: for arbitrary $(x_0,x)\\in C\\times C$ and $\\alpha\\in ]0,1[$, we have from the definition (1.1.1) of convexity\n\\[\nf(\\alpha x+(1-\\alpha)x_0)-f(x_0)\\le\\alpha[f(x)-f(x_0)].\n\\]\nDivide by $\\alpha$ and let $\\alpha\\downarrow0$: observing that $\\alpha x+(1-\\alpha)x_0=x_0+\\alpha(x-x_0)$, the lefthand side tends to $\\langle\\nabla f(x_0),x-x_0\\rangle$ and (4.1.1) is established.\n\nConversely, take $x_1$ and $x_2$ in $C$, $\\alpha\\in ]0,1[$ and set $x_0:=\\alpha x_1+(1-\\alpha)x_2\\in C$. By assumption,\n\\[\nf(x_i)\\ge f(x_0)+\\langle\\nabla f(x_0),x_i-x_0\\rangle\\quad\\text{for }i=1,2\n\\tag{4.1.3}\n\\]\nand we obtain by convex combination\n\\[\n\\alpha f(x_1)+(1-\\alpha)f(x_2)\\ge f(x_0)+\\langle\\nabla f(x_0),\\alpha x_1+(1-\\alpha)x_2-x_0\\rangle\n\\]\nwhich, after simplification, is just the relation of definition (1.1.1).\n\n[(ii)] If $f$ is strictly convex, we have for $x_0\\ne x$ in $C$ and $\\alpha\\in ]0,1[$,\n\\[\nf(x_0+\\alpha(x-x_0))-f(x_0)<\\alpha[f(x)-f(x_0)];\n\\]\nbut $f$ is in particular convex and we can use (i):\n\\[\n\\langle\\nabla f(x_0),\\alpha(x-x_0)\\rangle\\le f(x_0+\\alpha(x-x_0))-f(x_0),\n\\]\nso the required strict inequality follows.\n\nFor the converse, proceed as for (i), starting from strict inequalities in (4.1.3).\n\n[(iii)] Using Proposition 1.1.2, just apply (i) to the function $f-\\tfrac{1}{2}c\\|\\cdot\\|^2$, which is of course differentiable."
    },
    {
        "statement": "Let $f$ be a function differentiable on an open set $\\Omega\\subset\\mathbb{R}^n$, and let $C$ be a convex subset of $\\Omega$. Then, $f$ is convex [resp.\\ strictly convex, resp.\\ strongly convex with modulus $c$] on $C$ if and only if its gradient $\\nabla f$ is monotone [resp.\\ strictly monotone, resp.\\ strongly monotone with modulus $c$] on $C$.",
        "title": "",
        "label": "thm:FCA-chapB-4.1.4",
        "lean_tag": "no-lean-tag",
        "proof": "We combine the “convex $\\Leftrightarrow$ monotone” and “strongly convex $\\Leftrightarrow$ strongly monotone” cases by accepting the value $c=0$ in the relevant relations such as (4.1.2).\n\nThus, let $f$ be [strongly] convex on $C$: in view of Theorem 4.1.1, we can write for arbitrary $x_0$ and $x$ in $C$:\n\\[\nf(x)\\ge f(x_0)+\\langle\\nabla f(x_0),x-x_0\\rangle+\\tfrac{1}{2}c\\|x-x_0\\|^2\n\\]\n\\[\nf(x_0)\\ge f(x)+\\langle\\nabla f(x),x_0-x\\rangle+\\tfrac{1}{2}c\\|x_0-x\\|^2,\n\\]\nand mere addition shows that $\\nabla f$ is [strongly] monotone.\n\nConversely, let $(x_0,x_1)$ be a pair of elements in $C$. Consider the univariate function $t\\mapsto\\varphi(t):=f(x_t)$, where $x_t:=x_0+t(x_1-x_0)$; for $t$ in an open interval containing $[0,1]$, $x_t\\in\\Omega$ and $\\varphi$ is well-defined and differentiable; its derivative at $t$ is $\\varphi'(t)=\\langle\\nabla f(x_t),x_1-x_0\\rangle$. Thus, we have for all $0\\le t'<t\\le1$\n\\[\n\\varphi'(t)-\\varphi'(t')=\\langle\\nabla f(x_t)-\\nabla f(x_{t'}),\\,x_1-x_0\\rangle\n=\\frac{1}{t-t'}\\langle\\nabla f(x_t)-\\nabla f(x_{t'}),\\,x_t-x_{t'}\\rangle\\tag{4.1.4}\n\\]\nand the monotonicity relation for $\\nabla f$ shows that $\\varphi'$ is increasing, $\\varphi$ is therefore convex (Corollary 0.6.5).\n\nFor strong convexity, set $t'=0$ in (4.1.4) and use the strong monotonicity relation for $\\nabla f$:\n\\[\n\\varphi'(t)-\\varphi'(0)\\ge\\frac{1}{t}c\\|x_t-x_0\\|^2 = tc\\|x_1-x_0\\|^2. \\tag{4.1.5}\n\\]\n\nBecause the differentiable convex function $\\varphi$ is the integral of its derivative, we can write\n\\[\n\\varphi(1)-\\varphi(0)-\\varphi'(0)=\\int_0^1[\\varphi'(t)-\\varphi'(0)]\\,dt \\ge \\tfrac{1}{2}c\\|x_1-x_0\\|^2\n\\]\nwhich, by definition of $\\varphi$, is just (4.1.2) (the coefficient $1/2$ is $\\int_0^1 t\\,dt!$).\n\nThe same technique proves the ``strictly monotone \\(\\Leftrightarrow\\) strictly convex'' case; then, (4.1.5) becomes a strict inequality --- with $c=0$ --- and remains so after integration."
    },
    {
        "statement": "For $f\\in\\operatorname{Conv}\\mathbb{R}^n$ and $x\\in\\operatorname{int}\\dom f$, the three statements below are equivalent:\n\\begin{enumerate}\n\\item The function\n\\[\n\\mathbb{R}^n\\ni d\\mapsto\\lim_{t\\downarrow 0}\\frac{f(x+td)-f(x)}{t}\n\\]\nis linear in $d$;\n\\item for some basis of $\\mathbb{R}^n$ in which $x=(\\xi^1,\\dots,\\xi^n)$, the partial derivatives $\\dfrac{\\partial f}{\\partial \\xi^i}(x)$ exist at $x$, for $i=1,\\dots,n$;\n\\item $f$ is differentiable at $x$.\n\\end{enumerate}",
        "title": "",
        "label": "prop:FCA-chapB-4.2.1",
        "lean_tag": "no-lean-tag",
        "proof": "First of all, remember from Theorem 0.6.3 that the one-dimensional function $t\\mapsto f(x+td)$ has half-derivatives at $0$: the limits considered in (i) exist for all $d$. We will denote by $\\{b_1,\\dots,b_n\\}$ the basis postulated in (ii), so that $x=\\sum_{i=1}^n \\xi^i b_i$.\n\nDenote by $d\\mapsto \\ell(d)$ the function defined in (i); taking $d=\\pm b_i$, realize that, when (i) holds,\n\\[\n\\lim_{\\tau\\downarrow 0}\\frac{f(x+\\tau b_i)-f(x)}{-\\tau}\n=\\ell(-b_i)=-\\ell(b_i)=-\\lim_{t\\downarrow 0}\\frac{f(x+t b_i)-f(x)}{t}.\n\\]\n\nThis means that the two half-derivatives at $t=0$ of the function $t\\mapsto f(x+t b_i)$ coincide: the partial derivative of $f$ at $x$ along $b_i$ exists, (ii) holds. That (iii) implies (i) is clear: when $f$ is differentiable at $x$,\n\\[\n\\lim_{t\\downarrow 0}\\frac{f(x+td)-f(x)}{t}=\\langle\\nabla f(x),d\\rangle.\n\\]\n\nWe do not really complete the proof here, because everything follows in a straightforward way from subsequent chapters. More precisely, [(ii) $\\Rightarrow$ (i)] is Proposition C.1.1.6, which states that the function $\\ell$ is linear on the space generated by the $b_i$'s, whenever it is linear along each $b_i$. Finally [(i) $\\Rightarrow$ (iii)] results from Lemma D.2.1.1 and the proof goes as follows. One of the possible definitions of (iii) is:\n\\[\n\\lim_{t\\downarrow 0,\\;d'\\to d}\\frac{f(x+t d')-f(x)}{t}\\quad\\text{is linear in }d.\n\\]\n\nBecause $f$ is locally Lipschitzian, the above limit exists whenever it exists for fixed $d'=d$---i.e.\\ the expression in (i)."
    },
    {
        "statement": "Let $f\\in\\operatorname{Conv}\\mathbb{R}^n$. The subset of $\\operatorname{int}\\operatorname{dom}f$ where $f$ fails to be differentiable is of zero (Lebesgue) measure.",
        "title": "",
        "label": "thm:FCA-chapB-4.2.3",
        "lean_tag": "no-lean-tag",
        "proof": ""
    },
    {
        "statement": "Let $f$ be twice differentiable on an open convex set $\\Omega\\subset\\mathbb{R}^n$. Then\n\\begin{enumerate}\n\\item[(i)] $f$ is convex on $\\Omega$ if and only if $\\nabla^2 f(x_0)$ is positive semi-definite for all $x_0\\in\\Omega$;\n\\item[(ii)] if $\\nabla^2 f(x_0)$ is positive definite for all $x_0\\in\\Omega$, then $f$ is strictly convex on $\\Omega$;\n\\item[(iii)] $f$ is strongly convex with modulus $c$ on $\\Omega$ if and only if the smallest eigenvalue of $\\nabla^2 f(\\cdot)$ is minorized by $c$ on $\\Omega$: for all $x_0\\in\\Omega$ and all $d\\in\\mathbb{R}^n$,\n\\[\n\\langle\\nabla^2 f(x_0)d,d\\rangle\\ge c\\|d\\|^2.\n\\]\n\\end{enumerate}",
        "title": "",
        "label": "thm:FCA-chapB-4.3.1",
        "lean_tag": "no-lean-tag",
        "proof": "For given $x_0\\in\\Omega$, $d\\in\\mathbb{R}^n$ and $t\\in\\mathbb{R}$ such that $x_0+td\\in\\Omega$, we will set\n\\[\nx_t:=x_0+td\\quad\\text{and}\\quad \\varphi(t):=f(x_t)=f(x+td),\n\\]\nso that $\\varphi''(t)=\\langle\\nabla^2 f(x_t)d,d\\rangle$.\n\n[(i)] Assume $f$ is convex on $\\Omega$; let $(x_0,d)$ be arbitrary in $\\Omega\\times\\mathbb{R}^n$, with $d\\neq 0$: $\\varphi$ is then convex on the open interval $I:=\\{t\\in\\mathbb{R}:x_0+td\\in\\Omega\\}$. It follows\n\\[\n0 \\le \\varphi''(t) = \\langle \\nabla^2 f(x_t)d,d\\rangle\\quad\\text{for all }t\\in I\\ni 0\n\\]\nand $\\nabla^2 f(x_0)$ is positive semi-definite.\n\nConversely, take an arbitrary $[x_0,x_1]\\subset \\Omega$, set $d:=x_1-x_0$ and assume $\\nabla^2 f(x_t)$ positive semi-definite, i.e. $\\varphi''(t)\\ge 0$, for $t\\in[0,1]$. Then Theorem 0.6.6 tells us that $\\varphi$ is convex on $[0,1]$, i.e. $f$ is convex on $[x_0,x_1]$. The result follows since $x_0$ and $x_1$ were arbitrary in $\\Omega$.\n\n[(ii)] To establish the strict convexity of $f$ on $\\Omega$, we prove that $\\nabla f$ is strictly monotone on $\\Omega$: Theorem 4.1.4 will apply. As above, take an arbitrary $[x_0,x_1]\\subset \\Omega$, $x_1\\neq x_0$, $d:=x_1-x_0$, and apply the mean-value theorem to the function $\\varphi'$, differentiable on $[0,1]$, for some $\\tau\\in]0,1[$,\n\\[\n\\varphi'(1)-\\varphi'(0)=\\varphi''(\\tau)=\\langle \\nabla^2 f(x_\\tau)d,d\\rangle>0\n\\]\nand the result follows since\n\\[\n\\varphi'(1)-\\varphi'(0)=\\langle \\nabla f(x_1)-\\nabla f(x_0),x_1-x_0\\rangle.\n\\]\n\n[(iii)] Using Proposition 1.1.2, apply (i) to the function $f-{\\tfrac12}c\\|\\,\\cdot\\,\\|^2$, whose Hessian operator is $\\nabla^2 f-cI_n$ and has the eigenvalues $\\lambda-c$, with $\\lambda$ describing the eigenvalues of $\\nabla^2 f$."
    },
    {
        "statement": "A function \\(\\sigma:\\mathbb R^n\\to\\mathbb R\\cup\\{+\\infty\\}\\) is sublinear if and only if its epigraph \\(\\operatorname{epi}\\sigma\\) is a nonempty convex cone in \\(\\mathbb R^n\\times\\mathbb R\\).",
        "title": "",
        "label": "prop:FCA-chapC-1.1.3",
        "lean_tag": [
            "FCA_chap_C_1_1_3"
        ],
        "lean_formalization": "lemma FCA_chap_C_1_1_3 {n : ℕ}\n  (σ : EuclideanSpace ℝ (Fin n) → WithTop ℝ) :\n  let epi := epigraph (liftWithToptoEReal σ)\n  (IsSublinear (liftWithToptoEReal σ)) ↔\n  (Set.Nonempty epi ∧ Convex ℝ epi ∧ IsCone epi) := by\n  sorry",
        "proof": "We know that \\(\\sigma\\) is a convex function if and only if \\(\\operatorname{epi}\\sigma\\) is a nonempty convex set in \\(\\mathbb R^n\\times\\mathbb R\\) (Proposition B.1.1.6). Therefore, we just have to prove the equivalence between positive homogeneity and \\(\\operatorname{epi}\\sigma\\) being a cone.\n\nLet \\(\\sigma\\) be positively homogeneous. For \\((x,r)\\in\\operatorname{epi}\\sigma\\), the relation \\(\\sigma(x)\\le r\\) gives\n\\[\n\\sigma(tx)=t\\sigma(x)\\le tr\\qquad\\text{for all }t>0,\n\\]\nso \\(\\operatorname{epi}\\sigma\\) is a cone. Conversely, if \\(\\operatorname{epi}\\sigma\\) is a cone in \\(\\mathbb R^n\\times\\mathbb R\\), the property \\((x,\\sigma(x))\\in\\operatorname{epi}\\sigma\\) implies \\((tx,t\\sigma(x))\\in\\operatorname{epi}\\sigma\\), i.e.\n\\[\n\\sigma(tx)\\le t\\sigma(x)\\qquad\\text{for all }t>0.\n\\]\n\nFrom Remark 1.1.2, this is just positive homogeneity."
    },
    {
        "statement": "A function $\\sigma:\\mathbb{R}^n\\to\\mathbb{R}\\cup\\{+\\infty\\}$, not identically equal to $+\\infty$, is sublinear if and only if one of the following two properties holds:\n\\[\n\\sigma(t_1x_1+t_2x_2)\\le t_1\\sigma(x_1)+t_2\\sigma(x_2)\\qquad\\text{for all }x_1,x_2\\in\\mathbb{R}^n\\text{ and }t_1,t_2>0,\n\\tag{1.1.4}\n\\]\nor\n\\[\n\\sigma\\text{ is positively homogeneous and subadditive}.\n\\tag{1.1.5}\n\\]",
        "title": "",
        "label": "prop:FCA-chapC-1.1.4",
        "lean_tag": [
            "FCA_chap_C_1_1_4"
        ],
        "lean_formalization": "lemma FCA_chap_C_1_1_4 {n : ℕ}\n  (σ : EuclideanSpace ℝ (Fin n) → WithTop ℝ)\n  (hσ : ∃ x, σ x ≠ ⊤) :\n  (IsSublinear (liftWithToptoEReal σ)) ↔\n  ((∀ (x₁ x₂ : EuclideanSpace ℝ (Fin n)), ∀ (t₁ t₂ : ℝ),\n   t₁ > 0 → t₂ > 0 → σ (t₁ • x₁ + t₂ • x₂) ≤ t₁ • (σ x₁) + t₂ • (σ x₂)) ∨\n   ((IsKPosHomogeneous σ 1) ∧ (IsSubadditive σ))) := by\n  sorry",
        "proof": "[sublinearity $\\implies$ (1.1.4)] For $x_1,x_2\\in\\mathbb{R}^n$ and $t_1,t_2>0$, set $t:=t_1+t_2>0$; we have\n\\[\n\\begin{aligned}\n\\sigma(t_1x_1+t_2x_2)&=\\sigma\\bigl(t\\bigl[\\tfrac{t_1}{t}x_1+\\tfrac{t_2}{t}x_2\\bigr]\\bigr)\\\\\n&=t\\sigma\\bigl(\\tfrac{t_1}{t}x_1+\\tfrac{t_2}{t}x_2\\bigr)\\qquad\\text{[positive homogeneity]}\\\\\n&\\le t\\bigl[\\tfrac{t_1}{t}\\sigma(x_1)+\\tfrac{t_2}{t}\\sigma(x_2)\\bigr]\\qquad\\text{[convexity]},\n\\end{aligned}\n\\]\nand (1.1.4) is proved.\n\n[(1.1.4) $\\implies$ (1.1.5)] A function satisfying (1.1.4) is obviously subadditive (take $t_1=t_2=1$) and satisfies (take $x_1=x_2=x$, $t_1=t_2=1/2t$)\n\\[\n\\sigma(tx)=t\\sigma(x),\n\\]\ni.e. it is positively homogeneous.\n\n[(1.1.5)$\\Rightarrow\\text{sublinearity}$] Take $t_1,t_2>0$ with $t_1+t_2=1$ and apply successively subadditivity and positive homogeneity:\n\\[\n\\sigma(t_1x_1+t_2x_2)\\le\\sigma(t_1x_1)+\\sigma(t_2x_2)=t_1\\sigma(x_1)+t_2\\sigma(x_2),\n\\]\nhence $\\sigma$ is convex."
    },
    {
        "statement": "If $\\sigma$ is sublinear, then\n\\[\n\\sigma(x)+\\sigma(-x)\\ge 0\\quad\\text{for all }x\\in\\mathbb{R}^n.\n\\tag{1.1.6}\n\\]",
        "title": "",
        "label": "cor:FCA-chapC-1.1.5",
        "lean_tag": [
            "FCA_chap_C_1_1_5"
        ],
        "lean_formalization": "lemma FCA_chap_C_1_1_5 {n : ℕ}\n  (σ : EuclideanSpace ℝ (Fin n) → WithTop ℝ) :\n  (IsSublinear (liftWithToptoEReal σ)) →\n  (∀ (x : EuclideanSpace ℝ (Fin n)), σ x + σ (-x) ≥ 0) := by\n  sorry",
        "proof": "Take $x_2=-x_1$ in (1.1.3) and remember that $\\sigma(0)\\ge 0$."
    },
    {
        "statement": "Let $\\sigma$ be sublinear and suppose that there exist $x_1,\\dots,x_m$ in $\\dom\\sigma$ such that\n\\[\n\\sigma(x_j)+\\sigma(-x_j)=0\\quad\\text{for } j=1,\\dots,m.\n\\tag{1.1.7}\n\\]\nThen $\\sigma$ is linear on the subspace spanned by $x_1,\\dots,x_m$.",
        "title": "",
        "label": "thm:FCA-chapC-1.1.6",
        "lean_tag": [
            "FCA_chap_C_1_1_6"
        ],
        "lean_formalization": "lemma FCA_chap_C_1_1_6 {m n : ℕ}\n  (σ : EuclideanSpace ℝ (Fin n) → WithTop ℝ)\n  (x : ℕ → EuclideanSpace ℝ (Fin n))\n  (hσ : IsSublinear (liftWithToptoEReal σ))\n  (h_eq_0 : ∀ j ∈ Finset.range m, σ (x j) + σ (-1 • (x j)) = 0) :\n  let 𝓧 := {v | InSubspaceSpanVec m x v}\n  (IsLinearOn 𝓧 σ):= by\n  sorry",
        "proof": "With $x_1,\\dots,x_m$ as stated, each $-x_j$ is in $\\dom\\sigma$. Let $x:=\\sum_{j=1}^m t_j x_j$ be an arbitrary linear combination of $x_1,\\dots,x_m$; we must prove that $\\sigma(x)=\\sum_{j=1}^m t_j\\sigma(x_j)$. Set\n\\[\nJ_1 := \\{j : t_j > 0\\},\\qquad J_2 := \\{j : t_j < 0\\},\n\\]\nand obtain (as usual, $\\sum_\\emptyset=0$):\n\n\\begin{align}\n\\sigma(x)\n&=\\sigma\\!\\biggl(\\sum_{j\\in J_1} t_j x_j + \\sum_{j\\in J_2}(-t_j)(-x_j)\\biggr)\\\\\n&\\le \\sum_{j\\in J_1} t_j\\sigma(x_j)+\\sum_{j\\in J_2}(-t_j)\\sigma(-x_j) \\tag*{[from (1.1.4)]}\\\\\n&= \\sum_{j\\in J_1} t_j\\sigma(x_j)+\\sum_{j\\in J_2} t_j\\sigma(x_j)=\\sum_{j=1}^m t_j\\sigma(x_j) \\tag*{[from (1.1.7)]}\\\\\n&= -\\sum_{j\\in J_1} t_j\\sigma(-x_j)-\\sum_{j\\in J_2}(-t_j)\\sigma(x_j) \\tag*{[from (1.1.7)]}\\\\\n&\\le -\\sigma\\!\\bigl(-\\sum_{j=1}^m t_j x_j\\bigr)\\tag*{[from (1.1.4)]}\\\\\n&= -\\sigma(-x)\\le \\sigma(x). \\tag*{[from (1.1.6)]}\n\\end{align}\n\n\nIn summary, we have proved \\(\\sigma(x)\\le \\sum_{j=1}^m t_j\\sigma(x_j)\\le -\\sigma(-x)\\le\\sigma(x).\\) \\qedhere"
    },
    {
        "statement": "Let $\\sigma$ be sublinear. If $x\\in U$, i.e.\\ if\n\\[\n\\sigma(x)+\\sigma(-x)=0,\n\\]\nthen there holds\n\\[\n\\sigma(x+y)=\\sigma(x)+\\sigma(y)\\qquad\\text{for all }y\\in\\mathbb{R}^n.\n\\]",
        "title": "",
        "label": "prop:FCA-chapC-1.1.7",
        "lean_tag": [
            "FCA_chap_C_1_1_7"
        ],
        "lean_formalization": "lemma FCA_chap_C_1_1_7 {m n : ℕ}\n  (σ : EuclideanSpace ℝ (Fin n) → WithTop ℝ)\n  (x : EuclideanSpace ℝ (Fin n))\n  (hσ : IsSublinear (liftWithToptoEReal σ)) :\n  (σ x + σ (-1 • x) = 0) →\n  ∀ (y : EuclideanSpace ℝ (Fin n)), σ (x + y) = σ x + σ y := by\n  sorry",
        "proof": "In view of subadditivity, we just have to prove ``$\\ge$'' in (1.1.10). Start from the identity $y=x+y-x$; apply successively subadditivity and (1.1.9) to obtain\n\\[\n\\sigma(y)\\le\\sigma(x+y)+\\sigma(-x)=\\sigma(x+y)-\\sigma(x).\n\\]"
    },
    {
        "statement": "Let $C$ be a closed convex set containing the origin. Then\n\\begin{enumerate}\n\\item[(i)] its gauge $\\gamma_C$ is a nonnegative closed sublinear function;\n\\item[(ii)] $\\gamma_C$ is finite everywhere if and only if $0$ lies in the interior of $C$;\n\\item[(iii)] $C_\\infty$ being the asymptotic cone of $C$,\n\\[\n\\{x\\in\\mathbb{R}^n:\\ \\gamma_C(x)\\le r\\}=rC\\quad\\text{for all }r>0,\n\\qquad\n\\{x\\in\\mathbb{R}^n:\\ \\gamma_C(x)=0\\}=C_\\infty.\n\\]\n\\end{enumerate}",
        "title": "",
        "label": "thm:FCA-chapC-1.2.5",
        "lean_tag": [
            "FCA_chap_C_1_2_5"
        ],
        "lean_formalization": "lemma FCA_chap_C_1_2_5 {m n : ℕ}\n  (C : Set (EuclideanSpace ℝ (Fin n)))\n  (hC_closed : IsClosed C) (hC_convex : Convex ℝ C) (hC_origin : 0 ∈ C) :\n  let g : EuclideanSpace ℝ (Fin n) → WithTop ℝ := fun x => gauge C x\n  ((∀ x, g x ≥ 0) ∧ (IsSublinear (liftWithToptoEReal g)) ∧ (IsClosedFun g)) ∧\n  ((∀ x, g x ≠ ⊤) ↔ (0 ∈ interior C)) := by\n  sorry",
        "proof": "[(i) and (iii)] Nonnegativity and positive homogeneity are obvious from the definition of $\\gamma_C$; also, $\\gamma_C(0)=0$ because $0\\in C$. We prove convexity via a geometric interpretation of (1.2.2). Let\n\\[\nK_C:=\\operatorname{cone}(C\\times\\{1\\})=\\{(\\lambda c,\\lambda)\\in\\mathbb{R}^n\\times\\mathbb{R}:\\ c\\in C,\\ \\lambda\\ge 0\\}\n\\]\nbe the convex conical hull of $C\\times\\{1\\}\\subset\\mathbb{R}^n\\times\\mathbb{R}$. It is convex (beware that $K_C$ need not be closed) and $\\gamma_C$ is clearly given by\n\\[\n\\gamma_C(x)=\\inf\\{\\lambda:\\ (x,\\lambda)\\in K_C\\}.\n\\]\nThus, $\\gamma_C$ is the lower-bound function of \\S B.1.3(g), constructed on the convex set $K_C$; this establishes the convexity of $\\gamma_C$, hence its sublinearity.\n\nNow we prove\n\\begin{equation}\\label{eq:1.2.3}\n\\{x\\in\\mathbb{R}^n:\\ \\gamma_C(x)\\le 1\\}=C.\n\\end{equation}\nThis will imply the first part in (iii), thanks to positive homogeneity. Then the second part will follow because of (A.2.2.2): $C_\\infty=\\cap\\{rC:\\ r>0\\}$ and closedness of $\\gamma_C$ will also result from (iii) via Proposition B.1.2.2.\n\nSo, to prove (1.2.3), observe first that $x\\in C$ implies from (1.2.2) that certainly $\\gamma_C(x)\\le 1$. Conversely, let $x$ be such that $\\gamma_C(x)\\le 1$; we must prove that $x\\in C$. For this we prove that $x_k:= (1-1/k)x\\in C$ for $k=1,2,\\dots$ (and then, the desired property will come from the closedness of $C$). By positive homogeneity, $\\gamma_C(x_k)=(1-1/k)\\gamma_C(x)\\le 1$, so there is $\\lambda_k\\in[0,1]$ such that $x_k=\\lambda_k C$, or equivalently $x_k/\\lambda_k\\in C$. Because $C$ is convex and contains the origin, $\\lambda_k (x_k/\\lambda_k)+(1-\\lambda_k)0 = x_k$ is in $C$, which is what we want.\n\n[(ii)] Assume $0\\in\\operatorname{int}C$. There is $\\varepsilon>0$ such that for all $x\\neq 0$, $x_\\varepsilon := x/\\|x\\|\\in C$; hence $\\gamma_C(x_\\varepsilon)\\le 1$ because of (1.2.3). We deduce by positive homogeneity\n\\[\n\\gamma_C(x)=\\frac{\\|x\\|}{\\varepsilon}\\,\\gamma_C(x_\\varepsilon)\\le\\frac{\\|x\\|}{\\varepsilon}\\,;\n\\]\nthis inequality actually holds for all $x\\in\\mathbb{R}^n$ ($\\gamma_C(0)=0$) and $\\gamma_C$ is a finite function.\n\nConversely, suppose $\\gamma_C$ is finite everywhere. By continuity (Theorem B.3.1.2), $\\gamma_C$ has an upper bound $L>0$ on the unit ball:\n\\[\n\\|x\\|\\le 1 \\quad\\Longrightarrow\\quad \\gamma_C(x)\\le L \\quad\\Longrightarrow\\quad x\\in L C,\n\\]\nwhere the last implication comes from (iii). In other words, $B(0,1/L)\\subset C$."
    },
    {
        "statement": "C is compact if and only if $\\gamma_C(x)>0$ for all $x\\neq 0$.",
        "title": "",
        "label": "cor:FCA-chapC-1.2.6",
        "lean_tag": [
            "FCA_chap_C_1_2_6"
        ],
        "lean_formalization": "lemma FCA_chap_C_1_2_6 {n : ℕ}\n  (C : Set (EuclideanSpace ℝ (Fin n)))\n  (hC_closed : IsClosed C) (hC_convex : Convex ℝ C) (hC_origin : 0 ∈ C) :\n  let g : EuclideanSpace ℝ (Fin n) → WithTop ℝ := fun x => gauge C x\n  (IsCompact C) ↔ (∀ x, x ≠ 0 → g x > 0) := by\n  sorry",
        "proof": ""
    },
    {
        "statement": "\\begin{enumerate}\n    \\item[(i)] If $\\sigma_1$ and $\\sigma_2$ are [closed] sublinear and $t_1,t_2$ are positive numbers, then $\\sigma := t_1\\sigma_1 + t_2\\sigma_2$ is [closed] sublinear, if not identically $+\\infty$.\n    \\item[(ii)] If $\\{\\sigma_j\\}_{j\\in J}$ is a family of [closed] sublinear functions, then $\\sigma := \\sup_{j\\in J}\\sigma_j$ is [closed] sublinear, if not identically $+\\infty$.\n\\end{enumerate}",
        "title": "",
        "label": "prop:FCA-chapC-1.3.1",
        "lean_tag": [
            "FCA_chap_C_1_3_1_i",
            "FCA_chap_C_1_3_1_ii"
        ],
        "lean_formalization": "lemma FCA_chap_C_1_3_1_i {n : ℕ}\n  (σ₁ σ₂ : EuclideanSpace ℝ (Fin n) → WithTop ℝ)\n  (t₁ t₂ : ℝ)\n  (hσ : IsSublinear (liftWithToptoEReal σ₁) ∧ IsSublinear (liftWithToptoEReal σ₂))\n  (ht : t₁ > 0 ∧ t₂ > 0) :\n  let σ := t₁ • σ₁ + t₂ • σ₂\n  (IsSublinear (liftWithToptoEReal σ)) ∧\n  (IsClosedFun σ₁ ∧ IsClosedFun σ₂ → (IsSublinear (liftWithToptoEReal σ)) ∧ (IsClosedFun σ)):= by\n  sorry",
        "proof": "Concerning convexity and closedness, everything is known from \\S B.2. Note in passing that a closed sublinear function is zero (hence finite) at zero. As for positive homogeneity, it is straightforward."
    },
    {
        "statement": "Let $\\{\\sigma_j\\}_{j\\in J}$ be a family of sublinear functions all minorized by some linear function. Then\n\\begin{enumerate}\n    \\item[(i)] $\\sigma := \\operatorname{co}\\big(\\inf_{j\\in J}\\sigma_j\\big)$ is sublinear.\n    \\item[(ii)] If $J=\\{1,\\dots,m\\}$ is a finite set, we obtain the infimal convolution\n    \\[\n    \\operatorname{co}\\min\\{\\sigma_1,\\dots,\\sigma_m\\}=\\sigma_1\\infconv\\cdots\\infconv\\sigma_m.\n    \\]\n\\end{enumerate}",
        "title": "",
        "label": "prop:FCA-chapC-1.3.2",
        "lean_tag": [
            "FCA_chap_C_1_3_2_i",
            "FCA_chap_C_1_3_2_ii"
        ],
        "lean_formalization": "lemma FCA_chap_C_1_3_2_i {n : ℕ}\n  (σ : ℕ → (EuclideanSpace ℝ (Fin n) → WithTop ℝ))\n  (J : Set ℕ)\n  (hσ_sublinear : ∀ j ∈ J, IsSublinear (liftWithToptoEReal (σ j)))\n  (hσ_minorized : ∃ (s : EuclideanSpace ℝ (Fin n)), ∃ (b : ℝ),\n                  ∀ j ∈ J, ∀ (x : EuclideanSpace ℝ (Fin n)),\n                  (σ j) x ≥ (inner ℝ s x) + b) :\n  let σ_inf := fun x => sInf (⋃ j ∈ J, {(σ j) x})\n  let σ' := lscHull σ_inf\n  IsSublinear (liftWithToptoEReal σ') := by\n  sorry",
        "proof": "[(i)] Once again, the only thing to prove for (i) is positive homogeneity. Actually, it suffices to multiply $x$ and each $x_j$ by $t>0$ in a formula giving $\\operatorname{co}\\big(\\inf_j\\sigma_j\\big)(x)$, say (B.2.5.3).\n\n[(ii)] By definition, computing $\\operatorname{co}\\big(\\min_j\\sigma_j\\big)(x)$ amounts to solving the minimization problem in the $m$ couples of variables $(x_j,\\alpha_j)\\in\\operatorname{dom}\\sigma_j\\times\\mathbb{R}$\n\\[\n\\begin{aligned}\n&\\inf\\sum_{j=1}^m\\alpha_j\\sigma_j(x_j)\\qquad \\alpha_j\\ge 0\\\\\n&\\sum_{j=1}^m\\alpha_j=1,\\ \\sum_{j=1}^m\\alpha_j x_j=x.\n\\end{aligned}\n\\tag{1.3.1}\n\\]\n\nIn view of positive homogeneity, the variables $\\alpha_j$ play no role by themselves: the relevant variables are actually the products $\\alpha_j x_j$ and (1.3.1) can be written -- denoting $\\alpha_j x_j$ again by $x_j$:\n\\[\n\\operatorname{co}\\big(\\min_j\\sigma_j\\big)(x)=\\inf\\Big\\{\\sum_{j=1}^m\\sigma_j(x_j):\\sum_{j=1}^m x_j=x\\Big\\}.\n\\]\n\nWe recognize the infimal convolution of the $\\sigma_j$'s."
    },
    {
        "statement": "For $\\sigma_1$ and $\\sigma_2$ in the set $\\Phi$ of sublinear functions that are finite everywhere, define\n\\[\n\\Delta(\\sigma_1,\\sigma_2):=\\max_{\\|x\\|\\le 1}|\\sigma_1(x)-\\sigma_2(x)|.\n\\tag{1.3.2}\n\\]\nThen $\\Delta$ is a distance on $\\Phi$.",
        "title": "",
        "label": "thm:FCA-chapC-1.3.3",
        "lean_tag": "no-lean-tag",
        "proof": "Clearly $\\Delta(\\sigma_1,\\sigma_2)<+\\infty$ and $\\Delta(\\sigma_1,\\sigma_2)=\\Delta(\\sigma_2,\\sigma_1)$. Now positive homogeneity of $\\sigma_1$ and $\\sigma_2$ gives for all $x\\neq 0$\n\\[\n|\\sigma_1(x)-\\sigma_2(x)|=\\|x\\|\\Big|\\sigma_1\\Big(\\frac{x}{\\|x\\|}\\Big)-\\sigma_2\\Big(\\frac{x}{\\|x\\|}\\Big)\\Big|\n\\le \\|x\\|\\max_{\\|u\\|=1}|\\sigma_1(u)-\\sigma_2(u)|\n\\le \\|x\\|\\,\\Delta(\\sigma_1,\\sigma_2).\n\\]\n\nIn addition, $\\sigma_1(0)=\\sigma_2(0)=0$, so\n\\[\n|\\sigma_1(x)-\\sigma_2(x)|\\le\\|x\\|\\,\\Delta(\\sigma_1,\\sigma_2)\\qquad\\text{for all }x\\in\\mathbb R^n\n\\]\nand $\\Delta(\\sigma_1,\\sigma_2)=0$ if and only if $\\sigma_1=\\sigma_2$.\n\nAs for the triangle inequality, we have for arbitrary $\\sigma_1,\\sigma_2,\\sigma_3$ in $\\Phi$\n\\[\n|\\sigma_1(x)-\\sigma_3(x)|\\le|\\sigma_1(x)-\\sigma_2(x)|+|\\sigma_2(x)-\\sigma_3(x)|\\qquad\\text{for all }x\\in\\mathbb R^n,\n\\]\nso there holds\n\\[\n\\Delta(\\sigma_1,\\sigma_3)\\le\\max_{\\|x\\|=1}\\big[|\\sigma_1(x)-\\sigma_2(x)|+|\\sigma_2(x)-\\sigma_3(x)|\\big]\n\\le\\max_{\\|x\\|=1}|\\sigma_1(x)-\\sigma_2(x)|+\\max_{\\|x\\|=1}|\\sigma_2(x)-\\sigma_3(x)|,\n\\]\nwhich is the required inequality."
    },
    {
        "statement": "Let $(\\sigma_k)$ be a sequence of finite sublinear functions and let $\\sigma$ be a finite function.  Then the following are equivalent when $k\\to+\\infty$:\n\\begin{enumerate}\n\\item[(i)] $(\\sigma_k)$ converges pointwise to $\\sigma$;\n\\item[(ii)] $(\\sigma_k)$ converges to $\\sigma$ uniformly on each compact set of $\\mathbb{R}^n$;\n\\item[(iii)] $\\Delta(\\sigma_k,\\sigma)\\to 0$.\n\\end{enumerate}",
        "title": "",
        "label": "thm:FCA-chapC-1.3.5",
        "lean_tag": [
            "FCA_chap_C_1_3_5"
        ],
        "lean_formalization": "lemma FCA_chap_C_1_3_5 {n : ℕ}\n  (σk : ℕ → (EuclideanSpace ℝ (Fin n) → ℝ))\n  (σ : EuclideanSpace ℝ (Fin n) → ℝ) :\n  let d := fun k => DistOnFunctions (σk k) σ\n  List.TFAE [\n    (∀ x, Filter.Tendsto (fun k => (σk k) x) Filter.atTop (𝓝 (σ x))),\n    (∀ (K : Set (EuclideanSpace ℝ (Fin n))), IsCompact K → TendstoUniformlyOn σk σ Filter.atTop K),\n    (Filter.Tendsto d Filter.atTop (𝓝 0))\n  ] := by\n  sorry",
        "proof": "First, the (finite) function $\\sigma$ is of course sublinear whenever it is the pointwise limit of sublinear functions.  The equivalence between (i) and (ii) comes from the general Theorem B.3.1.4 on the convergence of convex functions.\n\nNow, (ii) clearly implies (iii).  Conversely $\\Delta(\\sigma_k,\\sigma)\\to 0$ is the uniform convergence on the unit ball, hence on any ball of radius $L>0$ (the maxmind in (1.3.2) is positively homogeneous), hence on any compact set."
    },
    {
        "statement": "A support function is closed and sublinear.",
        "title": "",
        "label": "prop:FCA-chapC-2.1.2",
        "lean_tag": [
            "FCA_chap_C_2_1_2"
        ],
        "lean_formalization": "lemma FCA_chap_C_2_1_2 {n : ℕ}\n  (S : Set (EuclideanSpace ℝ (Fin n)))\n  (hS : Set.Nonempty S) :\n  let support_fun := SupportFun S\n  (IsSublinear (liftWithToptoEReal support_fun)) ∧ (IsClosedFun support_fun)\n  := by\n  sorry",
        "proof": "This results from Proposition 1.3.1(ii) (a linear form is closed and convex!). Observe in particular that a support function is null (hence $< +\\infty$) at the origin."
    },
    {
        "statement": "The support function of $S$ is finite everywhere if and only if $S$ is bounded.",
        "title": "",
        "label": "prop:FCA-chapC-2.1.3",
        "lean_tag": [
            "FCA_chap_C_2_1_3"
        ],
        "lean_formalization": "lemma FCA_chap_C_2_1_3 {n : ℕ}\n  (S : Set (EuclideanSpace ℝ (Fin n)))\n  (hS : Set.Nonempty S) :\n  let support_fun := SupportFun S\n  (∀ x, support_fun x ≠ ⊤) ↔\n  (Bornology.IsBounded S) := by\n  sorry",
        "proof": "Let $S$ be bounded, say $S\\subset B(0,L)$ for some $L>0$. Then\n\\[\n\\langle s,x\\rangle \\le \\|s\\|\\,\\|x\\| \\le L\\|x\\|\\qquad\\text{for all } s\\in S,\n\\]\nwhich implies $\\sigma_S(x)\\le L\\|x\\|$ for all $x\\in\\mathbb{R}^n$.\n\nConversely, finiteness of the convex $\\sigma_S$ implies its continuity on the whole space (Theorem B.3.1.2), hence its local boundedness: for some $L$,\n\\[\n\\langle s,x\\rangle \\le \\sigma_S(x)\\le L\\qquad\\text{for all }(s,x)\\in S\\times B(0,1).\n\\]\nIf $s\\neq 0$, we can take $x=s/\\|s\\|$ in the above relation, which implies \\(\\|s\\|\\le L\\)."
    },
    {
        "statement": "For \\(S\\subset\\mathbb{R}^n\\) nonempty, there holds \\(\\sigma_S=\\sigma_{\\operatorname{cl} S}=\\sigma_{\\operatorname{co} S}\\); whence\n\\[\n\\sigma_S=\\sigma_{\\overline{\\operatorname{co}}\\,S}.\n\\tag{2.2.1}\n\\]",
        "title": "",
        "label": "prop:FCA-chapC-2.2.1",
        "lean_tag": [
            "FCA_chap_C_2_2_2"
        ],
        "lean_formalization": "lemma FCA_chap_C_2_2_2 {n : ℕ}\n  (S : Set (EuclideanSpace ℝ (Fin n)))\n  (s : EuclideanSpace ℝ (Fin n))\n  (hS : Set.Nonempty S) :\n  let sf_S := SupportFun S\n  (s ∈ closure (convexHull ℝ S)) ↔\n  (∀ (d : EuclideanSpace ℝ (Fin n)), inner ℝ s d ≤ sf_S d) := by\n  sorry",
        "proof": "The continuity [resp.\\ linearity, hence convexity] of the function \\(\\langle s,\\cdot\\rangle\\), which is maximized over \\(S\\), implies that \\(\\sigma_S=\\sigma_{\\operatorname{cl} S}\\) [resp.\\ \\(\\sigma_S=\\sigma_{\\operatorname{co} S}\\)]. Knowing that \\(\\overline{\\operatorname{co}}\\,S=\\operatorname{cl}\\,\\operatorname{co}\\,S\\) (Proposition A.1.4.2), (2.2.1) follows immediately."
    },
    {
        "statement": "For the nonempty $S\\subset\\mathbb{R}^n$ and its support function $\\sigma_S$, there holds\n\\[\ns\\in\\overline{\\operatorname{co}}S\\iff\\bigl[\\langle s,d\\rangle\\le\\sigma_S(d)\\quad\\text{for all }d\\in X\\bigr],\n\\tag{2.2.2}\n\\]\nwhere the set $X$ can be indifferently taken as: the whole of $\\mathbb{R}^n$, the unit ball $B(0,1)$ or its boundary the unit sphere $\\tilde{B}$, or $\\operatorname{dom}\\sigma_S$.",
        "title": "",
        "label": "thm:FCA-chapC-2.2.2",
        "lean_tag": [
            "FCA_chap_C_2_2_2"
        ],
        "lean_formalization": "lemma FCA_chap_C_2_2_2 {n : ℕ}\n  (S : Set (EuclideanSpace ℝ (Fin n)))\n  (s : EuclideanSpace ℝ (Fin n))\n  (hS : Set.Nonempty S) :\n  let sf_S := SupportFun S\n  (s ∈ closure (convexHull ℝ S)) ↔\n  (∀ (d : EuclideanSpace ℝ (Fin n)), inner ℝ s d ≤ sf_S d) := by\n  sorry",
        "proof": "First, the equivalence between all the choices for $X$ is clear enough; in particular due to positive homogeneity. Because ``$\\Rightarrow$'' is Proposition 2.2.1, we have to prove ``$\\Leftarrow$'' only, with $X=\\mathbb{R}^n$ say.\n\nSo suppose that $s\\notin\\overline{\\operatorname{co}}S$. Then $\\{s\\}$ and $\\overline{\\operatorname{co}}S$ can be strictly separated (Theorem A.4.1.1): there exists $d_0\\in\\mathbb{R}^n$ such that\n\\[\n\\langle s,d_0\\rangle>\\sup\\{\\langle s',d_0\\rangle:s'\\in\\overline{\\operatorname{co}}S\\}=\\sigma_S(d_0),\n\\]\nwhere the last equality is (2.2.1). Our result is proved by contradiction."
    },
    {
        "statement": "Let $S$ be a nonempty closed convex set in $\\mathbb{R}^n$. Then\n\\begin{enumerate}\n\\item[(i)] $s\\in\\operatorname{aff}S$ if and only if\n\\[\n\\langle s,d\\rangle=\\sigma_S(d)\\quad\\text{for all }d\\text{ with }\\sigma_S(d)+\\sigma_S(-d)=0;\n\\tag{2.2.3}\n\\]\n\\item[(ii)] $s\\in\\operatorname{ri}S$ if and only if\n\\[\n\\langle s,d\\rangle<\\sigma_S(d)\\quad\\text{for all }d\\text{ with }\\sigma_S(d)+\\sigma_S(-d)>0;\n\\tag{2.2.4}\n\\]\n\\item[(iii)] in particular, $s\\in\\operatorname{int}S$ if and only if\n\\[\n\\langle s,d\\rangle<\\sigma_S(d)\\quad\\text{for all }d\\neq0.\n\\tag{2.2.5}\n\\]\n\\end{enumerate}",
        "title": "",
        "label": "thm:FCA-chapC-2.2.3",
        "lean_tag": [
            "FCA_chap_C_2_2_3"
        ],
        "lean_formalization": "lemma FCA_chap_C_2_2_3 {n : ℕ}\n  (S : Set (EuclideanSpace ℝ (Fin n)))\n  (s : EuclideanSpace ℝ (Fin n))\n  (hS : Set.Nonempty S) :\n  let σS := SupportFun S\n  ((s ∈ affineHull S) ↔ (∀ d, σS d + σS (-1 • d) = 0 → inner ℝ s d = σS d)) ∧\n  ((s ∈ intrinsicInterior ℝ S) ↔ (∀ d, σS d + σS (-1 • d) > 0 → inner ℝ s d < σS d)) ∧\n  ((s ∈ interior S) ↔ (∀ d, d ≠ 0 → inner ℝ s d < σS d)):= by\n  sorry",
        "proof": "[(i)] Let first $s\\in S$. We have already seen in Definition 2.1.4 that\n\\[\n- \\sigma_S(-d) \\le \\langle s,d\\rangle \\le \\sigma_S(d)\\qquad\\text{for all } d\\in\\mathbb{R}^n.\n\\]\nIf the breadth of $S$ along $d$ is zero, we obtain a pair of equalities: for such $d$, there holds\n\\[\n\\langle s,d\\rangle = \\sigma_S(d),\n\\]\nan equality which extends by affine combination to any element \\(s\\in\\operatorname{aff}S\\).\n\nConversely, let \\(s\\) satisfy (2.2.3). A first case is when the only \\(d\\) described in (2.2.3) is \\(d = 0\\); as a consequence of our observations in Definition 2.1.4, there is no affine hyperplane containing \\(S\\), i.e.\\ \\(\\operatorname{aff}S = \\mathbb{R}^n\\) and there is nothing to prove. Otherwise, there does exist a hyperplane \\(H\\) containing \\(S\\); it is defined by\n\\[\nH := \\{p \\in \\mathbb{R}^n : \\langle p,d_H\\rangle = \\sigma_S(d_H)\\}, \\tag{2.2.6}\n\\]\nfor some \\(d_H \\neq 0\\). We proceed to prove \\(\\langle s,\\cdot\\rangle \\le \\sigma_H\\).\n\nIn fact, the breadth of \\(S\\) along \\(d_H\\) is certainly 0, hence \\(\\langle s,d_H\\rangle = \\sigma_S(d_H)\\) because of (2.2.3), while (2.2.6) shows that \\(\\sigma_S(d_H) = \\sigma_H(d_H)\\). On the other hand, it is obvious that \\(\\sigma_H(d) = +\\infty\\) if \\(d\\) is not collinear to \\(d_H\\). In summary, we have proved \\(\\langle s,d\\rangle \\le \\sigma_H(d)\\) for all \\(d\\), i.e.\\ \\(s \\in H\\). We conclude that our \\(s\\) is in any affine manifold containing \\(S\\): \\(s \\in \\operatorname{aff}S\\).\n\n[(iii)] In view of positive homogeneity, we can normalize \\(d\\) in (2.2.5). For \\(s \\in \\operatorname{int} S\\), there exists \\(\\varepsilon > 0\\) such that \\(s + \\varepsilon d \\in S\\) for all \\(d\\) in the unit sphere \\(\\widetilde{B}\\). Then, from the very definition (2.1.1),\n\\[\n\\sigma_S(d) \\ge \\langle s + \\varepsilon d,d\\rangle = \\langle s,d\\rangle + \\varepsilon \\quad\\text{for all } d \\in \\widetilde{B}.\n\\]\n\nConversely, let \\(s \\in \\mathbb{R}^n\\) be such that\n\\[\n\\sigma_S(d) - \\langle s,d\\rangle > 0 \\quad\\text{for all } d \\in \\widetilde{B},\n\\]\nwhich implies, because \\(\\sigma_S\\) is closed and the unit sphere is compact:\n\\[\n0 < \\varepsilon := \\inf\\{\\sigma_S(d) - \\langle s,d\\rangle : d \\in \\widetilde{B}\\} \\le +\\infty.\n\\]\nThus\n\\[\n\\langle s,d\\rangle + \\varepsilon \\le \\sigma_S(d)\\quad\\text{for all } d \\in \\widetilde{B}.\n\\]\n\nNow take \\(u\\) with \\(\\|u\\| < \\varepsilon\\). From the Cauchy-Schwarz inequality, we have for all \\(d \\in \\widetilde{B}\\)\n\\[\n\\langle s+u,d\\rangle = \\langle s,d\\rangle + \\langle u,d\\rangle \\le \\langle s,d\\rangle + \\varepsilon \\le \\sigma_S(d)\n\\]\nand this implies \\(s + u \\in S\\) because of Theorem 2.2.2: \\(s \\in \\operatorname{int} S\\) and (iii) is proved.\n\n[(ii)] Look at Fig. 2.2.2 again: decompose \\(\\mathbb{R}^n = V \\oplus U\\), where \\(V\\) is the subspace parallel to \\(\\operatorname{aff}S\\) and \\(U = V^\\perp\\). In the decomposition \\(d = d_V + d_U\\), \\(\\langle\\cdot,d_U\\rangle\\) is constant over \\(S\\), so \\(S\\) has 0-breadth along \\(d_U\\) and\n\\[\n\\sigma_S(d) = \\sup_{s\\in S}\\langle s,d_V + d_U\\rangle = \\langle s,d_U\\rangle + \\sup_{s\\in S}\\langle s,d_V\\rangle\n\\]\nfor any \\(s \\in S\\). With these notations, a direction described as in (2.2.4) is a \\(d\\) such that\n\\[\n\\sigma_S(d) + \\sigma_S(-d) = \\sigma_S(d_V) + \\sigma_S(-d_V) > 0.\n\\]\nThen, (ii) is just (iii) written in the subspace \\(V\\)."
    },
    {
        "statement": "Let $S$ be a nonempty closed convex set in $\\mathbb{R}^n$. Then $\\overline{\\operatorname{dom}\\sigma_S}$ and the asymptotic cone $S_\\infty$ of $S$ are mutually polar cones.",
        "title": "",
        "label": "prop:FCA-chapC-2.2.4",
        "lean_tag": [
            "FCA_chap_C_2_2_4"
        ],
        "lean_formalization": "lemma FCA_chap_C_2_2_4 {n : ℕ}\n  (S : Set (EuclideanSpace ℝ (Fin n)))\n  (x : EuclideanSpace ℝ (Fin n))\n  (hS_nonempty : Set.Nonempty S)\n  (hS_closed : IsClosed S)\n  (hS_convex : Convex ℝ S) :\n  let σS := SupportFun S\n  let Sinfty := AsymptoticCone S x\n  PolarCone (closure (effDom (liftWithToptoEReal σS))) = Sinfty := by\n  sorry",
        "proof": "Recall from \\S A.3.2 that, if $K_1$ and $K_2$ are two closed convex cones, then $K_1\\subset K_2$ if and only if $(K_1)^\\circ \\supset (K_2)^\\circ$.\n\nLet $p\\in S_\\infty$. Fix $s_0$ arbitrary in $S$ and use the fact that $S_\\infty = \\bigcap_{t>0} t(S-s_0)$ ( \\S A.2.2); for all $t>0$, we can find $s_t\\in S$ such that $p=t(s_t-s_0)$. Now, for $q\\in\\operatorname{dom}\\sigma_S$, there holds\n\\[\n\\langle p,q\\rangle = t\\langle s_t-s_0,q\\rangle \\le t\\big[\\sigma_S(q)-\\langle s_0,q\\rangle\\big]<+\\infty\n\\]\nand letting $t\\downarrow 0$ shows that $\\langle p,q\\rangle\\le 0$. In other words, $\\operatorname{dom}\\sigma_S\\subset (S_\\infty)^\\circ$; then $\\overline{\\operatorname{dom}\\sigma_S}\\subset (S_\\infty)^\\circ$ since the latter is closed.\n\nConversely, let $q\\in (\\operatorname{dom}\\sigma_S)^\\circ$, which is a cone, hence $tq\\in(\\operatorname{dom}\\sigma_S)^\\circ$ for any $t>0$. Thus, given $s_0\\in S$, we have for arbitrary $p\\in\\operatorname{dom}\\sigma_S$\n\\[\n\\langle s_0+tq,p\\rangle = \\langle s_0,p\\rangle + t\\langle q,p\\rangle \\le \\langle s_0,p\\rangle \\le \\sigma_S(p),\n\\]\nso $s_0+tq\\in S$ by virtue of Theorem 2.2.2. In other words: $q\\in (S-s_0)/t$ for all $t>0$ and $q\\in S_\\infty$."
    },
    {
        "statement": "Let $\\sigma$ be a closed sublinear function; then there is a linear function minorizing $\\sigma$.  In fact, $\\sigma$ is the supremum of the linear functions minorizing it. In other words, $\\sigma$ is the support function of the nonempty closed convex set\n\\[\nS_\\sigma := \\{s\\in\\mathbb{R}^n : \\langle s,d\\rangle \\le \\sigma(d)\\ \\text{for all } d\\in\\mathbb{R}^n\\}.\n\\tag{3.1.1}\n\\]",
        "title": "",
        "label": "thm:FCA-chapC-3.1.1",
        "lean_tag": [
            "FCA_chap_C_3_1_1"
        ],
        "lean_formalization": "lemma FCA_chap_C_3_1_1 {n : ℕ}\n  (σ : EuclideanSpace ℝ (Fin n) → WithTop ℝ)\n  (hσ_closed : IsClosedFun σ) (hσ_sublinear : IsSublinear (liftWithToptoEReal σ)) :\n  let Sσ := {s | ∀ d, inner ℝ s d ≤ σ d}\n  ((∃ (s : EuclideanSpace ℝ (Fin n)) (b : ℝ), ∀ x, inner ℝ s x + b ≤ σ x)) ∧\n  (∀ x, σ x = SupportFun Sσ x) := by\n  sorry",
        "proof": "Being convex, $\\sigma$ is minorized by some affine function (Proposition B.1.2.1): for some $(s,r)\\in\\mathbb{R}^n\\times\\mathbb{R}$,\n\\[\n\\langle s,d\\rangle - r \\le \\sigma(d)\\qquad\\text{for all } d\\in\\mathbb{R}^n.\n\\tag{3.1.2}\n\\]\nBecause $\\sigma(0)=0$, the above $r$ is nonnegative.  Also, by positive homogeneity,\n\\[\n\\langle s,d\\rangle - \\tfrac{1}{t}r \\le \\sigma(d)\\qquad\\text{for all } d\\in\\mathbb{R}^n\\ \\text{and all } t>0.\n\\]\nLetting $t\\to+\\infty$, we see that $\\sigma$ is actually minorized by a linear function:\n\\[\n\\langle s,d\\rangle \\le \\sigma(d)\\qquad\\text{for all } d\\in\\mathbb{R}^n.\n\\tag{3.1.3}\n\\]\nNow observe that the minorization (3.1.3) is sharper than (3.1.2): when expressing the closed convex $\\sigma$ as the supremum of all the affine functions minorizing it (Proposition B.1.2.8), we can restrict ourselves to linear functions.  In other words\n\\[\n\\sigma(d)=\\sup\\{\\langle s,d\\rangle :\\ \\text{the linear }\\langle s,\\cdot\\rangle\\ \\text{minorizes }\\sigma\\};\n\\]\nin the above index-set, we just recognize $S_\\sigma$."
    },
    {
        "statement": "For a nonempty closed convex set $S$ and a closed sublinear function $\\sigma$, the following are equivalent:\n\\begin{enumerate}\n    \\item[(i)] \\(\\sigma\\) is the support function of \\(S\\).\n    \\item[(ii)] \\(S=\\{s:\\ \\langle s,d\\rangle\\le\\sigma(d)\\text{ for all }d\\in X\\},\\) where the set \\(X\\) can be indifferently taken as: the whole of \\(\\mathbb{R}^n\\), the unit ball \\(B(0,1)\\) or its boundary, or \\(\\operatorname{dom}\\sigma\\).\n\\end{enumerate}",
        "title": "",
        "label": "cor:FCA-chapC-3.1.2",
        "lean_tag": [
            "FCA_chap_C_3_1_2"
        ],
        "lean_formalization": "lemma FCA_chap_C_3_1_2 {n : ℕ}\n  (S : Set (EuclideanSpace ℝ (Fin n)))\n  (σ : EuclideanSpace ℝ (Fin n) → WithTop ℝ)\n  (hS_nonempty : Set.Nonempty S) (hS_closed : IsClosed S) (hS_convex : Convex ℝ S)\n  (hσ_closed : IsClosedFun σ) (hσ_sublinear : IsSublinear (liftWithToptoEReal σ)) :\n  let support_fun := SupportFun S\n  let S' := {s | ∀ (d : EuclideanSpace ℝ (Fin n)), inner ℝ s d ≤ σ d}\n  List.TFAE [\n    ∀ (x : EuclideanSpace ℝ (Fin n)), support_fun x = σ x,\n    S = S'\n  ] := by\n  sorry",
        "proof": "The case \\(X=\\mathbb{R}^n\\) is just Theorem 3.1.1. The other cases are then clear."
    },
    {
        "statement": "For $x$ in a nonempty closed convex set $C$, it holds\n\\[\nx\\in F_C(d)\\iff d\\in N_C(x).\n\\]",
        "title": "",
        "label": "prop:FCA-chapC-3.1.4",
        "lean_tag": [
            "FCA_chap_C_3_1_4"
        ],
        "lean_formalization": "lemma FCA_chap_C_3_1_4 {n : ℕ}\n  (C : Set (EuclideanSpace ℝ (Fin n)))\n  (x : EuclideanSpace ℝ (Fin n)) (d : EuclideanSpace ℝ (Fin n))\n  (hC_nonempty : Set.Nonempty C) (hC_closed : IsClosed C) (hC_convex : Convex ℝ C)\n  (hx : x ∈ C) (hd : d ≠ 0) :\n  x ∈ DirectionExposingFace C d ↔ d ∈ NormalCone x C := by\n  sorry",
        "proof": ""
    },
    {
        "statement": "For a nonempty closed convex set $C$, it holds\n\\[\n\\operatorname{bd} C = \\bigcup\\{F_C(d):\\; d\\in X\\}\n\\]\nwhere $X$ can be indifferently taken as: $\\mathbb{R}^n\\setminus\\{0\\}$, the unit sphere $\\widetilde{B}$, or $\\operatorname{dom}\\sigma_C\\setminus\\{0\\}$.",
        "title": "",
        "label": "prop:FCA-chapC-3.1.5",
        "lean_tag": "no-lean-tag",
        "proof": "Observe from Definition 3.1.3 that the face exposed by \\(d\\neq 0\\) does not depend on \\(\\|d\\|\\). This establishes the equivalence between the first two choices for \\(X\\). As for the third choice, it is due to the fact that \\(F_C(d)=\\varnothing\\) if \\(d\\notin\\operatorname{dom}\\sigma_C\\).\n\nNow, if \\(x\\) is interior to \\(C\\) and \\(d\\neq 0\\), then \\(x+\\varepsilon d\\in C\\) and \\(x\\) cannot be a maximizer of \\(\\langle\\cdot,d\\rangle\\); \\(x\\) is not in the face exposed by \\(d\\). Conversely, take \\(x\\) on the boundary of \\(C\\). Then \\(N_C(x)\\) contains a nonzero vector \\(d\\); by Proposition 3.1.4, \\(x\\in F_C(d)\\)."
    },
    {
        "statement": "Let $B$ and $B^*$ be defined by (3.2.1) and (3.2.2), where $\\|\\cdot\\|$ is a norm on $\\mathbb{R}^n$. The support function of $B$ and the gauge of $B^*$ are the same function $\\|\\cdot\\|_* $ defined by\n\\[\n\\|s\\|_* := \\max\\{\\langle s,x\\rangle : \\|x\\|\\le 1\\}. \\tag{3.2.3}\n\\]\nFurthermore, $\\|\\cdot\\|_*$ is a norm on $\\mathbb{R}^n$. The support function of its unit ball $B^*$ and the gauge of its supported set $B$ are the same function $\\|\\cdot\\|$: there holds\n\\[\n\\|x\\| = \\max\\{\\langle s,x\\rangle : \\|s\\|_* \\le 1\\}. \\tag{3.2.4}\n\\]",
        "title": "",
        "label": "prop:FCA-chapC-3.2.1",
        "lean_tag": "no-lean-tag",
        "proof": "It is a particular case of the results 3.2.4 and 3.2.5 below."
    },
    {
        "statement": "Let $C$ be a closed convex set containing the origin. Its gauge $\\gamma_C$ is the support function of a closed convex set containing the origin, namely\n\\[\nC^\\circ := \\{ s\\in\\mathbb{R}^n : \\langle s,d\\rangle \\le 1\\ \\text{for all } d\\in C\\},\n\\tag{3.2.8}\n\\]\nwhich defines the polar (set) of $C$.",
        "title": "",
        "label": "prop:FCA-chapC-3.2.4",
        "lean_tag": [
            "FCA_chap_C_3_2_4"
        ],
        "lean_formalization": "lemma FCA_chap_C_3_2_4 {n : ℕ}\n  (C : Set (EuclideanSpace ℝ (Fin n)))\n  (hC_closed : IsClosed C) (hC_convex : Convex ℝ C) (hC_origin : 0 ∈ C) :\n  let γC := gauge C\n  let C' := {s | ∀ d ∈ C, inner ℝ s d ≤ 1}\n  let σC' := SupportFun C'\n  ∀ (x : EuclideanSpace ℝ (Fin n)), γC x = σC' x := by\n  sorry",
        "proof": "We know that $\\gamma_C$ (which, by Theorem 1.2.5(i), is closed, sublinear and nonnegative) is the support function of some closed convex set containing the origin, say $D$; from (3.1.1),\n\\[\nD=\\{s\\in\\mathbb{R}^n:\\ \\langle s,d\\rangle \\le r\\ \\text{ for all }(d,r)\\in\\operatorname{epi}\\gamma_C\\}.\n\\]\nAs seen in (1.2.4), $\\operatorname{epi}\\gamma_C$ is the closed convex conical hull of $C\\times\\{1\\}$; we can use positive homogeneity to write\n\\[\nD=\\{s\\in\\mathbb{R}^n:\\ \\langle s,d\\rangle \\le 1\\ \\text{ for all }d\\ \\text{such that }\\gamma_C(d)\\le 1\\}.\n\\]\nIn view of Theorem 1.2.5(iii), the above index-set is just $C$; in other words, $D=C^\\circ$."
    },
    {
        "statement": "Let $C$ be a closed convex set containing the origin. Its support function $\\sigma_C$ is the gauge of $C^\\circ$.",
        "title": "",
        "label": "cor:FCA-chapC-3.2.5",
        "lean_tag": [
            "FCA_chap_C_3_2_5"
        ],
        "lean_formalization": "lemma FCA_chap_C_3_2_5 {n : ℕ}\n  (C : Set (EuclideanSpace ℝ (Fin n)))\n  (hC_closed : IsClosed C) (hC_convex : Convex ℝ C) (hC_origin : 0 ∈ C) :\n  let C' := {s | ∀ d ∈ C, inner ℝ s d ≤ 1}\n  let σC := SupportFun C\n  let γC' := gauge C'\n  ∀ (x : EuclideanSpace ℝ (Fin n)), γC' x = σC x := by\n  sorry",
        "proof": ""
    },
    {
        "statement": "Let $C$ be a nonempty compact convex set having $0$ in its interior, so that $C^\\circ$ enjoys the same properties. Then, for all $d$ and $s$ in $\\mathbb{R}^n$, the following statements are equivalent (the notation (3.2.9) is used)\n\\begin{enumerate}\n\\item[(i)] $H(s)$ is a supporting hyperplane to $C$ at $d$;\n\\item[(ii)] $H(d)$ is a supporting hyperplane to $C^\\circ$ at $s$;\n\\item[(iii)] $d\\in\\operatorname{bd}C,\\ s\\in\\operatorname{bd}C^\\circ\\text{ and }\\langle s,d\\rangle=1$;\n\\item[(iv)] $d\\in C,\\ s\\in C^\\circ\\text{ and }\\langle s,d\\rangle=1$.\n\\end{enumerate}",
        "title": "",
        "label": "prop:FCA-chapC-3.2.7",
        "lean_tag": [
            "FCA_chap_C_3_2_7"
        ],
        "lean_formalization": "lemma FCA_chap_C_3_2_7 {n : ℕ}\n  (C : Set (EuclideanSpace ℝ (Fin n)))\n  (d s : EuclideanSpace ℝ (Fin n))\n  (hC_nonempty : Set.Nonempty C)\n  (hC_compact : IsCompact C)\n  (hC_convex : Convex ℝ C)\n  (hC_origin : 0 ∈ interior C) :\n  let C' := {s | ∀ v ∈ C, inner ℝ s v ≤ 1}\n  let Hs := {y | inner ℝ s y = 1}\n  let Hd := {y | inner ℝ d y = 1}\n  List.TFAE [\n    ∃ (s' : EuclideanSpace ℝ (Fin n)) (t' : ℝ), (SupportingHyperplaneAt s' d t' C) ∧ (AffineHyperplane s' t' = Hs),\n    ∃ (d' : EuclideanSpace ℝ (Fin n)) (t' : ℝ), (SupportingHyperplaneAt d' s t' C) ∧ (AffineHyperplane d' t' = Hd),\n    (d ∈ frontier C) ∧ (s ∈ frontier C') ∧ (inner ℝ s d = 1),\n    (d ∈ C) ∧ (s ∈ C') ∧ (inner ℝ s d = 1)\n  ] := by\n  sorry",
        "proof": "Left as an exercise; the assumptions are present to make sure that every nonzero vector in $\\mathbb{R}^n$ does expose a face in each set."
    },
    {
        "statement": "Let $S_1$ and $S_2$ be nonempty closed convex sets; call $\\sigma_1$ and $\\sigma_2$ their support functions. Then\n\\[\nS_1\\subset S_2 \\iff \\sigma_1(d)\\le \\sigma_2(d)\\text{ for all }d\\in\\mathbb{R}^n.\n\\]",
        "title": "",
        "label": "thm:FCA-chapC-3.3.1",
        "lean_tag": [
            "FCA_chap_C_3_3_1"
        ],
        "lean_formalization": "lemma FCA_chap_C_3_3_1 {n : ℕ}\n  (S₁ S₂ : Set (EuclideanSpace ℝ (Fin n)))\n  (hS_nonempty : Set.Nonempty S₁ ∧ Set.Nonempty S₂)\n  (hS_closed : IsClosed S₁ ∧ IsClosed S₂)\n  (hS_convex : Convex ℝ S₁ ∧ Convex ℝ S₂) :\n  let σ₁ := SupportFun S₁\n  let σ₂ := SupportFun S₂\n  S₁ ⊆ S₂ ↔ ∀ d, σ₁ d ≤ σ₂ d := by\n  sorry",
        "proof": "Apply the equivalence stated in Corollary 3.1.2:\n\\[\nS_1\\subset S_2 \\iff s\\in S_2\\text{ for all }s\\in S_1\n\\]\n\\[\n\\iff \\sigma_2(d)\\ge\\langle s,d\\rangle\\text{ for all }s\\in S_1\\text{ and all }d\\in\\mathbb{R}^n\n\\]\n\\[\n\\iff \\sigma_2(d)\\ge\\sup_{s\\in S_1}\\langle s,d\\rangle\\text{ for all }d\\in\\mathbb{R}^n.\n\\]"
    },
    {
        "statement": "\\begin{enumerate}\n    \\item[(i)] Let $\\sigma_1$ and $\\sigma_2$ be the support functions of the nonempty closed convex sets $S_1$ and $S_2$. If $t_1$ and $t_2$ are positive, then\n    \\[\n    t_1\\sigma_1+t_2\\sigma_2\\ \\text{is the support function of }\\ \\overline{(t_1S_1+t_2S_2)}.\n    \\]\n    \\item[(ii)] Let $\\{\\sigma_j\\}_{j\\in J}$ be the support functions of the family of nonempty closed convex sets $\\{S_j\\}_{j\\in J}$. Then\n    \\[\n    \\sup_{j\\in J}\\sigma_j\\text{ is the support function of }\\overline{\\operatorname{co}}\\{\\,\\bigcup_{j\\in J}S_j:\\; j\\in J\\}.\n    \\]\n    \\item[(iii)] Let $\\{\\sigma_j\\}_{j\\in J}$ be the support functions of the family of closed convex sets $\\{S_j\\}_{j\\in J}$. If\n    \\[\n    S:=\\bigcap_{j\\in J}S_j\\neq\\varnothing,\n    \\]\n    then\n    \\[\n    \\sigma_S=\\overline{\\operatorname{co}}\\{\\inf\\sigma_j:\\; j\\in J\\}.\n    \\]\n\\end{enumerate}",
        "title": "",
        "label": "thm:FCA-chapC-3.3.2",
        "lean_tag": [
            "FCA_chap_C_3_3_2_i",
            "FCA_chap_C_3_3_2_ii",
            "FCA_chap_C_3_3_2_iii"
        ],
        "lean_formalization": "lemma FCA_chap_C_3_3_2_i {n : ℕ}\n  (S₁ S₂ : Set (EuclideanSpace ℝ (Fin n)))\n  (t₁ t₂ : ℝ)\n  (hS_nonempty : Set.Nonempty S₁ ∧ Set.Nonempty S₂)\n  (hS_closed : IsClosed S₁ ∧ IsClosed S₂)\n  (hS_convex : Convex ℝ S₁ ∧ Convex ℝ S₂)\n  (ht : t₁ > 0 ∧ t₂ > 0):\n  let σ₁ := SupportFun S₁\n  let σ₂ := SupportFun S₂\n  let S₁' := {x | ∃ s ∈ S₁, x = t₁ • s}\n  let S₂' := {x | ∃ s ∈ S₂, x = t₂ • s}\n  let S := closure {x | ∃ s₁ ∈ S₁', ∃ s₂ ∈ S₂', x = s₁ + s₂}\n  ∀ x, (t₁ • (σ₁ x) + t₂ • (σ₂ x)) = SupportFun S x := by\n  sorry",
        "proof": "[(i)] Call $S$ the closed convex set $\\operatorname{cl}(t_1S_1+t_2S_2)$. By definition, its support function is\n\\[\n\\sigma_S(d)=\\sup\\{\\langle t_1s_1+t_2s_2,d\\rangle:\\; s_1\\in S_1,\\; s_2\\in S_2\\}.\n\\]\nIn the above expression, $s_1$ and $s_2$ run independently in their index sets $S_1$ and $S_2$, $t_1$ and $t_2$ are positive, so\n\\[\n\\sigma_S(d)=t_1\\sup_{s\\in S_1}\\langle s,d\\rangle+t_2\\sup_{s\\in S_2}\\langle s,d\\rangle.\n\\]\n\n[(ii)] The support function of $S:=\\bigcup_{j\\in J}S_j$ is\n\\[\n\\sup_{s\\in\\bigcup_{j\\in J}S_j}\\langle s,d\\rangle=\\sup_{j\\in J}\\sup_{s_j\\in S_j}\\langle s_j,d\\rangle=\\sup_{j\\in J}\\sigma_j(d).\n\\]\nThis implies (ii) since $\\sigma_S=\\sigma_{\\overline{\\operatorname{co}}\\,S}$.\n\n[(iii)] The set $S:=\\bigcap_j S_j$ being nonempty, it has a support function $\\sigma_S$. Now, from Corollary 3.1.2,\n\\[\ns\\in S\\iff s\\in S_j\\text{ for all }j\\in J\n\\]\n\\[\n\\iff\\langle s,\\cdot\\rangle\\le\\sigma_j\\text{ for all }j\\in J\n\\]\n\\[\n\\iff\\langle s,\\cdot\\rangle\\le\\inf_{j\\in J}\\sigma_j\n\\]\n\\[\n\\iff\\langle s,\\cdot\\rangle\\le\\overline{\\operatorname{co}}\\bigl(\\inf_{j\\in J}\\sigma_j\\bigr),\n\\]\nwhere the last equivalence comes directly from the Definition B.2.5.3 of a closed convex hull. Again Corollary 3.1.2 tells us that the closed sublinear function $\\overline{\\operatorname{co}}(\\inf\\sigma_j)$ is just the support function of $S$."
    },
    {
        "statement": "Let $A:\\mathbb{R}^n\\to\\mathbb{R}^m$ be a linear operator, with adjoint $A^*$ (for some scalar product $\\langle\\cdot,\\cdot\\rangle$ in $\\mathbb{R}^m$). For $S\\subset\\mathbb{R}^n$ nonempty, we have\n\\[\n\\sigma_{\\mathrm{cl}\\,A(S)}(y)=\\sigma_{S}(A^*y)\\qquad\\text{for all }y\\in\\mathbb{R}^m.\n\\]",
        "title": "",
        "label": "prop:FCA-chapC-3.3.3",
        "lean_tag": [
            "FCA_chap_C_3_3_3"
        ],
        "lean_formalization": "lemma FCA_chap_C_3_3_3 {m n : ℕ}\n  (A : EuclideanSpace ℝ (Fin n) →ₗ[ℝ] EuclideanSpace ℝ (Fin m))\n  (S : Set (EuclideanSpace ℝ (Fin n)))\n  (hS_nonempty : Set.Nonempty S) :\n  let A_adj := A.adjoint\n  let cl_AS := closure (Set.image A S)\n  let σ_clAS := SupportFun cl_AS\n  let σ_S_Aadj := fun y => SupportFun S (A_adj y)\n  ∀ (y : EuclideanSpace ℝ (Fin m)), σ_clAS y = σ_S_Aadj y := by\n  sorry",
        "proof": "Just write the definitions\n\\[\n\\sigma_{A(S)}(y)=\\sup_{s\\in S}\\langle As,y\\rangle=\\sup_{s\\in S}\\langle s,A^*y\\rangle\n\\]\nand use Proposition 2.2.1 to obtain the result."
    },
    {
        "statement": "Let $A:\\mathbb{R}^m\\to\\mathbb{R}^n$ be a linear operator, with adjoint $A^*$ (for some scalar product $\\langle\\cdot,\\cdot\\rangle$ in $\\mathbb{R}^m$). Let $\\sigma$ be the support function of a nonempty closed convex set $S\\subset\\mathbb{R}^m$. If $\\sigma$ is minorized on the inverse image\n\\[\nA^{-1}(d)=\\{p\\in\\mathbb{R}^m:\\;Ap=d\\}\\tag{3.3.3}\n\\]\nof each $d\\in\\mathbb{R}^n$, then the support function of the set $(A^{-1})^*(S)$ is the closure of the image-function $A\\sigma$.",
        "title": "",
        "label": "prop:FCA-chapC-3.3.4",
        "lean_tag": [
            "FCA_chap_C_3_3_4"
        ],
        "lean_formalization": "lemma FCA_chap_C_3_3_4 {m n : ℕ}\n  (A : EuclideanSpace ℝ (Fin m) →ₗ[ℝ] EuclideanSpace ℝ (Fin n))\n  (s : EuclideanSpace ℝ (Fin m) → EuclideanSpace ℝ (Fin m) → ℝ)\n  (S : Set (EuclideanSpace ℝ (Fin m)))\n  (hs : IsScalarProduct s)\n  (hS_nonempty : Set.Nonempty S) (hS_closed : IsClosed S) (hS_convex : Convex ℝ S) :\n  let A_star := A.adjoint\n  let σ := SupportFun S\n  let A_inv := fun d => {p : EuclideanSpace ℝ (Fin m) | A p = d}\n  let A_adj_inv_S := {p : EuclideanSpace ℝ (Fin n) | ∃ d ∈ S, A_star p = d}\n  let σ_adj_inv := SupportFun A_adj_inv_S\n  (∀ (d : EuclideanSpace ℝ (Fin n)), IsMinorizedOn σ (A_inv d)) →\n  (∀ (x : EuclideanSpace ℝ (Fin n)), σ_adj_inv x = lscHull (fun v => ImageFunction A σ v) x) := by\n  sorry\n",
        "proof": "Our assumption is tailored to guarantee $A\\sigma\\in\\operatorname{Conv}\\mathbb{R}^n$ (Theorem B.2.4.2). The positive homogeneity of $A\\sigma$ is clear: for $d\\in\\mathbb{R}^n$ and $t>0$,\n\\[\n(A\\sigma)(td)=\\inf_{Ap=td}\\sigma(p)=\\inf_{A(p/t)=d}t\\sigma(p/t)=t\\inf_{Aq=d}\\sigma(q)=t(A\\sigma)(d).\n\\]\n\nThus, the closed sublinear function $\\operatorname{cl}(A\\sigma)$ supports some set $S'$; by definition,\n$s\\in S'$ if and only if\n\\[\n\\langle s,d\\rangle \\le \\inf\\{\\sigma(p):\\;Ap=d\\}\\qquad\\text{for all }d\\in\\mathbb R^n;\n\\]\nbut this just means\n\\[\n\\langle s,Ap\\rangle \\le \\sigma(p)\\qquad\\text{for all }p\\in\\mathbb R^m,\n\\]\ni.e.\\ $A^*s\\in S$, because $\\langle s,Ap\\rangle=\\langle A^*s,p\\rangle$."
    },
    {
        "statement": "Let $S$ and $S'$ be two nonempty compact convex sets of $\\mathbb{R}^n$. Then\n\\[\n\\Delta(\\sigma_S,\\sigma_{S'}) := \\max_{\\|d\\|\\le 1} |\\sigma_S(d)-\\sigma_{S'}(d)| = \\Delta_H(S,S') .\n\\tag{3.3.5}\n\\]",
        "title": "",
        "label": "thm:FCA-chapC-3.3.6",
        "lean_tag": "no-lean-tag",
        "proof": "As mentioned in \\S0.5.1, for all $r\\ge 0$, the property\n\\[\n\\max\\{d_S(d):\\, d\\in S'\\}\\le r\n\\tag{3.3.6}\n\\]\nsimply means $S'\\subset S+B(0,r)$.\n\nNow, the support function of $B(0,1)$ is $\\|\\cdot\\|$ --- see (2.3.1). Calculus rules on support functions therefore tell us that (3.3.6) is also equivalent to\n\\[\n\\sigma_{S'}(d)\\le \\sigma_S(d)+r\\|d\\|\\qquad\\text{for all }d\\in\\mathbb{R}^n,\n\\]\nwhich in turn can be written\n\\[\n\\max_{\\|d\\|\\le 1}\\big[\\sigma_{S'}(d)-\\sigma_S(d)\\big]\\le r.\n\\]\n\nIn summary, we have proved\n\\[\n\\max_{d\\in S'} d_S(d) = \\max_{\\|d\\|\\le 1}\\big[\\sigma_{S'}(d)-\\sigma_S(d)\\big]\n\\]\nand symmetrically\n\\[\n\\max_{d\\in S} d_{S'}(d) = \\max_{\\|d\\|\\le 1}\\big[\\sigma_S(d)-\\sigma_{S'}(d)\\big];\n\\]\nthe result follows."
    },
    {
        "statement": "A convex-compact-valued and locally bounded multifunction $F:\\mathbb{R}^n\\longrightarrow 2^{\\mathbb{R}^n}$ is outer [resp.\\ inner] semi-continuous at $x_0\\in\\operatorname{int}\\dom F$ if and only if its support function $x\\mapsto\\sigma_{F(x)}(d)$ is upper [resp.\\ lower] semi-continuous at $x_0$ for all $d$ of norm $1$.",
        "title": "",
        "label": "prop:FCA-chapC-3.3.7",
        "lean_tag": "no-lean-tag",
        "proof": "Calculus with support functions tells us that our definition (0.5.2) of outer semi-continuity is equivalent to\n\\[\n\\forall\\varepsilon>0,\\ \\exists\\delta>0:\\ y\\in B(x_0,\\delta)\\implies\n\\;\\sigma_{F(y)}(d)\\le\\sigma_{F(x_0)}(d)+\\varepsilon\\lVert d\\rVert\\quad\\text{for all }d\\in\\mathbb{R}^n\n\\]\nand division by $\\lVert d\\rVert$ shows that this is exactly upper semi-continuity of the support function for $\\lVert d\\rVert=1$. Same proof for inner/lower semi-continuity."
    },
    {
        "statement": "Let $(S_k)$ be a sequence of nonempty convex compact sets and $S$ a nonempty convex compact set. When $k \\to +\\infty$, the following are equivalent\n\\begin{enumerate}\n\\item[(i)] $S_k \\to S$ in the Hausdorff sense, i.e. $\\Delta_H(S_k,S)\\to 0$;\n\\item[(ii)] $\\sigma_{S_k}\\to\\sigma_S$ pointwise;\n\\item[(iii)] $\\sigma_{S_k}\\to\\sigma_S$ uniformly on each compact set of $\\mathbb{R}^n$.\n\\end{enumerate}",
        "title": "",
        "label": "cor:FCA-chapC-3.3.8",
        "lean_tag": "no-lean-tag",
        "proof": ""
    },
    {
        "statement": "For fixed $x$, the function $f'(x,\\cdot)$ is finite sublinear.",
        "title": "",
        "label": "prop:FCA-chapD-1.1.2",
        "lean_tag": [
            "FCA_chap_D_1_1_2"
        ],
        "lean_formalization": "lemma FCA_chap_D_1_1_2 {n : ℕ}\n  (f : EuclideanSpace ℝ (Fin n) → ℝ)\n  (x : EuclideanSpace ℝ (Fin n))\n  (hf : ConvexOn ℝ Set.univ f) :\n  let f' := fun d => directionalDeriv (liftRealtoEReal f) x d\n  (∀ (z : EuclideanSpace ℝ (Fin n)), f' z < ⊤ ∧ f' z > ⊥) ∧ (IsSublinear f') := by\n  sorry",
        "proof": "Let $d_1,d_2\\in\\mathbb{R}^n$, and positive $\\alpha_1,\\alpha_2$ with $\\alpha_1+\\alpha_2=1$. From the convexity of $f$:\n\\[\nf\\bigl(x+t(\\alpha_1d_1+\\alpha_2d_2)\\bigr)-f(x)=\n\\]\n\\[\nf\\bigl(\\alpha_1(x+td_1)+\\alpha_2(x+td_2)\\bigr)-\\alpha_1f(x)-\\alpha_2f(x)\\le\n\\]\n\\[\n\\le\\alpha_1\\bigl[f(x+td_1)-f(x)\\bigr]+\\alpha_2\\bigl[f(x+td_2)-f(x)\\bigr].\n\\]\nfor all $t$. Dividing by $t>0$ and letting $t\\downarrow0$, we obtain\n\\[\nf'(x,\\alpha_1d_1+\\alpha_2d_2)\\le\\alpha_1f'(x,d_1)+\\alpha_2f'(x,d_2)\n\\]\nwhich establishes the convexity of $f'$ with respect to $d$. Its positive homogeneity is clear: for $\\lambda>0$\n\\[\nf'(x,\\lambda d)=\\lim_{t\\downarrow0}\\lambda\\frac{f(x+\\lambda td)-f(x)}{\\lambda t}\n=\\lambda\\lim_{\\tau\\downarrow0}\\frac{f(x+\\tau d)-f(x)}{\\tau}\n=\\lambda f'(x,d).\n\\]\n\nFinally suppose $\\|d\\|=1$. As a finite convex function, $f$ is Lipschitz continuous around $x$ (Theorem B.3.1.2); in particular there exist $\\varepsilon>0$ and $L>0$ such that\n\\[\n|f(x+td)-f(x)|\\le Lt\\quad\\text{for }0\\le t\\le\\varepsilon.\n\\]\nHence, $|f'(x,d)|\\le L$ and we conclude with positive homogeneity:\n\\[\n|f'(x,d)|\\le L\\|d\\|\\qquad\\text{for all }d\\in\\mathbb{R}^n.\n\\tag{1.1.5}\n\\]"
    },
    {
        "statement": "The finite sublinear function $d\\mapsto\\sigma(d):=f'(x,d)$ satisfies\n\\[\n\\sigma'(0,\\delta)=f'(x,\\delta)\\qquad\\text{for all }\\delta\\in\\mathbb{R}^n;\n\\tag{1.1.8}\n\\]\n\\[\n\\sigma(\\delta)=\\sigma(0)+\\sigma'(0,\\delta)=\\sigma'(0,\\delta)\\qquad\\text{for all }\\delta\\in\\mathbb{R}^n;\n\\tag{1.1.9}\n\\]\n\\[\n\\partial\\sigma(0)=\\partial f(x).\n\\tag{1.1.10}\n\\]",
        "title": "",
        "label": "prop:FCA-chapD-1.1.6",
        "lean_tag": [
            "FCA_chap_D_1_1_6"
        ],
        "lean_formalization": "lemma FCA_chap_D_1_1_6 {n : ℕ}\n  (f : EuclideanSpace ℝ (Fin n) → ℝ)\n  (x : EuclideanSpace ℝ (Fin n))\n  (hf : ConvexOn ℝ Set.univ f) :\n  let σ := fun (d : EuclideanSpace ℝ (Fin n)) => directionalDeriv (liftRealtoEReal f) x d\n  let σ' := fun (d : EuclideanSpace ℝ (Fin n)) => directionalDeriv σ 0 d\n  (∀ (δ : EuclideanSpace ℝ (Fin n)), σ' δ = σ δ) ∧\n  (∀ (δ : EuclideanSpace ℝ (Fin n)), (σ δ = σ 0 + σ' δ) ∧ (σ δ = σ' δ)) ∧\n  (SubdifferentialI σ 0 = SubdifferentialI (liftRealtoEReal f) x) := by\n  sorry",
        "proof": "Because $\\sigma$ is positively homogeneous and $\\sigma(0)=0$,\n\\[\n\\frac{\\sigma(t\\delta)-\\sigma(0)}{t}=\\sigma(\\delta)=f'(x,\\delta)\\qquad\\text{for all }t>0.\n\\]\nThis implies immediately (1.1.8) and (1.1.9). Then (1.1.10) follows from uniqueness of the supported set."
    },
    {
        "statement": "The definitions 1.1.4 and 1.2.1 are equivalent.",
        "title": "",
        "label": "thm:FCA-chapD-1.2.2",
        "lean_tag": [
            "FCA_chap_D_1_2_2"
        ],
        "lean_formalization": "lemma FCA_chap_D_1_2_2 {n : ℕ}\n  (f : EuclideanSpace ℝ (Fin n) → ℝ)\n  (x : EuclideanSpace ℝ (Fin n)) (s : EuclideanSpace ℝ (Fin n))\n  (hf : ConvexOn ℝ Set.univ f) :\n  s ∈ SubdifferentialI (liftRealtoEReal f) x ↔ s ∈ SubdifferentialII (liftRealtoEReal f) x := by\n  sorry",
        "proof": "Let $s$ satisfy (1.1.6), i.e.\n\\[\n\\langle s,d\\rangle \\le f'(x,d)\\qquad\\text{for all }d\\in\\mathbb{R}^n. \\tag{1.2.2}\n\\]\n\nThe second equality in (1.1.2) makes it clear that (1.2.2) is equivalent to\n\\[\n\\langle s,d\\rangle \\le \\frac{f(x+td)-f(x)}{t}\\qquad\\text{for all }d\\in\\mathbb{R}^n\\text{ and }t>0. \\tag{1.2.3}\n\\]\n\nWhen $d$ describes $\\mathbb{R}^n$ and $t$ describes $\\mathbb{R}_+^*$, $y:=x+td$ describes $\\mathbb{R}^n$ and we realize that (1.2.3) is just (1.2.1)."
    },
    {
        "statement": "\\begin{enumerate}\n    \\item[(i)] A vector $s\\in\\mathbb{R}^n$ is a subgradient of $f$ at $x$ if and only if $(s,-1)\\in\\mathbb{R}^n\\times\\mathbb{R}$ is normal to $\\operatorname{epi}f$ at $(x,f(x))$.  In other words:\n    \\[\n    N_{\\operatorname{epi}f}(x,f(x))=\\{(\\lambda s,-\\lambda): s\\in\\partial f(x),\\ \\lambda\\ge 0\\}.\n    \\]\n    \\item[(ii)] The tangent cone to the set $\\operatorname{epi}f$ at $(x,f(x))$ is the epigraph of the directional-derivative function $d\\mapsto f'(x,d)$:\n    \\[\n    T_{\\operatorname{epi}f}(x,f(x))=\\{(d,r): r\\ge f'(x,d)\\}.\n    \\]\n\\end{enumerate}",
        "title": "",
        "label": "prop:FCA-chapD-1.3.1",
        "lean_tag": [
            "FCA_chap_D_1_3_1"
        ],
        "lean_formalization": "lemma FCA_chap_D_1_3_1 {n : ℕ}\n  (f : EuclideanSpace ℝ (Fin n) → ℝ)\n  (x s : EuclideanSpace ℝ (Fin n))\n  (hf : ConvexOn ℝ Set.univ f) :\n  let f' := fun (d : EuclideanSpace ℝ (Fin n)) => directionalDeriv (liftRealtoEReal f) x\n  let f'_epi := {v : EuclideanSpace ℝ (Fin (n + 1)) |\n                     ∃ z ∈ (epigraph (f' x)), v = Fin.snoc z.1 z.2}\n  let epi_concat := {v : EuclideanSpace ℝ (Fin (n + 1)) |\n                         ∃ z ∈ (epigraph (liftRealtoEReal f)), v = Fin.snoc z.1 z.2}\n  let s' : EuclideanSpace ℝ (Fin (n + 1)) := Fin.snoc s (-1)\n  (IsSubgradientAt (liftRealtoEReal f) x s ↔ IsNormalTo epi_concat (Fin.snoc x (f x)) s') ∧\n  (tangentConeAt ℝ epi_concat (Fin.snoc x (f x)) = f'_epi) := by\n  sorry",
        "proof": "[(i)] Apply Definition A.5.2.3 to see that $(s,-1)\\in N_{\\operatorname{epi}f}(x,f(x))$ means\n\\[\n\\langle s,y-x\\rangle+(-1)[r-f(x)]\\le 0\\qquad\\text{for all }y\\in\\mathbb{R}^n\\text{ and }r\\ge f(y)\n\\]\nand the equivalence with (1.2.1) is clear.  The formula follows since the set of normals forms a cone containing the origin.\n\n[(ii)] The tangent cone to $\\operatorname{epi}f$ is the polar of the above normal cone, i.e. the set of $(d,r)\\in\\mathbb{R}^n\\times\\mathbb{R}$ such that\n\\[\n\\langle\\lambda s,d\\rangle+(-\\lambda)r\\le 0\\qquad\\text{for all }s\\in\\partial f(x)\\text{ and }\\lambda\\ge 0.\n\\]\nBarring the trivial case $\\lambda=0$, we divide by $\\lambda>0$ to obtain\n\\[\nr\\ge\\max\\{\\langle s,d\\rangle: s\\in\\partial f(x)\\}=f'(x,d).\n\\]"
    },
    {
        "statement": "For the convex function $f:\\mathbb{R}^n\\to\\mathbb{R}$ and the sublevel-set (1.3.1), we have\n\\[\nT_{S_{f(x)} }(x)\\subset\\{d:\\;f'(x,d)\\le 0\\}. \\tag{1.3.2}\n\\]",
        "title": "",
        "label": "lem:FCA-chapD-1.3.2",
        "lean_tag": [
            "FCA_chap_D_1_3_2"
        ],
        "lean_formalization": "lemma FCA_chap_D_1_3_2 {n : ℕ}\n  (f : EuclideanSpace ℝ (Fin n) → ℝ)\n  (x : EuclideanSpace ℝ (Fin n))\n  (hf : ConvexOn ℝ Set.univ f) :\n  let S := sublevelSetFun (liftRealtoEReal f) x\n  tangentConeAt ℝ S x ⊆ {d | directionalDeriv (liftRealtoEReal f) x d ≤ 0} := by\n  sorry",
        "proof": "Take arbitrary $y\\in S_{f(x)},\\ t>0$, and set $d:=t(y-x)$. Then, using the second equality in (1.1.2),\n\\[\n0\\ge t[f(y)-f(x)]=\\frac{f(x+d/t)-f(x)}{1/t}\\ge f'(x,d).\n\\]\nSo we have proved\n\\[\n\\mathbb{R}^+[S_{f(x)}-x]\\subset\\{d:\\;f'(x,d)\\le 0\\} \\tag{1.3.3}\n\\]\n(note: the case $d=0$ is covered since $0\\in S_{f(x)}-x$).\n\nBecause $f'(\\cdot,\\cdot)$ is a closed function, the righthand set in (1.3.3) is closed. Knowing that $T_{S_{f(x)}}(x)$ is the closure of the lefthand side in (1.3.3) (Proposition A.5.2.1), we deduce the result by taking the closure of both sides in (1.3.3)."
    },
    {
        "statement": "Let $g:\\mathbb{R}^n\\to\\mathbb{R}$ be convex and suppose that $g(x_0)<0$ for some $x_0\\in\\mathbb{R}^n$. Then\n\\[\n\\operatorname{cl}\\{z: g(z)<0\\}=\\{z: g(z)\\le 0\\},\\tag{1.3.4}\n\\]\n\\[\n\\{z: g(z)<0\\}=\\operatorname{int}\\{z: g(z)\\le 0\\}.\\tag{1.3.5}\n\\]\nIt follows\n\\[\n\\operatorname{bd}\\{z: g(z)\\le 0\\}=\\{z: g(z)=0\\}.\\tag{1.3.6}\n\\]",
        "title": "",
        "label": "prop:FCA-chapD-1.3.3",
        "lean_tag": [
            "FCA_chap_D_1_3_3"
        ],
        "lean_formalization": "lemma FCA_chap_D_1_3_3 {n : ℕ}\n  (g : EuclideanSpace ℝ (Fin n) → ℝ)\n  (hg_convex : ConvexOn ℝ Set.univ g)\n  (hg_neg : ∃ (x₀ : EuclideanSpace ℝ (Fin n)), g x₀ < 0):\n  (closure {z | g z < 0} = {z | g z ≤ 0}) ∧\n  ({z | g z < 0} = interior {z | g z ≤ 0}) ∧\n  (frontier {z | g z < 0} = {z | g z = 0}) := by\n  sorry",
        "proof": "Because $g$ is (lower semi-) continuous, the inclusion ``$\\subset$'' automatically holds in (1.3.4). Conversely, let $\\bar z$ be arbitrary with $g(\\bar z)\\le 0$ and, for $k>0$, set\n\\[\nz_k:=\\tfrac{1}{k}x_0+(1-\\tfrac{1}{k})\\bar z.\n\\]\nBy convexity of $g$, $g(z_k)<0$, so (1.3.4) is established by letting $k\\to+\\infty$.\n\nNow, take the interior of both sides in (1.3.4). The ``int cl'' on the left is actually an ``int'' (Proposition A.2.1.8), and this ``int''-operation is useless because $g$ is (upper semi-) continuous: (1.3.5) is established."
    },
    {
        "statement": "Let $f:\\mathbb{R}^n\\to\\mathbb{R}$ be convex and suppose $0\\notin\\partial f(x)$. Then, $S_{f}(x)$ being the sublevel-set (1.3.1),\n\\[\nT_{S_{f}(x)}(x)=\\{d\\in\\mathbb{R}^n:\\;f'(x,d)\\le 0\\}\\tag{1.3.7}\n\\]\n\\[\n\\operatorname{int}\\big[T_{S_{f}(x)}(x)\\big]=\\{d\\in\\mathbb{R}^n:\\;f'(x,d)<0\\}\\neq\\varnothing.\\tag{1.3.8}\n\\]",
        "title": "",
        "label": "thm:FCA-chapD-1.3.4",
        "lean_tag": [
            "FCA_chap_D_1_3_4"
        ],
        "lean_formalization": "lemma FCA_chap_D_1_3_4 {n : ℕ}\n  (f : EuclideanSpace ℝ (Fin n) → ℝ)\n  (x : EuclideanSpace ℝ (Fin n))\n  (hf_convex : ConvexOn ℝ Set.univ f)\n  (hf_subdiff : Set.Nonempty (SubdifferentialI (liftRealtoEReal f) x)) :\n  let S := sublevelSetFun (liftRealtoEReal f) x\n  (tangentConeAt ℝ S x = {d | directionalDeriv (liftRealtoEReal f) x d ≤ 0}) ∧\n  (interior (tangentConeAt ℝ S x) = {d | directionalDeriv (liftRealtoEReal f) x d < 0}) ∧\n  Set.Nonempty (interior (tangentConeAt ℝ S x)):= by\n  sorry",
        "proof": "From the very definition (1.1.6), our assumption means that $f'(x,d)<0$ for some $d$, and (1.1.2) then implies that $f(x+td)<f(x)$ for $t>0$ small enough: our $d$ is of the form $(x+td-x)/t$ with $x+td\\in S_{f}(x)$ and we have proved\n\\[\n\\{d:\\;f'(x,d)<0\\}\\subset\\mathbb{R}^+[S_{f}(x)-x]\\subset T_{S_{f}(x)}(x).\\tag{1.3.9}\n\\]\n\nNow, we can apply (1.3.4) with $g=f'(x,\\cdot)$:\n\\[\n\\operatorname{cl}\\{d:\\;f'(x,d)<0\\}=\\{d:\\;f'(x,d)\\le 0\\},\n\\]\nso (1.3.7) is proved by closing the sets in (1.3.9) and using (1.3.2). Finally, take the interior of both sides in (1.3.7) and apply (1.3.5) with $g=f'(x,\\cdot)$ to prove (1.3.8)."
    },
    {
        "statement": "Let $f:\\mathbb{R}^n\\to\\mathbb{R}$ be convex and suppose $0\\notin\\partial f(x)$.  Then a direction $d$ is normal to $S f(x)$ at $x$ if and only if there is some $t\\ge 0$ and some $s\\in\\partial f(x)$ such that $d=ts$:\n\\[\nN_{S f(x)}(x)=\\mathbb{R}^+ \\partial f(x).\n\\]",
        "title": "",
        "label": "thm:FCA-chapD-1.3.5",
        "lean_tag": [
            "FCA_chap_D_1_3_5"
        ],
        "lean_formalization": "lemma FCA_chap_D_1_3_5 {n : ℕ}\n  (f : EuclideanSpace ℝ (Fin n) → ℝ)\n  (x d : EuclideanSpace ℝ (Fin n))\n  (hf_convex : ConvexOn ℝ Set.univ f)\n  (hf_subdiff : Set.Nonempty (SubdifferentialI (liftRealtoEReal f) x)) :\n  let S := sublevelSetFun (liftRealtoEReal f) x\n  (IsNormalTo S x d) ↔\n  (∃ (t : ℝ) (s : EuclideanSpace ℝ (Fin n)),\n  (t ≥ 0) ∧ (s ∈ SubdifferentialI (liftRealtoEReal f) x) ∧ (d = t • s)) := by\n  sorry",
        "proof": "Write (1.3.7) as\n\\[\nT_{S f(x)}(x)=\\{d\\in\\mathbb{R}^n:\\langle s,d\\rangle\\le 0\\ \\text{for all }s\\in\\partial f(x)\\}\n=\\{d\\in\\mathbb{R}^n:\\langle \\lambda s,d\\rangle\\le 0\\ \\text{for all }\\lambda\\ge 0\\ \\text{and }s\\in\\partial f(x)\\}=[\\mathbb{R}^+\\partial f(x)]^\\circ.\n\\]\nThe result follows by taking the polar cone of both sides, and observing that the assumption implies closedness of $\\mathbb{R}^+\\partial f(x)$ (Proposition A.1.4.7):\n\\[\nN_{S f(x)}(x)=\\operatorname{cl}[\\mathbb{R}^+\\partial f(x)]=\\mathbb{R}^+\\partial f(x).\n\\]"
    },
    {
        "statement": "Let $f:\\mathbb{R}^n\\to\\mathbb{R}$ be convex and $x\\in\\mathbb{R}^n$. For any $\\varepsilon>0$, there exists $\\delta>0$ such that $\\|h\\|\\le\\delta$ implies\n\\[\n\\big|f(x+h)-f(x)-f'(x,h)\\big|\\le\\varepsilon\\|h\\|.\n\\tag{2.1.1}\n\\]",
        "title": "",
        "label": "lem:FCA-chapD-2.1.1",
        "lean_tag": [
            "FCA_chap_D_2_1_1"
        ],
        "lean_formalization": "lemma FCA_chap_D_2_1_1 {n : ℕ}\n  (f : EuclideanSpace ℝ (Fin n) → ℝ)\n  (x : EuclideanSpace ℝ (Fin n))\n  (hf_convex : ConvexOn ℝ Set.univ f) :\n  let f'_ereal := fun (d : EuclideanSpace ℝ (Fin n)) => directionalDeriv (liftRealtoEReal f) x d\n  let f'_real := fun (d : EuclideanSpace ℝ (Fin n)) => realDirectionalDeriv f x d\n  (∀ x, f'_ereal x = f'_real x) ∧\n  (∀ (ε : ℝ), (ε > 0) → ∃ δ > 0,\n  ∀ (h : EuclideanSpace ℝ (Fin n)), ‖h‖ ≤ δ →\n  abs (f (x + h) - f x - realDirectionalDeriv f x h) ≤ ε • ‖h‖) := by\n  sorry",
        "proof": "Suppose for contradiction that there is $\\varepsilon>0$ and a sequence $(h_k)$ with $\\|h_k\\|=:t_k\\le 1/k$ such that\n\n\\[\n|f(x+h_k)-f(x)-f'(x,h_k)|>\\varepsilon t_k\\qquad\\text{for }k=1,2,\\dots\n\\]\n\nExtracting a subsequence if necessary, assume that $h_k/t_k\\to d$ for some $d$ of norm $1$. Then take a local Lipschitz constant $L$ of $f$ (see Remark 1.1.3) and expand:\n\\[\n\\varepsilon t_k<|f(x+h_k)-f(x)-f'(x,h_k)|\n\\le |f(x+h_k)-f(x+t_kd)|+\n\\]\n\\[\n\\quad +|f(x+t_kd)-f(x)-f'(x,t_kd)|+|f'(x,t_kd)-f'(x,h_k)|\n\\]\n\\[\n\\le 2L\\|h_k-t_kd\\|+|f(x+t_kd)-f(x)-t_kf'(x,d)|.\n\\]\n\nDivide by $t_k>0$ and pass to the limit to obtain the contradiction $\\varepsilon\\le 0$."
    },
    {
        "statement": "Let $f:\\mathbb{R}^n\\to\\mathbb{R}$ be convex. At any $x$,\n\\[\nf(x+h)=f(x)+\\langle s,h\\rangle+o(\\|h\\|)\n\\]\nwhenever one of the following equivalent properties holds:\n\\[\ns\\in\\partial f(x)(h)\\iff h\\in N_{\\partial f(x)}(s)\\iff s=p_{\\partial f(x)}(s+h).\n\\]",
        "title": "",
        "label": "cor:FCA-chapD-2.1.3",
        "lean_tag": [
            "FCA_chap_D_2_1_3"
        ],
        "lean_formalization": "lemma FCA_chap_D_2_1_3 {n : ℕ}\n  (f : EuclideanSpace ℝ (Fin n) → ℝ)\n  (s h : EuclideanSpace ℝ (Fin n))\n  (hf_convex : ConvexOn ℝ Set.univ f) :\n  (∀ (x : EuclideanSpace ℝ (Fin n)), (h ∈ normalConeAt (SubdifferentialI (liftRealtoEReal f) x) s) →\n  Asymptotics.IsLittleO (𝓝 0) (fun h => f (x + h) - f x - inner ℝ s h) (fun h => ‖h‖)) ∧\n  (∀ (x : EuclideanSpace ℝ (Fin n)), (s ∈ ExposedFace (SubdifferentialI (liftRealtoEReal f) x) h) →\n  Asymptotics.IsLittleO (𝓝 0) (fun h => f (x + h) - f x - inner ℝ s h) (fun h => ‖h‖)) := by\n  sorry",
        "proof": ""
    },
    {
        "statement": "If the convex $f$ is (G\\^ateaux) differentiable at $x$, its only subgradient at $x$ is its gradient $\\nabla f(x)$. Conversely, if $\\partial f(x)$ contains only one element $s$, then $f$ is (Fr\\'echet) differentiable at $x$, with $\\nabla f(x)=s$.",
        "title": "",
        "label": "cor:FCA-chapD-2.1.4",
        "lean_tag": "no-lean-tag",
        "proof": ""
    },
    {
        "statement": "Let $f:\\mathbb{R}^n\\to\\mathbb{R}$ be convex. For all $x$ and $d$ in $\\mathbb{R}^n$, we have\n\\[\nF_{\\partial f(x)}(d)=\\partial[f'(x,\\cdot)](d).\n\\]",
        "title": "",
        "label": "prop:FCA-chapD-2.1.5",
        "lean_tag": [
            "FCA_chap_D_2_1_5"
        ],
        "lean_formalization": "lemma FCA_chap_D_2_1_5 {n : ℕ}\n  (f : EuclideanSpace ℝ (Fin n) → ℝ)\n  (x d : EuclideanSpace ℝ (Fin n))\n  (hf_convex : ConvexOn ℝ Set.univ f) :\n  let f' := fun (d : EuclideanSpace ℝ (Fin n)) => directionalDeriv (liftRealtoEReal f) x d\n  SubdifferentialI f' d = ExposedFace (SubdifferentialI (liftRealtoEReal f) x) d := by\n  sorry",
        "proof": "If $s\\in\\partial f(x)$ then $f'(x,d')\\ge\\langle s,d'\\rangle$ for all $d'\\in\\mathbb{R}^n$, simply because $f'(x,\\cdot)$ is the support function of $\\partial f(x)$. If, in addition, $\\langle s,d\\rangle=f'(x,d)$, we get\n\\[\nf'(x,d')\\ge f'(x,d)+\\langle s,d'-d\\rangle\\qquad\\text{for all }d'\\in\\mathbb{R}^n\n\\tag{2.1.4}\n\\]\nwhich proves the inclusion $F_{\\partial f(x)}(d)\\subset\\partial[f'(x,\\cdot)](d)$.\n\nConversely, let $s$ satisfy (2.1.4). Set $d'':=d'-d$ and deduce from subadditivity\n\\[\nf'(x,d)+f'(x,d'')\\ge f'(x,d')\\ge f'(x,d)+\\langle s,d''\\rangle\\qquad\\text{for all }d''\\in\\mathbb{R}^n\n\\]\nwhich implies $f'(x,\\cdot)\\ge\\langle s,\\cdot\\rangle$, hence $s\\in\\partial f(x)$. Also, putting $d'=0$ in (2.1.4) shows that $\\langle s,d\\rangle\\ge f'(x,d)$. Altogether, we have $s\\in F_{\\partial f(x)}(d)$. \\qedhere"
    },
    {
        "statement": "For $f:\\mathbb{R}^n\\to\\mathbb{R}$ convex, the following three properties are equivalent:\n\\begin{enumerate}\n\\item[(i)] $f$ is minimized at $x$ over $\\mathbb{R}^n$, i.e., $f(y)\\ge f(x)$ for all $y\\in\\mathbb{R}^n$;\n\\item[(ii)] $0\\in\\partial f(x)$;\n\\item[(iii)] $f'(x,d)\\ge 0$ for all $d\\in\\mathbb{R}^n$.\n\\end{enumerate}",
        "title": "",
        "label": "thm:FCA-chapD-2.2.1",
        "lean_tag": [
            "FCA_chap_D_2_2_1"
        ],
        "lean_formalization": "lemma FCA_chap_D_2_2_1 {n : ℕ}\n  (f : EuclideanSpace ℝ (Fin n) → ℝ)\n  (x : EuclideanSpace ℝ (Fin n))\n  (hf_convex : ConvexOn ℝ Set.univ f) :\n  List.TFAE [\n    ∀ (y : EuclideanSpace ℝ (Fin n)), f y ≥ f x,\n    0 ∈ SubdifferentialI (liftRealtoEReal f) x,\n    ∀ (d : EuclideanSpace ℝ (Fin n)), directionalDeriv (liftRealtoEReal f) x d ≥ 0\n  ] := by\n  sorry",
        "proof": "The equivalence (i) $\\Leftrightarrow$ (ii) [resp.\\ (ii) $\\Leftrightarrow$ (iii)] is obvious from (1.2.1) [resp.\\ (1.1.6)]."
    },
    {
        "statement": "The subdifferential of $\\varphi$ defined by (2.3.1) is\n\\[\n\\partial\\varphi(t)=\\{\\;s,y-x\\;:\\;s\\in\\partial f(x_t)\\;\\}\n\\]\nor, more symbolically:\n\\[\n\\partial\\varphi(t)=\\langle\\partial f(x_t),\\,y-x\\rangle.\n\\]",
        "title": "",
        "label": "lem:FCA-chapD-2.3.1",
        "lean_tag": [
            "FCA_chap_D_2_3_1"
        ],
        "lean_formalization": "lemma FCA_chap_D_2_3_1 {n : ℕ}\n  (f : EuclideanSpace ℝ (Fin n) → ℝ)\n  (x y : EuclideanSpace ℝ (Fin n)) (t : ℝ)\n  (hf_convex : ConvexOn ℝ Set.univ f) (ht : 0 ≤ t ∧ t ≤ 1):\n  let xt := t • y + (1 - t) • x\n  let phi := fun (v : EuclideanSpace ℝ (Fin 1)) => f xt\n  let t_asvec := fun _ => t\n  -- work in ℝ, rather than EuclideanSpace ℝ (Fin n)\n  let subdiff := {v : ℝ | ∃ v' ∈ SubdifferentialI (liftRealtoEReal phi) t_asvec, v' 0 = v}\n  subdiff = {z : ℝ | ∃ s ∈ (SubdifferentialI (liftRealtoEReal f) xt), z = inner ℝ s (y - x)} := by\n  sorry",
        "proof": "In terms of right- and left-derivatives (see Theorem 0.6.3), we have\n\\[\nD_{+}\\varphi(t)=\\lim_{\\tau\\downarrow 0}\\frac{f(x_t+\\tau(y-x))-f(x_t)}{\\tau}=f'(x_t,y-x),\n\\]\n\\[\nD_{-}\\varphi(t)=\\lim_{\\tau\\uparrow 0}\\frac{f(x_t+\\tau(y-x))-f(x_t)}{\\tau}=-f'(x_t,-(y-x));\n\\]\nso, knowing that\n\\[\nf'(x_t,y-x)=\\max_{s\\in\\partial f(x_t)}\\langle s,y-x\\rangle,\n\\]\n\\[\n-\\,f'(x_t,-(y-x))=\\min_{s\\in\\partial f(x_t)}\\langle s,y-x\\rangle,\n\\]\nwe obtain \\(\\partial\\varphi(t):=[D_{-}\\varphi(t),D_{+}\\varphi(t)]=\\{\\langle s,y-x\\rangle : s\\in\\partial f(x)\\}.\\)"
    },
    {
        "statement": "Let $f:\\mathbb{R}^n\\to\\mathbb{R}$ be convex. Given two points $x\\neq y$ in $\\mathbb{R}^n$, there exist $t\\in ]0,1[$ and $s\\in\\partial f(x_t)$ such that\n\\[\nf(y)-f(x)=\\langle s,\\,y-x\\rangle.\n\\tag{2.3.2}\n\\]\nIn other words,\n\\[\nf(y)-f(x)\\in\\bigcup_{t\\in]0,1[}\\{\\langle\\partial f(x_t),\\,y-x\\rangle\\}.\n\\]",
        "title": "",
        "label": "thm:FCA-chapD-2.3.3",
        "lean_tag": [
            "FCA_chap_D_2_3_3"
        ],
        "lean_formalization": "lemma FCA_chap_D_2_3_3 {n : ℕ}\n  (f : EuclideanSpace ℝ (Fin n) → ℝ)\n  (x y : EuclideanSpace ℝ (Fin n))\n  (hf_convex : ConvexOn ℝ Set.univ f) (hxy : x ≠ y) :\n  ∃ t ∈ Set.Ioo 0 1, ∃ s ∈ SubdifferentialI (liftRealtoEReal f) (t • y + (1 - t) • x),\n  f y - f x = inner ℝ s (y - x) := by\n  sorry",
        "proof": "Start from the function $\\varphi$ of (2.3.1) and, as usual in this context, consider the auxiliary function\n\\[\n\\psi(t):=\\varphi(t)-\\varphi(0)-t[\\varphi(1)-\\varphi(0)],\n\\]\nwhich is clearly convex. Computing directional derivatives gives easily $\\partial\\psi(t)=\\partial\\varphi(t)-[\\varphi(1)-\\varphi(0)]$. Now $\\psi$ is continuous on $[0,1]$, it has been constructed so that $\\psi(0)=\\psi(1)=0$, so it is minimal at some $t\\in ]0,1[$: at this $t$, $0\\in\\partial\\psi(t)$ (Theorem 2.2.1). In view of Lemma 2.3.1, this means that there is $s\\in\\partial f(x_t)$ such that\n\\[\n\\langle s,\\,y-x\\rangle=\\varphi(1)-\\varphi(0)=f(y)-f(x).\n\\]"
    },
    {
        "statement": "Let $f:\\mathbb{R}^n\\to\\mathbb{R}$ be convex. For $x,y\\in\\mathbb{R}^n$,\n\\[\nf(y)-f(x)=\\int_0^1\\langle\\partial f(xt),\\,y-x\\rangle\\,dt.\n\\]",
        "title": "",
        "label": "thm:FCA-chapD-2.3.4",
        "lean_tag": "no-lean-tag",
        "proof": ""
    },
    {
        "statement": "Let $f_1,f_2$ be two convex functions from $\\mathbb{R}^n$ to $\\mathbb{R}$ and $t_1,t_2$ be positive. Then\n\\[\n\\partial (t_1 f_1 + t_2 f_2)(x) = t_1\\partial f_1(x) + t_2\\partial f_2(x)\n\\qquad\\text{for all }x\\in\\mathbb{R}^n.\n\\tag{4.1.1}\n\\]",
        "title": "",
        "label": "thm:FCA-chapD-4.1.1",
        "lean_tag": [
            "FCA_chap_D_4_1_1"
        ],
        "lean_formalization": "lemma FCA_chap_D_4_1_1 {n : ℕ}\n  (f₁ f₂ : EuclideanSpace ℝ (Fin n) → ℝ)\n  (t₁ t₂ : ℝ)\n  (hf_convex : ConvexOn ℝ Set.univ f₁ ∧ ConvexOn ℝ Set.univ f₂)\n  (ht : t₁ > 0 ∧ t₂ > 0) :\n  ∀ (x : EuclideanSpace ℝ (Fin n)),\n  SubdifferentialI (fun x => t₁ • (f₁ x) + t₂ • (f₂ x)) x =\n  {v | ∃ v₁ ∈ (SubdifferentialI (liftRealtoEReal f₁) x), ∃ v₂ ∈ (SubdifferentialI (liftRealtoEReal f₂) x),\n       v = t₁ • v₁ + t₂ • v₂} := by\n  sorry",
        "proof": "Apply Theorem C.3.3.2(i): $t_1\\partial f_1(x)+t_2\\partial f_2(x)$ is a compact convex set whose support function is\n\\[\nt_1 f_1'(x,\\cdot) + t_2 f_2'(x,\\cdot).\n\\tag{4.1.2}\n\\]\nOn the other hand, the support function of $\\partial (t_1 f_1 + t_2 f_2)(x)$ is by definition the directional derivative $(t_1 f_1 + t_2 f_2)'(x,\\cdot)$ which, from elementary calculus, is just (4.1.2). Therefore the two (compact convex) sets in (4.1.1) coincide, since they have the same support function."
    },
    {
        "statement": "Let $A:\\mathbb{R}^n\\to\\mathbb{R}^m$ be an affine mapping ( $Ax = A_0x + b$, with $A_0$ linear and $b\\in\\mathbb{R}^m$) and let $g$ be a finite convex function on $\\mathbb{R}^m$. Then\n\\[\n\\partial (g\\circ A)(x)=A_0^*\\partial g(Ax)\\qquad\\text{for all }x\\in\\mathbb{R}^n.\n\\tag{4.2.1}\n\\]",
        "title": "",
        "label": "thm:FCA-chapD-4.2.1",
        "lean_tag": [
            "FCA_chap_D_4_2_1"
        ],
        "lean_formalization": "lemma FCA_chap_D_4_2_1 {n m : ℕ}\n  (g : EuclideanSpace ℝ (Fin m) → ℝ)\n  (A₀ : EuclideanSpace ℝ (Fin n) →ₗ[ℝ] EuclideanSpace ℝ (Fin m))\n  (b : EuclideanSpace ℝ (Fin m))\n  (hf_convex : ConvexOn ℝ Set.univ g) :\n  ∀ (x : EuclideanSpace ℝ (Fin n)),\n  (SubdifferentialI (fun v => g (A₀ v + b)) x) = Set.image A₀.adjoint (SubdifferentialI (liftRealtoEReal g) (A₀ x + b)) := by\n  sorry",
        "proof": "Form the difference quotient giving rise to $(g\\circ A)'(x,d)$ and use the relation $A(x+td)=Ax+tA_0d$ to obtain\n\\[\n(g\\circ A)'(x,d)=g'(Ax,A_0d)\\qquad\\text{for all }d\\in\\mathbb{R}^n.\n\\]\nFrom Proposition C.3.3.3, the righthand side in the above equality is the support function of the convex compact set $A_0^*\\partial g(Ax)$. \\qedhere"
    },
    {
        "statement": "Let $f$, $F$ and $g$ be defined as above. For all $x\\in\\mathbb{R}^n$,\n\\[\n\\partial (g\\circ F)(x)=\\big\\{\\sum_{i=1}^m \\rho^i s_i :\\; (\\rho^1,\\dots,\\rho^m)\\in\\partial g(F(x)),\n\\; s_i\\in\\partial f_i(x)\\ \\text{ for } i=1,\\dots,m\\big\\}.\n\\tag{4.3.1}\n\\]",
        "title": "",
        "label": "thm:FCA-chapD-4.3.1",
        "lean_tag": [
            "FCA_chap_D_4_3_1"
        ],
        "lean_formalization": "lemma FCA_chap_D_4_3_1 {n m : ℕ}\n  (f : ℕ → (EuclideanSpace ℝ (Fin n) → ℝ))\n  (g : EuclideanSpace ℝ (Fin m) → ℝ)\n  (hf_convex : ∀ i ∈ Finset.range m, ConvexOn ℝ Set.univ (f i))\n  (hg_convex : ConvexOn ℝ Set.univ g)\n  (hg_increasing : ∀ y z, (∀ i, y i ≥ z i) → g y ≥ g z) :\n  let F : (EuclideanSpace ℝ (Fin n)) → EuclideanSpace ℝ (Fin m) :=\n          fun x => (fun i => (f i) x)\n  ∀ (x : EuclideanSpace ℝ (Fin n)),\n  SubdifferentialI (liftRealtoEReal (g ∘ F)) x =\n  {v | ∃ ρ ∈ SubdifferentialI (liftRealtoEReal g) (F x), ∃ (s : ℕ → EuclideanSpace ℝ (Fin n)),\n       (∀ i ∈ Finset.range m, s i ∈ SubdifferentialI (liftRealtoEReal (f i)) x) ∧\n       (v = ∑ i : Fin m, (ρ i) • (s i))} := by\n  sorry",
        "proof": "[Preamble] Our aim is to show the formula via support functions, hence we need to establish the convexity and compactness of the righthand side in (4.3.1) -- call it $S$. Boundedness and closedness are easy, coming from the fact that a subdifferential (be it $\\partial g$ or $\\partial f_i$) is bounded and closed. As for convexity, pick two points in $S$ and form their convex combination\n\\[\ns=\\alpha\\sum_{i=1}^m \\rho^i s_i + (1-\\alpha)\\sum_{i=1}^m \\rho^{\\prime i} s_i' =\n\\sum_{i=1}^m\\big[\\alpha\\rho^i s_i + (1-\\alpha)\\rho^{\\prime i}s_i'\\big],\n\\]\nwhere $\\alpha\\in[0,1]$. Remember that each $\\rho^i$ and $\\rho^{\\prime i}$ is nonnegative and the above sum can be restricted to those terms such that $\\rho^{\\prime\\prime i}:=\\alpha\\rho^i+(1-\\alpha)\\rho^{\\prime i}>0$. Then we write each such term as\n\\[\n\\rho^{\\prime\\prime i}\\big[\\frac{\\alpha\\rho^i}{\\rho^{\\prime\\prime i}} s_i +\n\\frac{(1-\\alpha)\\rho^{\\prime i}}{\\rho^{\\prime\\prime i}} s_i'\\big].\n\\]\nIt suffices to observe that $\\rho^{\\prime\\prime i}\\in\\partial g(F(x))$, so the bracketed expression is in $\\partial f_i(x)$; thus $s\\in S$.\n\n[Step 1] Now let us compute the support function $\\sigma_S$ of $S$. For $d\\in\\mathbb{R}^n$, we denote by $F'(x,d)\\in\\mathbb{R}^m$ the vector whose components are $f_i'(x,d)$ and we proceed to prove\n\\[\n\\sigma_S(d)=g' (F(x),F'(x,d)).\n\\tag{4.3.2}\n\\]\nFor any $s=\\sum_{i=1}^m \\rho^i s_i\\in S$, we write $\\langle s,d\\rangle$ as\n\\[\n\\sum_{i=1}^m \\rho^i\\langle s_i,d\\rangle \\le\n\\sum_{i=1}^m \\rho^i f_i'(x,d) \\le g'(F(x),F'(x,d)) ;\n\\tag{4.3.3}\n\\]\nthe first inequality uses $\\rho^i\\ge 0$ and the definition of $f_i'(x,\\cdot)=\\sigma_{\\partial f_i}(x)$; the second uses the definition $g'(F(x),\\cdot)=\\sigma_{\\partial g(F(x))}$.\n\nOn the other hand, the compactness of $\\partial g(F(x))$ implies the existence of an $m$-tuple $(\\bar\\rho^i)\\in\\partial g(F(x))$ such that\n\\[\ng'(F(x),F'(x,d))=\\sum_{i=1}^m\\bar{\\rho}^i f_i'(x,d),\n\\]\nand the compactness of each $\\partial f_i(x)$ yields likewise an $\\bar{s}_i\\in\\partial f_i(x)$ such that\n\\[\nf_i'(x,d)=\\langle\\bar{s}_i,d\\rangle\\quad\\text{for }i=1,\\dots,m.\n\\]\n\nAltogether, we have exhibited an $\\bar{s}=\\sum_{i=1}^m\\bar{\\rho}^i\\bar{s}_i\\in S$ such that equality holds in (4.3.3), so (4.3.2) is established.\n\n[Step 2] It remains to prove that the support function (4.3.2) is really the directional derivative $(g\\circ F)'(x,d)$. For $t>0$, expand $F(x+td)$, use the fact that $g$ is locally Lipschitzian, and then expand $g(F(x+td))$:\n\\[\ng(F(x+td))=g(F(x)+tF'(x,d)+o(t))=g(F(x)+tF'(x,d))+o(t)\n= g(F(x))+tg'(F(x),F'(x,d))+o(t).\n\\]\n\nFrom there, it follows\n\\[\n(g\\circ F)'(x,d):=\\lim_{t\\downarrow 0}\\frac{g(F(x+td))-g(F(x))}{t}=g'(F(x),F'(x,d)).\n\\quad\\square\n\\]"
    },
    {
        "statement": "Let $f_1,\\dots,f_m$ be $m$ convex functions from $\\mathbb{R}^n$ to $\\mathbb{R}$ and define\n\\[\nf := \\max\\{f_1,\\dots,f_m\\}.\n\\]\nDenoting by $I(x) := \\{i : f_i(x) = f(x)\\}$ the active index-set, we have\n\\[\n\\partial f(x) = \\operatorname{co}\\{\\bigcup_{i\\in I(x)}\\partial f_i(x)\\}.\n\\tag{4.3.4}\n\\]",
        "title": "",
        "label": "thm:FCA-chapD-4.3.2",
        "lean_tag": [
            "FCA_chap_D_4_3_2"
        ],
        "lean_formalization": "lemma FCA_chap_D_4_3_2 {n m : ℕ}\n  (f : ℕ → (EuclideanSpace ℝ (Fin n) → ℝ))\n  (x : EuclideanSpace ℝ (Fin n))\n  (hf_convex : ∀ i ∈ Finset.range m, ConvexOn ℝ Set.univ (f i)) :\n  let F : (EuclideanSpace ℝ (Fin n)) → ℝ := fun x => sSup (⋃ i ∈ Finset.range m, {(f i) x})\n  let I := {i | (f i) x = F x}\n  SubdifferentialI (liftRealtoEReal F) x = convexHull ℝ (⋃ i ∈ I, SubdifferentialI (liftRealtoEReal (f i)) x):= by\n  sorry",
        "proof": "Take \\(g(y)=\\max\\{y^{1},\\dots,y^{m}\\}\\), whose subdifferential was computed in (3.7): \\(\\{e_{i}\\}\\) denoting the canonical basis of \\(\\mathbb{R}^{m}\\),\n\\[\n\\partial g(y)=\\operatorname{co}\\{e_{i} : i\\ \\text{such that}\\ y^{i}=g(y)\\}.\n\\]\nThen, using the notation of Theorem 4.3.1, we write \\(\\partial g(F(x))\\) as\n\\[\n\\left\\{(\\rho^{1},\\dots,\\rho^{m}):\\ \\rho^{i}=0\\ \\text{for}\\ i\\notin I(x),\\ \\rho^{i}\\ge 0\\ \\text{for}\\ i\\in I(x),\\ \\sum_{i=1}^{m}\\rho^{i}=1\\right\\},\n\\]\nand (4.3.1) gives\n\\[\n\\partial f(x)=\\left\\{\\sum_{i\\in I(x)}\\rho^{i}\\partial f_{i}(x):\\ \\rho^{i}\\ge 0\\ \\text{for}\\ i\\in I(x),\\ \\sum_{i\\in I(x)}\\rho^{i}=1\\right\\}.\n\\]\nRemembering Example A.1.3.5, it suffices to recognize in the above expression the convex hull announced in (4.3.4)."
    },
    {
        "statement": "With the notation (4.4.1), (4.4.2),\n\\[\n\\partial f(x)\\supset\\operatorname{co}\\{\\partial f_j(x):\\; j\\in J(x)\\}.\n\\tag{4.4.3}\n\\]",
        "title": "",
        "label": "lem:FCA-chapD-4.4.1",
        "lean_tag": [
            "FCA_chap_D_4_4_1"
        ],
        "lean_formalization": "lemma FCA_chap_D_4_4_1 {n : ℕ} {J : Type*}\n  (f : J → (EuclideanSpace ℝ (Fin n) → ℝ))\n  (x : EuclideanSpace ℝ (Fin n))\n  (hf_convex : ∀ (i : J), ConvexOn ℝ Set.univ (f i))\n  (hf_finite : ∀ z, ⨆ (j : J), (f j z : WithTop ℝ) < ⊤) :\n  let F : (EuclideanSpace ℝ (Fin n)) → EReal := fun z => ⨆ (j : J), (f j z)\n  let Jx := {j | f j x = F x}\n  closure (convexHull ℝ (⋃ j ∈ Jx, SubdifferentialI (liftRealtoEReal (f j)) x)) ⊆\n  SubdifferentialI F x := by\n  sorry",
        "proof": "Take $j\\in J(x)$ and $s\\in\\partial f_j(x)$; from the definition (1.2.1) of the subdifferential,\n\\[\nf(y)\\ge f_j(y)\\ge f_j(x)+\\langle s,y-x\\rangle\\qquad\\text{for all }y\\in\\mathbb R^n,\n\\]\nso $\\partial f(x)$ contains $\\partial f_j(x)$. Being closed and convex, it also contains the closed convex hull appearing in (4.4.3)."
    },
    {
        "statement": "With the notation (4.4.1), (4.4.2), assume that $J$ is a compact set (in some metric space), on which the functions $j\\mapsto f_j(x)$ are upper semi-continuous for each $x\\in\\mathbb{R}^n$. Then\n\\[\n\\partial f(x)=\\operatorname{co}\\{\\cup\\partial f_j(x):\\; j\\in J(x)\\}.\n\\tag{4.4.4}\n\\]",
        "title": "",
        "label": "thm:FCA-chapD-4.4.2",
        "lean_tag": [
            "FCA_chap_D_4_4_2"
        ],
        "lean_formalization": "lemma FCA_chap_D_4_4_2 {n : ℕ} {J : Type*} [TopologicalSpace J] [CompactSpace J]\n  (f : J → (EuclideanSpace ℝ (Fin n) → ℝ))\n  (x : EuclideanSpace ℝ (Fin n))\n  (hf_convex : ∀ (i : J), ConvexOn ℝ Set.univ (f i))\n  (hf_finite : ∀ z, ⨆ (j : J), (f j z : WithTop ℝ) < ⊤)\n  (hf_usc : ∀ (j : J), ∀ z, lscAt (fun v => -1 • f j v) z) :\n  let F : (EuclideanSpace ℝ (Fin n)) → EReal := fun z => ⨆ (j : J), (f j z)\n  let Jx := {j | f j x = F x}\n  convexHull ℝ (⋃ j ∈ Jx, SubdifferentialI (liftRealtoEReal (f j)) x) =\n  SubdifferentialI F x := by\n  sorry",
        "proof": "[Step 0] Our assumptions make $J(x)$ nonempty and compact. Denote by $S$ the curly bracketed set in (4.4.4); because of (4.4.3), $S$ is bounded, let us check that it is closed. Take a sequence $(s_k)\\subset S$ converging to $s$; to each $s_k$, we associate some $j_k\\in J(x)$ such that $s_k\\in\\partial f_{j_k}(x)$, i.e.\n\\[\nf_{j_k}(y)\\ge f_{j_k}(x)+\\langle s_k,y-x\\rangle\\qquad\\text{for all }y\\in\\mathbb{R}^n.\n\\]\nLet $k\\to\\infty$; extract a subsequence so that $j_k\\to j\\in J(x)$; we have $f_{j_k}(x)\\equiv f(x)=:f_j(x)$; and by upper semi-continuity of the function $f_{(\\cdot)}(y)$, we obtain\n\\[\nf_j(y)\\ge\\limsup f_{j_k}(y)\\ge f_j(x)+\\langle s,y-x\\rangle\\qquad\\text{for all }y\\in\\mathbb{R}^n,\n\\]\nwhich shows $s\\in\\partial f_j(x)\\subset S$. Altogether, $S$ is compact and its convex hull is also compact (Theorem A.1.4.3).\n\nIn view of Lemma 4.4.1, it suffices to prove the ``$\\subset$''-inclusion in (4.4.4); for this, we will establish the corresponding inequality between support functions which, in view of the calculus rule C.3.3.2(ii), is: for all $d\\in\\mathbb{R}^n$,\n\\[\nf'(x,d)\\le\\sigma_S(d)=\\sup\\{f_j'(x,d):\\; j\\in J(x)\\}.\n\\tag{4.4.5}\n\\]\n\n[Step 1] Let $\\varepsilon>0$; from the definition (1.1.2) of $f'(x,d)$,\n\\[\n\\frac{f(x+td)-f(x)}{t}>f'(x,d)-\\varepsilon\\qquad\\text{for all }t>0.\n\\tag{4.4.6}\n\\]\nFor $t>0$, set\n\\[\nJ_t:=\\Big\\{j\\in J:\\;\\frac{f_j(x+td)-f_j(x)}{t}\\ge f'(x,d)-\\varepsilon\\Big\\}.\n\\]\nThe definition of $f(x+td)$ shows with (4.4.6) that $J_t$ is nonempty. Because $J$ is compact and $f_{(\\cdot)}(x+td)$ is upper semi-continuous, $J_t$ is visibly compact. Observe that $J_t$ is a superlevel-set of the function\n\\[\n0<t\\mapsto\\frac{f_j(x+td)-f_j(x)}{t}+\\frac{f_j(x)-f(x)}{t},\n\\]\nwhich is nondecreasing: the first fraction is the slope of a convex function, and the second fraction has a nonpositive numerator. Thus, $J_{t_1}\\subset J_{t_2}$ for $0<t_1\\le t_2$.\n\n[Step 2] By compactness, we deduce the existence of some $j^*\\in\\bigcap_{t>0}J_t$ (for each $\\tau\\in ]0,t]$, pick some $j_\\tau\\in J_\\tau\\subset J_t$; take a cluster point for $\\tau\\downarrow 0$: it is in $J_t$). We therefore have\n\\[\nf_{j^*}(x+td)-f(x)\\ge t\\bigl[f'(x,d)-\\varepsilon\\bigr]\\qquad\\text{for all }t>0,\n\\]\nhence $j^*\\in J(x)$ (\\text{continuity of the convex function } $f_{j^*}$ for $t\\downarrow0$). In this inequality, we can replace $f(x)$ by $f_{j^*}(x)$, divide by $t$ and let $t\\downarrow0$ to obtain\n\\[\n\\sigma_S(d)\\ge f'_{j^*}(x,d)\\ge f'(x,d)-\\varepsilon.\n\\]\nSince $d\\in\\mathbb{R}^n$ and $\\varepsilon>0$ were arbitrary, (4.4.5) is established."
    },
    {
        "statement": "The notation and assumptions are those of Theorem 4.4.2. Assume also that each $f_j$ is differentiable; then\n\\[\n\\partial f(x)=\\operatorname{co}\\{\\nabla f_j(x):\\; j\\in J(x)\\}.\n\\]",
        "title": "",
        "label": "cor:FCA-chapD-4.4.4",
        "lean_tag": [
            "FCA_chap_D_4_4_4"
        ],
        "lean_formalization": "lemma FCA_chap_D_4_4_4 {n : ℕ} {J : Type*} [TopologicalSpace J] [CompactSpace J]\n  (f : J → (EuclideanSpace ℝ (Fin n) → ℝ))\n  (x : EuclideanSpace ℝ (Fin n))\n  (hf_convex : ∀ (i : J), ConvexOn ℝ Set.univ (f i))\n  (hf_finite : ∀ z, ⨆ (j : J), (f j z : WithTop ℝ) < ⊤)\n  (hf_usc : ∀ (j : J), ∀ z, lscAt (fun v => -1 • f j v) z)\n  (hf_differentiable : ∀ (j : J), Differentiable ℝ (f j)) :\n  let F : (EuclideanSpace ℝ (Fin n)) → EReal := fun z => ⨆ (j : J), (f j z)\n  let Jx := {j | f j x = F x}\n  SubdifferentialI F x = convexHull ℝ (⋃ j ∈ Jx, {gradient (f j) x}) := by\n  sorry",
        "proof": ""
    },
    {
        "statement": "For some compact set $Y\\subset\\mathbb{R}^p$, let $g:\\mathbb{R}^n\\times Y\\to\\mathbb{R}$ be a function satisfying the following properties:\n\\begin{itemize}\n\\item[(i)] for each $x\\in\\mathbb{R}^n$, $g(x,\\cdot)$ is upper semi-continuous;\n\\item[(ii)] for each $y\\in Y$, $g(\\cdot,y)$ is convex and differentiable;\n\\item[(iii)] the function $f:=\\sup_{y\\in Y} g(\\cdot,y)$ is finite-valued on $\\mathbb{R}^n$;\n\\item[(iv)] at some $x\\in\\mathbb{R}^n$, $g(x,\\cdot)$ is maximized at a unique $y(x)\\in Y$.\n\\end{itemize}\nThen $f$ is differentiable at this $x$, and its gradient is\n\\[\n\\nabla f(x)=\\nabla_x g\\bigl(x,y(x)\\bigr)\n\\tag{4.4.8}\n\\]\n(where $\\nabla_x g(x,y)$ denotes the gradient of the function $g(\\cdot,y)$ at $x$).",
        "title": "",
        "label": "cor:FCA-chapD-4.4.5",
        "lean_tag": [
            "FCA_chap_D_4_4_5"
        ],
        "lean_formalization": "lemma FCA_chap_D_4_4_5 {n p : ℕ}\n  (Y : Set (EuclideanSpace ℝ (Fin p)))\n  (g : EuclideanSpace ℝ (Fin n) → EuclideanSpace ℝ (Fin p) → ℝ)\n  (x₀ : EuclideanSpace ℝ (Fin n))\n  (hf_usc : ∀ x, ∀ y, lscAt (fun v => -1 • (g x v)) y)\n  (hf_convex : ∀ y ∈ Y, ConvexOn ℝ Set.univ (fun x => g x y))\n  (hf_differentiable : ∀ y ∈ Y, Differentiable ℝ (fun x => g x y))\n  (hf_finite : ∀ x, ⨆ y ∈ Y, (g x y : WithTop ℝ) < ⊤) :\n  let F : EuclideanSpace ℝ (Fin n) → EReal := fun x => ⨆ y ∈ Y, (g x y : EReal)\n  ∃ (F_finite : EuclideanSpace ℝ (Fin n) → ℝ),\n  ∃! (y₀ : EuclideanSpace ℝ (Fin p)),\n  (∀ x, F_finite x = F x) ∧\n  (y₀ ∈ Y) ∧\n  (IsMaxOn (fun y => g x₀ y) Y y₀) →\n  (Differentiable ℝ F_finite) ∧\n  (gradient F_finite x₀ = gradient (fun x => g x y₀) x₀) := by\n  sorry",
        "proof": ""
    },
    {
        "statement": "With the notation (4.5.1), (4.5.2), assume $A$ is surjective. Let $x$ be such that $Y(x)$ is nonempty. Then, for arbitrary $y\\in Y(x)$,\n\\[\n\\partial(Ag)(x)=\\{\\,s\\in\\mathbb{R}^n:\\;A^*s\\in\\partial g(y)\\,\\}=\\bigl(A^*\\bigr)^{-1}[\\partial g(y)]\n\\tag{4.5.3}\n\\]\n(and this set is thus independent of the particular optimal $y$).",
        "title": "",
        "label": "thm:FCA-chapD-4.5.1",
        "lean_tag": [
            "FCA_chap_D_4_5_1"
        ],
        "lean_formalization": "lemma FCA_chap_D_4_5_1 {m n : ℕ}\n  (g : EuclideanSpace ℝ (Fin m) → ℝ)\n  (A : EuclideanSpace ℝ (Fin m) →ₗ[ℝ] EuclideanSpace ℝ (Fin n))\n  (x : EuclideanSpace ℝ (Fin n))\n  (hg_convex : ConvexOn ℝ Set.univ g)\n  (hA_surjective : Function.Surjective A) :\n  let Ag := fun x => sInf (Set.image g {y | A y = x})\n  let Yx := {y | (A y = x) ∧ (g y = Ag x)}\n  (Set.Nonempty Yx) → ∀ y ∈ Yx,\n  (SubdifferentialI (liftRealtoEReal Ag) x =\n  {s | A.adjoint s ∈ SubdifferentialI (liftRealtoEReal g) y}) ∧\n  (SubdifferentialI (liftRealtoEReal Ag) x =\n  Set.preimage A.adjoint (SubdifferentialI (liftRealtoEReal g) y)) := by\n  sorry",
        "proof": "By definition, $s\\in\\partial(Ag)(x)$ if and only if $(Ag)(x')\\ge(Ag)(x)+\\langle s,x'-x\\rangle$ for all $x'\\in\\mathbb{R}^n$, which can be rewritten\n\\[\n(Ag)(x')\\ge g(y)+\\langle s,x'-Ay\\rangle\\qquad\\text{for all }x'\\in\\mathbb{R}^n\n\\]\nwhere $y$ is arbitrary in $Y(x)$. Furthermore, because $A$ is surjective and by definition of $Ag$, this last relation is equivalent to\n\\[\ng(y')\\ge g(y)+\\langle s,Ay'-Ay\\rangle = g(y)+\\langle A^*s,y'-y\\rangle\\qquad\\text{for all }y'\\in\\mathbb{R}^m\n\\]\nwhich means that $A^*s\\in\\partial g(y)$."
    },
    {
        "statement": "Make the assumptions of Theorem 4.5.1. If $g$ is differentiable at some $y\\in Y(x)$, then $Ag$ is differentiable at $x$.",
        "title": "",
        "label": "cor:FCA-chapD-4.5.2",
        "lean_tag": [
            "FCA_chap_D_4_5_2"
        ],
        "lean_formalization": "lemma FCA_chap_D_4_5_2 {m n : ℕ}\n  (g : EuclideanSpace ℝ (Fin m) → ℝ)\n  (A : EuclideanSpace ℝ (Fin m) →ₗ[ℝ] EuclideanSpace ℝ (Fin n))\n  (x : EuclideanSpace ℝ (Fin n))\n  (hg_convex : ConvexOn ℝ Set.univ g)\n  (hA_surjective : Function.Surjective A) :\n  let Ag := fun x => sInf (Set.image g {y | A y = x})\n  let Yx := {y | (A y = x) ∧ (g y = Ag x)}\n  (Set.Nonempty Yx) → (∃ y ∈ Yx, DifferentiableAt ℝ g y) →\n  (DifferentiableAt ℝ Ag x) := by\n  sorry",
        "proof": "Surjectivity of $A$ is equivalent to injectivity of $A^*$: in (4.5.3), we have an equation in $s$: $A^*s=\\nabla g(y)$, whose solution is unique, and is therefore $\\nabla(Ag)(x)$."
    },
    {
        "statement": "Suppose that the subdifferential of $g$ in (4.5.4) is associated with a scalar product $\\langle\\!\\langle\\cdot,\\cdot\\rangle\\!\\rangle$ preserving the structure of a product space:\n\\[\n\\langle\\!\\langle (x,y),(x',y')\\rangle\\!\\rangle=\\langle x,x'\\rangle_n+\\langle y,y'\\rangle_m\n\\qquad\\text{for }x,x'\\in\\mathbb{R}^n\\text{ and }y,y'\\in\\mathbb{R}^m.\n\\]\nAt a given $x\\in\\mathbb{R}^n$, take an arbitrary $y$ solving (4.5.4). Then\n\\[\n\\partial f(x)=\\{\\,s\\in\\mathbb{R}^n:\\,(s,0)\\in\\partial_{(x,y)}g(x,y)\\,\\}.\n\\]",
        "title": "",
        "label": "cor:FCA-chapD-4.5.3",
        "lean_tag": "no-lean-tag",
        "proof": "With our notation, $A^*s=(s,0)$ for all $s\\in\\mathbb{R}^n$. It suffices to apply Theorem 4.5.1 (the symbol $\\partial_{(x,y)}g$ is used as a reminder that we are dealing with the subdifferential of $g$ with respect to the variable $(\\cdot,\\cdot)\\in\\mathbb{R}^n\\times\\mathbb{R}^m$)."
    },
    {
        "statement": "Let $f_1$ and $f_2:\\mathbb{R}^n\\to\\mathbb{R}$ be two convex functions minorized by a common affine function. For given $x$, let $(y_1,y_2)$ be such that the inf-convolution is exact at $x=y_1+y_2$, i.e.: $(f_1\\infconv f_2)(x)=f_1(y_1)+f_2(y_2)$. Then\n\\[\n\\partial(f_1\\infconv f_2)(x)=\\partial f_1(y_1)\\cap\\partial f_2(y_2).\n\\tag{4.5.6}\n\\]",
        "title": "",
        "label": "cor:FCA-chapD-4.5.5",
        "lean_tag": [
            "FCA_chap_D_4_5_5"
        ],
        "lean_formalization": "lemma FCA_chap_D_4_5_5 {n : ℕ}\n  (f₁ f₂ : EuclideanSpace ℝ (Fin n) → ℝ)\n  (y₁ y₂ : EuclideanSpace ℝ (Fin n))\n  (hf_convex : ConvexOn ℝ Set.univ f₁ ∧ ConvexOn ℝ Set.univ f₂)\n  (hf_minorized : ∃ (s : EuclideanSpace ℝ (Fin n)) (b : ℝ),\n                 ∀ x, (inner ℝ s x + b ≤ f₁ x) ∧ (inner ℝ s x + b ≤ f₂ x)) :\n  let infconv := infimalConv (liftRealtoWT f₁) (liftRealtoWT f₂)\n  (infconv (y₁ + y₂) = f₁ y₁ + f₂ y₂) →\n  SubdifferentialI infconv (y₁ + y₂) =\n  (SubdifferentialI (liftRealtoEReal f₁) y₁) ∩ (SubdifferentialI (liftRealtoEReal f₂) y₂) := by\n  sorry",
        "proof": "First observe that $A^*s=(s,s)$. Also, apply Definition 1.2.1 to see that $(s_1,s_2)\\in\\partial g(y_1,y_2)$ if and only if $s_1\\in\\partial f_1(y_1)$ and $s_2\\in\\partial f_2(y_2)$. Then (4.5.6) is just the copy of (4.5.3) in the present context."
    },
    {
        "statement": "With the notations (5.3.1), (5.3.2), suppose $\\varphi_{0}\\notin H$. A necessary and sufficient condition for $\\bar{x}=(\\bar{\\xi}^{1},\\dots,\\bar{\\xi}^{n})\\in\\mathbb{R}^{n}$ to minimize $f$ of (5.3.1) is that, for some positive integer $p\\le n+1$, there exist $p$ points $t_{1},\\dots,t_{p}$ in $T$, $p$ integers $\\varepsilon_{1},\\dots,\\varepsilon_{p}$ in $\\{-1,+1\\}$ and $p$ positive numbers $\\alpha_{1},\\dots,\\alpha_{p}$ such that\n\\[\n\\sum_{i=1}^{n}\\xi_i^r\\varphi_i(t_k)-\\varphi_0(t_k)=\\varepsilon_k f(\\bar{x})\\qquad\\text{for }k=1,\\dots,p,\n\\]\n\\[\n\\sum_{k=1}^{p}\\alpha_k\\varepsilon_k\\varphi_i(t_k)=0\\qquad\\text{for }i=1,\\dots,n\n\\]\n(or equivalently: \\(\\displaystyle \\sum_{k=1}^{p}\\alpha_k\\varepsilon_k\\psi(t_k)=0\\quad\\text{for all }\\psi\\in H\\)). \\(\\square\\)",
        "title": "",
        "label": "thm:FCA-chapD-5.3.1",
        "lean_tag": "no-lean-tag",
        "proof": ""
    },
    {
        "statement": "The subdifferential mapping is monotone in the sense that, for all $x_1$ and $x_2$ in $\\mathbb{R}^n$,\n\\begin{equation}\\tag{6.1.1}\n\\langle s_2 - s_1, x_2 - x_1\\rangle \\ge 0\\quad\\text{for all } s_i\\in\\partial f(x_i),\\; i=1,2.\n\\end{equation}",
        "title": "",
        "label": "prop:FCA-chapD-6.1.1",
        "lean_tag": [
            "FCA_chap_D_6_1_1"
        ],
        "lean_formalization": "lemma FCA_chap_D_6_1_1 {n : ℕ}\n  (f : EuclideanSpace ℝ (Fin n) → ℝ)\n  (x₁ x₂ : EuclideanSpace ℝ (Fin n))\n  (hf_convex : ConvexOn ℝ Set.univ f) :\n  ∀ s₁ ∈ SubdifferentialI (liftRealtoEReal f) x₁, ∀ s₂ ∈ SubdifferentialI (liftRealtoEReal f) x₂,\n  inner ℝ (s₂ - s₁) (x₂ - x₂) ≥ 0 := by\n  sorry",
        "proof": "The subgradient inequalities\n\\[\nf(x_2)\\ge f(x_1)+\\langle s_1, x_2-x_1\\rangle\\quad\\text{for all }s_1\\in\\partial f(x_1)\n\\]\n\\[\nf(x_1)\\ge f(x_2)+\\langle s_2, x_1-x_2\\rangle\\quad\\text{for all }s_2\\in\\partial f(x_2)\n\\]\ngive the result simply by addition."
    },
    {
        "statement": "A necessary and sufficient for a convex function $f:\\mathbb{R}^n\\to\\mathbb{R}$ to be strongly convex with modulus $c>0$ on a convex set $C$ is: for all $x_1,x_2$ in $C$,\n\\begin{equation}\\tag{6.1.3}\nf(x_2)\\ge f(x_1)+\\langle s,x_2-x_1\\rangle+\\frac{c}{2}\\|x_2-x_1\\|^2\\quad\\text{for all }s\\in\\partial f(x_1),\n\\end{equation}\nor equivalently\n\\begin{equation}\\tag{6.1.4}\n\\langle s_2-s_1,x_2-x_1\\rangle\\ge c\\|x_2-x_1\\|^2\\quad\\text{for all }s_i\\in\\partial f(x_i),\\; i=1,2.\n\\end{equation}",
        "title": "",
        "label": "thm:FCA-chapD-6.1.2",
        "lean_tag": [
            "FCA_chap_D_6_1_2"
        ],
        "lean_formalization": "lemma FCA_chap_D_6_1_2 {n : ℕ}\n  (f : EuclideanSpace ℝ (Fin n) → ℝ)\n  (C : Set (EuclideanSpace ℝ (Fin n)))\n  (c : ℝ)\n  (hC_convex : Convex ℝ C) (hc : c > 0) :\n  StrongConvexOn C c f ↔\n  ∀ x₁ ∈ C, ∀ x₂ ∈ C, ∀ s ∈ SubdifferentialI (liftRealtoEReal f) x₁,\n  f x₂ ≥ f x₁ + inner ℝ s (x₂ - x₁) + (c / 2) * ‖x₂ - x₁‖^2 := by\n  sorry",
        "proof": "For $x_1,x_2$ given in $C$ and $\\alpha\\in]0,1[$, we will use the notation\n\\[\nx^\\alpha:=\\alpha x_2+(1-\\alpha)x_1=x_1+\\alpha(x_2-x_1)\n\\]\nand we will prove (6.1.3) $\\Rightarrow$ (6.1.2) $\\Rightarrow$ (6.1.4) $\\Rightarrow$ (6.1.3).\n\n[(6.1.3) $\\Rightarrow$ (6.1.2)] Write (6.1.3) with $x_1$ replaced by $x^\\alpha\\in C$: for $s\\in\\partial f(x^\\alpha)$,\n\\[\nf(x_2)\\ge f(x^\\alpha)+\\langle s,x_2-x^\\alpha\\rangle+\\frac{c}{2}\\|x_2-x^\\alpha\\|^2,\n\\]\nor equivalently\n\\[\nf(x_2)\\ge f(x^\\alpha)+(1-\\alpha)\\langle s,x_2-x_1\\rangle+\\frac{c}{2}(1-\\alpha)^2\\|x_2-x_1\\|^2.\n\\]\n\nLikewise,\n\\[\nf(x_1)\\ge f(x^\\alpha)+\\alpha\\langle s,x_1-x_2\\rangle+\\frac{c}{2}\\alpha^2\\|x_1-x_2\\|^2.\n\\]\n\nMultiply these last two inequalities by $\\alpha$ and $(1-\\alpha)$ respectively, and add to obtain\n\\[\n\\alpha f(x_2)+(1-\\alpha)f(x_1)\\ge f(x^\\alpha)+\\frac{c}{2}\\|x_2-x_1\\|^2\\big[\\alpha(1-\\alpha)^2+(1-\\alpha)\\alpha^2\\big].\n\\]\nThen realize after simplification that this is just (6.1.2).\n\n[(6.1.2) $\\Rightarrow$ (6.1.4)] Write (6.1.2) as\n\\[\n\\frac{f(x^\\alpha)-f(x_1)}{\\alpha}+\\frac{c}{2}(1-\\alpha)\\|x_2-x_1\\|^2\\le f(x_2)-f(x_1)\n\\]\nand let $\\alpha\\downarrow0$ to obtain $f'(x_1,x_2-x_1)+\\tfrac{c}{2}\\|x_2-x_1\\|^2\\le f(x_2)-f(x_1)$, which implies (6.1.3). Then, copying (6.1.3) with $x_1$ and $x_2$ interchanged and adding yields (6.1.4) directly.\n\n[(6.1.4) $\\Rightarrow$ (6.1.3)] Apply Theorem 2.3.4 to the one-dimensional convex function $\\mathbb{R}\\ni\\alpha\\mapsto\\varphi(\\alpha):=f(x^\\alpha)$:\n\\begin{equation}\\tag{6.1.5}\nf(x_2)-f(x_1)=\\varphi(1)-\\varphi(0)=\\int_0^1\\langle s^\\alpha,x_2-x_1\\rangle\\,d\\alpha\n\\end{equation}\nwhere $s^\\alpha\\in\\partial f(x^\\alpha)$ for $\\alpha\\in[0,1]$. Then take $s_1$ arbitrary in $\\partial f(x_1)$ and apply (6.1.4): $\\langle s^\\alpha-s_1,x^\\alpha-x_1\\rangle\\ge c\\|x^\\alpha-x_1\\|^2$ i.e., using the value of $x^\\alpha$,\n\\[\n\\alpha\\langle s^\\alpha,x_2-x_1\\rangle\\ge\\alpha\\langle s_1,x_2-x_1\\rangle+c\\alpha^2\\|x_2-x_1\\|^2.\n\\]\n\nThe result follows by using this inequality to minorize the integral in (6.1.5)."
    },
    {
        "statement": "A necessary and sufficient condition for a convex function $f:\\mathbb{R}^n\\to\\mathbb{R}$ to be strictly convex on a convex set $C$ is: for all $x_1,x_2\\in C$ with $x_2\\neq x_1$,\n\\[\nf(x_2)>f(x_1)+\\langle s,x_2-x_1\\rangle\\quad\\text{for all } s\\in\\partial f(x_1)\n\\]\nor equivalently\n\\[\n\\langle s_2-s_1,x_2-x_1\\rangle>0\\quad\\text{for all } s_i\\in\\partial f(x_i),\\ i=1,2.\n\\]",
        "title": "",
        "label": "prop:FCA-chapD-6.1.3",
        "lean_tag": [
            "FCA_chap_D_6_1_3"
        ],
        "lean_formalization": "lemma FCA_chap_D_6_1_3 {n : ℕ}\n  (f : EuclideanSpace ℝ (Fin n) → ℝ)\n  (C : Set (EuclideanSpace ℝ (Fin n)))\n  (hC_convex : Convex ℝ C) :\n  StrictConvexOn ℝ C f ↔\n  ∀ x₁ ∈ C, ∀ x₂ ∈ C, (x₁ ≠ x₂) →\n  ∀ s ∈ SubdifferentialI (liftRealtoEReal f) x₁,\n  f x₂ > f x₁ + inner ℝ s (x₂ - x₁) := by\n  sorry",
        "proof": "Copy the proof of Theorem 6.1.2 with $c=0$ and the relevant ``$ \\ge $''-signs replaced by strict inequalities. The only delicate point is in the [(6.1.2) $\\Rightarrow$ (6.1.4)]-stage: use monotonicity of the difference quotient."
    },
    {
        "statement": "Let $f:\\mathbb{R}^n\\to\\mathbb{R}$ be convex. The graph of its subdifferential mapping is closed in $\\mathbb{R}^n\\times\\mathbb{R}^n$.",
        "title": "",
        "label": "prop:FCA-chapD-6.2.1",
        "lean_tag": [
            "FCA_chap_D_6_2_1"
        ],
        "lean_formalization": "lemma FCA_chap_D_6_2_1 {n : ℕ}\n  (f : EuclideanSpace ℝ (Fin n) → ℝ)\n  (hf_convex : ConvexOn ℝ Set.univ f) :\n  let graph_subdiff := ⋃ x, {z : EuclideanSpace ℝ (Fin n) × EuclideanSpace ℝ (Fin n) |\n                                  z.1 = x ∧ z.2 ∈ SubdifferentialI (liftRealtoEReal f) x}\n  IsClosed graph_subdiff := by\n  sorry",
        "proof": "Let $(x_k,s_k)$ be a sequence in $\\operatorname{gr}\\partial f$ converging to $(x,s)\\in\\mathbb{R}^n\\times\\mathbb{R}^n$. We must prove that $(x,s)\\in\\operatorname{gr}\\partial f$, which is easy. We have for all $k$\n\\[\nf(y)\\ge f(x_k)+\\langle s_k,y-x_k\\rangle\\quad\\text{for all }y\\in\\mathbb{R}^n;\n\\]\npass to the limit on $k$, using continuity of $f$ and of the scalar product."
    },
    {
        "statement": "The mapping $\\partial f$ is locally bounded, i.e.\\ the image $\\partial f(B)$ of a bounded set $B\\subset\\mathbb{R}^n$ is a bounded set in $\\mathbb{R}^n$.",
        "title": "",
        "label": "prop:FCA-chapD-6.2.2",
        "lean_tag": [
            "FCA_chap_D_6_2_2"
        ],
        "lean_formalization": "lemma FCA_chap_D_6_2_2 {n : ℕ}\n  (f : EuclideanSpace ℝ (Fin n) → ℝ)\n  (hf_convex : ConvexOn ℝ Set.univ f) :\n  let subdiff_im := fun (C : Set (EuclideanSpace ℝ (Fin n))) =>\n                     {v | ∃ x, v ∈ SubdifferentialI (liftRealtoEReal f) x}\n  ∀ (B : Set (EuclideanSpace ℝ (Fin n))),\n  Bornology.IsBounded B → Bornology.IsBounded (subdiff_im B) := by\n  sorry",
        "proof": "For arbitrary $x$ in $B$ and $s\\neq 0$ in $\\partial f(x)$, the subgradient inequality implies in particular $f(x+s/\\|s\\|)\\ge f(x)+\\|s\\|$. On the other hand, $f$ is Lipschitz-continuous on the bounded set $B+B(0,1)$ (Theorem B.3.1.2). Hence $\\|s\\|\\le L$ for some $L$."
    },
    {
        "statement": "The subdifferential mapping of a convex function $f:\\mathbb{R}^n\\to\\mathbb{R}$ is outer semi-continuous at any $x\\in\\mathbb{R}^n$, i.e.\n\\[\n\\forall\\varepsilon>0,\\ \\exists\\delta>0:\\quad y\\in B(x,\\delta)\\implies \\partial f(y)\\subset \\partial f(x)+B(0,\\varepsilon).\n\\tag{6.2.1}\n\\]",
        "title": "",
        "label": "thm:FCA-chapD-6.2.4",
        "lean_tag": [
            "FCA_chap_D_6_2_4"
        ],
        "lean_formalization": "lemma FCA_chap_D_6_2_4 {n : ℕ}\n  (f : EuclideanSpace ℝ (Fin n) → ℝ)\n  (x : EuclideanSpace ℝ (Fin n))\n  (hf_convex : ConvexOn ℝ Set.univ f) :\n  let subdiff_neighborhood := fun (ε : ℝ) =>\n                              {v | ∃ w d,(‖d‖ ≤ ε) ∧ (w ∈ SubdifferentialI (liftRealtoEReal f) x) ∧ (v = w + d)}\n  ∀ ε > 0, ∃ δ > 0, ∀ y,\n  y ∈ Metric.ball x δ → SubdifferentialI (liftRealtoEReal f) y ⊆ subdiff_neighborhood ε := by\n  sorry",
        "proof": "Assume for contradiction that, at some $x$, there are $\\varepsilon>0$ and a sequence $(x_k,s_k)_k$ with\n\\[\nx_k\\to x\\quad\\text{for }k\\to\\infty\n\\qquad\\text{and}\\qquad\ns_k\\in\\partial f(x_k),\\ s_k\\not\\in\\partial f(x)+B(0,\\varepsilon)\\quad\\text{for }k=1,2,\\dots\n\\tag{6.2.2}\n\\]\nA subsequence of the bounded $(s_k)$ (Proposition 6.2.2) converges to $s\\in\\partial f(x)$ (Proposition 6.2.1). This contradicts (6.2.2), which implies $s\\not\\in\\partial f(x)+B(0,1/2\\varepsilon)$."
    },
    {
        "statement": "For $f:\\mathbb{R}^n\\to\\mathbb{R}$ convex, the function $f'(\\cdot,d)$ is upper semicontinuous: at all $x\\in\\mathbb{R}^n$,\n\\[\nf'(x,d)=\\limsup_{y\\to x} f'(y,d)\\qquad\\text{for all }d\\in\\mathbb{R}^n.\n\\]",
        "title": "",
        "label": "cor:FCA-chapD-6.2.5",
        "lean_tag": [
            "FCA_chap_D_6_2_5"
        ],
        "lean_formalization": "lemma FCA_chap_D_6_2_5 {n : ℕ}\n  (f : EuclideanSpace ℝ (Fin n) → ℝ)\n  (hf_convex : ConvexOn ℝ Set.univ f) :\n  ∀ (x : EuclideanSpace ℝ (Fin n)),\n  ∀ (d : EuclideanSpace ℝ (Fin n)),\n  directionalDeriv (liftRealtoEReal f) x d =\n  Filter.limsup (fun y => directionalDeriv (liftRealtoEReal f) y d) (𝓝 x):= by\n  sorry",
        "proof": "Use Theorem 6.2.4, in conjunction with Proposition C.3.3.7."
    },
    {
        "statement": "Let $(f_k)$ be a sequence of (finite) convex functions converging pointwise to $f:\\mathbb{R}^n\\to\\mathbb{R}$ and let $(x_k)$ converge to $x\\in\\mathbb{R}^n$. For any $\\varepsilon>0$,\n\\[\n\\partial f_k(x_k)\\subset\\partial f(x)+B(0,\\varepsilon)\\qquad\\text{for $k$ large enough.}\n\\]",
        "title": "",
        "label": "thm:FCA-chapD-6.2.7",
        "lean_tag": [
            "FCA_chap_D_6_2_7"
        ],
        "lean_formalization": "lemma FCA_chap_D_6_2_7 {n : ℕ}\n  (fk : ℕ → (EuclideanSpace ℝ (Fin n) → ℝ))\n  (xk : ℕ → EuclideanSpace ℝ (Fin n))\n  (f : EuclideanSpace ℝ (Fin n) → ℝ)\n  (x : EuclideanSpace ℝ (Fin n))\n  (hf_convex : ∀ i, ConvexOn ℝ Set.univ (fk i))\n  (hf_pointwise : ∀ v, Filter.Tendsto (fun k => fk k v) Filter.atTop (𝓝 (f v)))\n  (hx_limit : Filter.Tendsto xk Filter.atTop (𝓝 x)) :\n  let subdiff_neighborhood := fun (ε : ℝ) =>\n                              {v | ∃ w d,(‖d‖ ≤ ε) ∧ (w ∈ SubdifferentialI (liftRealtoEReal f) x) ∧ (v = w + d)}\n  ∀ ε > 0, ∃ (K : ℕ), ∀ k ≥ K,\n  SubdifferentialI (liftRealtoEReal (fk k)) (xk k) ⊆ subdiff_neighborhood ε:= by\n  sorry",
        "proof": "Let $\\varepsilon>0$ be given. Recall (Theorem B.3.1.4) that the pointwise convergence of $(f_k)$ to $f$ implies its uniform convergence on every compact set of $\\mathbb{R}^n$.\n\nFirst, we establish boundedness: for $s_k\\neq 0$ arbitrary in $\\partial f_k(x_k)$, we have\n\\[\nf_k(x_k+s_k/\\|s_k\\|)\\ge f_k(x_k)+\\|s_k\\|.\n\\]\n\nThe uniform convergence of $(f_k)$ to $f$ on $B(x,2)$ implies for $k$ large enough\n\\[\n\\|s_k\\|\\le f(x_k+s_k/\\|s_k\\|)-f(x_k)+\\varepsilon,\n\\]\nand the Lipschitz property of $f$ on $B(x,2)$ ensures that $(s_k)$ is bounded.\n\nNow suppose for contradiction that, for some infinite subsequence, there is some $s_k\\in\\partial f_k(x_k)$ which is not in $\\partial f(x)+B(0,\\varepsilon)$. Any cluster point of this $(s_k)$ --- and there is at least one --- is out of $\\partial f(x)+B(0,1/2\\varepsilon)$. Yet, with $y$ arbitrary in $\\mathbb{R}^n$, write\n\\[\nf_k(y)\\ge f_k(x_k)+\\langle s_k,y-x_k\\rangle\n\\]\nand pass to the limit (on a further subsequence such that $s_k\\to s$): pointwise [resp.\\ uniform] convergence of $(f_k)$ to $f$ at $y$ [resp.\\ around $x$], and continuity of the scalar product give $f(y)\\ge f(x)+\\langle s,y-x\\rangle$. Because $y$ was arbitrary, we obtain the contradiction $s\\in\\partial f(x)$."
    },
    {
        "statement": "Let $(f_k)$ be a sequence of (finite) differentiable convex functions converging pointwise to the differentiable $f:\\mathbb{R}^n\\to\\mathbb{R}$. Then $\\nabla f_k$ converges to $\\nabla f$ uniformly on every compact set of $\\mathbb{R}^n$.",
        "title": "",
        "label": "cor:FCA-chapD-6.2.8",
        "lean_tag": [
            "FCA_chap_D_6_2_8"
        ],
        "lean_formalization": "lemma FCA_chap_D_6_2_8 {n : ℕ}\n  (fk : ℕ → (EuclideanSpace ℝ (Fin n) → ℝ))\n  (f : EuclideanSpace ℝ (Fin n) → ℝ)\n  (hf_convex : ∀ i, ConvexOn ℝ Set.univ (fk i))\n  (hf_seq_diff : ∀ i, Differentiable ℝ (fk i))\n  (hf_diff : Differentiable ℝ f)\n  (hf_pointwise : ∀ v, Filter.Tendsto (fun k => fk k v) Filter.atTop (𝓝 (f v))) :\n  ∀ (K : Set (EuclideanSpace ℝ (Fin n))), IsCompact K →\n  TendstoUniformlyOn (fun k => (fun x => gradient (fk k) x)) (fun x => gradient f x) Filter.atTop K := by\n  sorry",
        "proof": "Take $S$ compact; suppose for contradiction that there exists $\\varepsilon>0$ and a sequence $(x_k)\\subset S$ such that\n\\[\n\\|\\nabla f_k(x_k)-\\nabla f(x_k)\\|>\\varepsilon\\quad\\text{for }k=1,2,\\dots\n\\]\nExtracting a subsequence if necessary, we may suppose $x_k\\to x\\in S$; Theorem 6.2.7 assures that the sequences $(\\nabla f_k(x_k))$ and $(\\nabla f(x_k))$ both converge to $\\nabla f(x)$, implying $0\\ge\\varepsilon$."
    },
    {
        "statement": "Let $f:\\mathbb{R}^n\\to\\mathbb{R}$ be convex. With the notation (6.3.1), $\\partial f(x)=\\operatorname{co}\\gamma f(x)$ for all $x\\in\\mathbb{R}^n$.",
        "title": "",
        "label": "thm:FCA-chapD-6.3.1",
        "lean_tag": [
            "FCA_chap_D_6_3_1"
        ],
        "lean_formalization": "lemma FCA_chap_D_6_3_1 {n : ℕ}\n  (f : EuclideanSpace ℝ (Fin n) → ℝ)\n  (hf_convex : ConvexOn ℝ Set.univ f) :\n  let delta_set := {y | SubdifferentialI (liftRealtoEReal f) y = {gradient f y}}\n  let lim_set := fun x => {s | ∃ (yk : ℕ → EuclideanSpace ℝ (Fin n)),\n                               (∀ k, yk k ∈ delta_set) ∧\n                               (Filter.Tendsto yk Filter.atTop (𝓝 x)) ∧\n                               (Filter.Tendsto (fun k => gradient f (yk k)) Filter.atTop (𝓝 s))}\n  ∀ (x : EuclideanSpace ℝ (Fin n)),\n  SubdifferentialI (liftRealtoEReal f) x = convexHull ℝ (lim_set x) := by\n  sorry\n",
        "proof": ""
    },
    {
        "statement": "Let $x$ and $d\\neq 0$ be given in $\\mathbb{R}^n$. For any sequence $(t_k,s_k,d_k)\\subset\\mathbb{R}_+^*\\times\\mathbb{R}^n\\times\\mathbb{R}^n$ satisfying\n\\[\nt_k\\downarrow0,\\quad s_k\\in\\partial f(x+t_k d_k),\\quad d_k\\to d,\\qquad\\text{for }k=1,2,\\dots\n\\]\nand any cluster point $s$ of $(s_k)$, there holds\n\\[\ns\\in\\partial f(x)\\qquad\\text{and}\\qquad\\langle s,d\\rangle=f'(x,d).\n\\]",
        "title": "",
        "label": "prop:FCA-chapD-6.3.4",
        "lean_tag": "no-lean-tag",
        "proof": "The first property comes from the results in \\S6.2. For the second, use the monotonicity of $\\partial f$:\n\\[\n0\\le\\langle s_k-s',x+t_k d_k-x\\rangle = t_k\\langle s_k-s',d_k\\rangle\\qquad\\text{for all }s'\\in\\partial f(x).\n\\]\nDivide by $t_k>0$ and pass to the limit to get $f'(x,d)\\le\\langle s,d\\rangle$. The converse inequality being trivial, the proof is complete."
    },
    {
        "statement": "For $f$ satisfying (1.1.1), the conjugate $f^*$ is a closed convex function: $f^*\\in\\Conv\\mathbb{R}^n$.",
        "title": "",
        "label": "thm:FCA-chapE-1.1.2",
        "lean_tag": [
            "FCA_chap_E_1_1_2"
        ],
        "lean_formalization": "lemma FCA_chap_E_1_1_2 {n : ℕ}\n  (f : EuclideanSpace ℝ (Fin n) → WithTop ℝ)\n  (hf_nondegenerate : IsNondegenerate f) :\n  InClosedConvRn (Conjugate f) := by\n  sorry",
        "proof": "See Example B.2.1.3."
    },
    {
        "statement": "There holds for all $x\\in\\mathbb{R}^n$\n\\[\nf^*(s)=\\sigma_{\\mathrm{epi}\\,f}(s,-1)=\\sup\\{\\langle s,x\\rangle - r : (x,r)\\in\\mathrm{epi}\\,f\\}.\n\\tag{1.2.1}\n\\]\nIt follows that the support function of $\\mathrm{epi}\\,f$ has the expression\n\\[\n\\sigma_{\\mathrm{epi}\\,f}(s,-u)=\n\\begin{cases}\nu\\,f^*\\!\\bigl(\\tfrac{1}{u}s\\bigr) & \\text{if } u>0,\\\\[4pt]\n\\sigma_{\\mathrm{epi}\\,f}(s,0)=\\sigma_{\\mathrm{dom}\\,f}(s) & \\text{if } u=0,\\\\[4pt]\n+\\infty & \\text{if } u<0.\n\\end{cases}\n\\tag{1.2.2}\n\\]",
        "title": "",
        "label": "prop:FCA-chapE-1.2.1",
        "lean_tag": [
            "FCA_chap_E_1_2_1"
        ],
        "lean_formalization": "lemma FCA_chap_E_1_2_1 {n : ℕ}\n  (f : EuclideanSpace ℝ (Fin n) → WithTop ℝ)\n  (hf_nondegenerate : IsNondegenerate f) :\n  let f_conj := fun s => Conjugate f s\n  let f_epi_supportfun := fun (s : EuclideanSpace ℝ (Fin n)) (u : ℝ) =>\n  SupportFun {z | (vecHead z, vecLast z) ∈ (epigraph (liftWithToptoEReal f))} (Fin.snoc s (-u))\n  let f_dom_supportfun := fun s => SupportFun {z | z ∈ effDom (liftWithToptoEReal f)} s\n  (Conjugate f =\n  fun s => sSup {z : WithTop ℝ | ∃ (x : EuclideanSpace ℝ (Fin n)) (r : ℝ),\n  (z = (inner ℝ s x) - r) ∧ ((x, r) ∈ epigraph (liftWithToptoEReal f))}) ∧\n  f_epi_supportfun = (fun s u =>\n    if u > 0 then u • f_conj (u⁻¹ • s)\n    else if u = 0 then f_dom_supportfun s\n    else ⊤\n  ) := by\n  sorry",
        "proof": "In (1.2.1), the right-most term can be written\n\\[\n\\sup_x\\;\\sup_{r\\ge f(x)}\\bigl[\\langle s,x\\rangle - r\\bigr]\n= \\sup_x\\bigl[\\langle s,x\\rangle - f(x)\\bigr]\n\\]\nand the first equality is established. As for (1.2.2), the case $u<0$ is trivial; when $u>0$, use the positive homogeneity of support functions to get\n\\[\n\\sigma_{\\operatorname{epi} f}(s,-u)=u\\sigma_{\\operatorname{epi} f}\\bigl(\\tfrac{1}{u}s,-1\\bigr)=u f^*\\bigl(\\tfrac{1}{u}s\\bigr).\n\\]\nFinally, for $u=0$, we have by definition\n\\[\n\\sigma_{\\operatorname{epi} f}(s,0)=\\sup\\{ \\langle s,x\\rangle : (x,r)\\in\\operatorname{epi} f\\text{ for some }r\\in\\mathbb{R}\\},\n\\]\nand we recognize $\\sigma_{\\operatorname{dom} f}(s)$."
    },
    {
        "statement": "For $f\\in\\operatorname{Conv}\\mathbb{R}^n$,\n\\[\n\\sigma_{\\operatorname{dom} f}(s)=\\sigma_{\\operatorname{epi} f}(s,0)=(f^*)^\\infty(s)\\quad\\text{for all }s\\in\\mathbb{R}^n.\n\\tag{1.2.3}\n\\]",
        "title": "",
        "label": "thm:FCA-chapE-1.2.2",
        "lean_tag": [
            "FCA_chap_E_1_2_2"
        ],
        "lean_formalization": "lemma FCA_chap_E_1_2_2 {n : ℕ}\n  (f : EuclideanSpace ℝ (Fin n) → WithTop ℝ) (x₀ : EuclideanSpace ℝ (Fin n))\n  (hf_nondegenerate : IsNondegenerate f) (hf_closed_convex : InClosedConvRn f)\n  (hx₀ : f x₀ ≠ ⊤) :\n  let f_conj := fun s => Conjugate f s\n  let f_epi_supportfun := fun (s : EuclideanSpace ℝ (Fin n)) (u : ℝ) =>\n                               SupportFun {z | (vecHead z, vecLast z) ∈ (epigraph (liftWithToptoEReal f))} (Fin.snoc s (-u))\n  let f_dom_supportfun := fun s => SupportFun {z | z ∈ effDom (liftWithToptoEReal f)} s\n  let f_conj_asympfun := fun s => AsymptoticFun f_conj x₀ s\n  ∀ s, (f_epi_supportfun s 0 = f_dom_supportfun s) ∧ (f_dom_supportfun s = f_conj_asympfun s) := by\n  sorry",
        "proof": "Use direct calculations; or see Proposition B.2.2.2 and the calculations in Example B.3.2.3."
    },
    {
        "statement": "The functions $f$, $f_j$ appearing below are assumed to satisfy (1.1.1).\n\\begin{enumerate}\n    \\item[(i)] The conjugate of the function $g(x):=f(x)+r$ is $g^*(s)=f^*(s)-r$.\n    \\item[(ii)] With $t>0$, the conjugate of the function $g(x):=tf(x)$ is $g^*(s)=t f^*(s/t)$.\n    \\item[(iii)] With $t\\neq0$, the conjugate of the function $g(x):=f(tx)$ is $g^*(s)=f^*(s/t)$.\n    \\item[(iv)] More generally: if $A$ is an invertible linear operator, $(f\\circ A)^*=f^*\\circ (A^{-1})^*$.\n    \\item[(v)] The conjugate of the function $g(x):=f(x-x_{0})$ is $g^{*}(s)=f^{*}(s)+\\langle s,x_{0}\\rangle$.\n    \\item[(vi)] The conjugate of the function $g(x):=f(x)+\\langle s_{0},x\\rangle$ is $g^{*}(s)=f^{*}(s-s_{0})$.\n    \\item[(vii)] If $f_{1}\\le f_{2}$, then $f_{1}^{*}\\ge f_{2}^{*}$.\n    \\item[(viii)] ``Convexity'' of the conjugation: if $\\mathrm{dom}\\,f_{1}\\cap\\mathrm{dom}\\,f_{2}\\neq\\varnothing$ and $\\alpha\\in]0,1[$,\n    \\[\n    [\\alpha f_{1}+(1-\\alpha)f_{2}]^{*}\\le \\alpha f_{1}^{*}+(1-\\alpha)f_{2}^{*};\n    \\]\n    \\item[(ix)] The Legendre-Fenchel transform preserves decomposition: with\n    \\[\n    \\mathbb{R}^{n}:=\\mathbb{R}^{n_{1}}\\times\\cdots\\times\\mathbb{R}^{n_{m}}\\ni x\\mapsto f(x):=\\sum_{j=1}^{m}f_{j}(x_{j})\n    \\]\n    and assuming that $\\mathbb{R}^{n}$ has the scalar product of a product-space, there holds\n    \\[\n    f^{*}(s_{1},\\dots,s_{m})=\\sum_{j=1}^{m}f_{j}^{*}(s_{j}) .\n    \\]\n\\end{enumerate}",
        "title": "",
        "label": "prop:FCA-chapE-1.3.1",
        "lean_tag": [
            "FCA_chap_E_1_3_1_i",
            "FCA_chap_E_1_3_1_ii",
            "FCA_chap_E_1_3_1_iii",
            "FCA_chap_E_1_3_1_iv",
            "FCA_chap_E_1_3_1_v",
            "FCA_chap_E_1_3_1_vi",
            "FCA_chap_E_1_3_1_vii",
            "FCA_chap_E_1_3_1_viii"
        ],
        "lean_formalization": "lemma FCA_chap_E_1_3_1_i {n : ℕ}\n  (f : EuclideanSpace ℝ (Fin n) → WithTop ℝ) (r : ℝ) (hf_nondegenerate : IsNondegenerate f) :\n  let g := fun x => f x + r\n  let f_conj := fun s => Conjugate f s\n  let g_conj := fun s => Conjugate g s\n  ∀ s, g_conj s = f_conj s - r := by\n  sorry",
        "proof": ""
    },
    {
        "statement": "Let $f$ satisfy (1.1.1), let $H$ be a subspace of $\\mathbb{R}^n$, and call $p_H$ the operator of orthogonal projection onto $H$. Suppose that there is a point in $H$ where $f$ is finite. Then $f+p_{iH}$ satisfies (1.1.1) and its conjugate is\n\\[\n(f+p_{iH})^*=(f\\circ p_H)^*\\circ p_H .\n\\tag{1.3.1}\n\\]",
        "title": "",
        "label": "prop:FCA-chapE-1.3.2",
        "lean_tag": [
            "FCA_chap_E_1_3_2"
        ],
        "lean_formalization": "lemma FCA_chap_E_1_3_2 {n : ℕ}\n  (f : EuclideanSpace ℝ (Fin n) → WithTop ℝ) (H : Subspace ℝ (EuclideanSpace ℝ (Fin n)))\n  (hf_nondegenerate : IsNondegenerate f)\n  : let pH : EuclideanSpace ℝ (Fin n) → EuclideanSpace ℝ (Fin n) := fun x => Submodule.orthogonalProjection H x\n    let g := fun x => f x + (Indicator H) x\n    ∀ s, Conjugate g s = Conjugate (f ∘ pH) (pH s)\n  := by sorry",
        "proof": "When $y$ describes $\\mathbb{R}^n$, $p_H y$ describes $H$ so we can write, knowing that $p_H$ is symmetric:\n\\[\n(f+p_{iH})^*(s):=\\sup\\{\\langle s,x\\rangle-f(x):x\\in H\\}\n=\\sup\\{\\langle s,p_H y\\rangle-f(p_H y):y\\in\\mathbb{R}^n\\}\n=\\sup\\{\\langle p_H s,y\\rangle-f(p_H y):y\\in\\mathbb{R}^n\\}.\n\\]"
    },
    {
        "statement": "For $f$ satisfying (1.1.1), let a subspace $V$ contain the subspace parallel to $\\operatorname{aff\\,dom}f$ and set $U:=V^{\\perp}$. For any $z\\in\\operatorname{aff\\,dom}f$ and any $s\\in\\mathbb{R}^{n}$ decomposed as $s=s_{U}+s_{V}$, there holds\n\\[\nf^{*}(s)=\\langle s_{U},z\\rangle+f^{*}(s_{V}).\n\\]",
        "title": "",
        "label": "prop:FCA-chapE-1.3.4",
        "lean_tag": [
            "FCA_chap_E_1_3_4"
        ],
        "lean_formalization": "lemma FCA_chap_E_1_3_4 {n : ℕ}\n  (f : EuclideanSpace ℝ (Fin n) → WithTop ℝ) (V : Subspace ℝ (EuclideanSpace ℝ (Fin n)))\n  (hf_nondegenerate : IsNondegenerate f) (hV_contains_affdom : affineSpan ℝ (effDom (liftWithToptoEReal f)))\n  : let U := Vᗮ\n    let pV : EuclideanSpace ℝ (Fin n) → EuclideanSpace ℝ (Fin n) := fun x => Submodule.orthogonalProjection V x\n    let pU : EuclideanSpace ℝ (Fin n) → EuclideanSpace ℝ (Fin n) := fun x => Submodule.orthogonalProjection U x\n    ∀ z ∈ affineSpan ℝ (effDom (liftWithToptoEReal f)), ∀ s,\n    Conjugate f s = inner ℝ (pU s) z + Conjugate f (pV s)\n  := by sorry",
        "proof": "In (1.1.2), the variable $x$ can range through $z+V\\supset\\operatorname{aff\\,dom}f$:\n\\[\n\\begin{aligned}\nf^{*}(s)&=\\sup_{v\\in V}[\\langle s_{U}+s_{V},z+v\\rangle-f(z+v)]\\\\\n&=\\langle s_{U},z\\rangle+\\sup_{v\\in V}[\\langle s_{V},z+v\\rangle-f(z+v)]\\\\\n&=\\langle s_{U},z\\rangle+f^{*}(s_{V}).\n\\end{aligned}\n\\]"
    },
    {
        "statement": "For $f$ satisfying (1.1.1), the function $f^{**}$ of (1.3.2) is the pointwise supremum of all the affine functions on $\\mathbb{R}^n$ majorized by $f$.  In other words\n\\[\n\\operatorname{epi} f^{**}=\\overline{\\operatorname{co}}\\bigl(\\operatorname{epi} f\\bigr).\n\\tag{1.3.3}\n\\]",
        "title": "",
        "label": "thm:FCA-chapE-1.3.5",
        "lean_tag": [
            "FCA_chap_E_1_3_5"
        ],
        "lean_formalization": "lemma FCA_chap_E_1_3_5 {n : ℕ}\n  (f : EuclideanSpace ℝ (Fin n) → WithTop ℝ) (hf_nondegenerate : IsNondegenerate f)\n  : epigraph (liftWithToptoEReal (Biconjugate f)) = closure (convexHull ℝ (epigraph (liftWithToptoEReal f)))\n  := by sorry",
        "proof": "Call $\\Sigma\\subset\\mathbb{R}^n\\times\\mathbb{R}$ the set of pairs $(s,r)$ defining affine functions $\\langle s,\\cdot\\rangle - r$ majorized by $f$:\n\\[\n(s,r)\\in\\Sigma\\iff f(x)\\ge\\langle s,x\\rangle - r\\quad\\text{for all }x\\in\\mathbb{R}^n\n\\]\n\\[\n\\iff r\\ge\\sup\\{\\,\\langle s,x\\rangle - f(x):x\\in\\mathbb{R}^n\\,\\}\n\\]\n\\[\n\\iff r\\ge f^*(s)\\qquad(\\text{and }s\\in\\operatorname{dom}f^*!).\n\\]\n\nThen we obtain, for $x\\in\\mathbb{R}^n$,\n\\[\n\\sup_{(s,r)\\in\\Sigma}\\{\\langle s,x\\rangle - r\\}\n= \\sup\\{\\langle s,x\\rangle - r : s\\in\\operatorname{dom}f^*,\\ -r\\le -f^*(s)\\}\n\\]\n\\[\n= \\sup\\{\\langle s,x\\rangle - f^*(s): s\\in\\operatorname{dom}f^*\\}=f^{**}(x).\n\\]\n\nGeometrically, the epigraphs of the affine functions associated with $(s,r)\\in\\Sigma$ are the (non-vertical) closed half-spaces containing $\\operatorname{epi} f$.  From §B.2.5, the epigraph of their supremum is the closed convex hull of $\\operatorname{epi} f$, and this proves (1.3.3)."
    },
    {
        "statement": "If $g$ is a function satisfying $\\overline{\\operatorname{co}}\\,f \\le g \\le f$, then $g^* = f^*$. The function $f$ is equal to its biconjugate $f^{**}$ if and only if $f\\in\\overline{\\operatorname{Conv}}\\mathbb{R}^n$.",
        "title": "",
        "label": "cor:FCA-chapE-1.3.6",
        "lean_tag": [
            "FCA_chap_E_1_3_6"
        ],
        "lean_formalization": "lemma FCA_chap_E_1_3_6 {n : ℕ}\n  (f : EuclideanSpace ℝ (Fin n) → WithTop ℝ)\n  (g : EuclideanSpace ℝ (Fin n) → WithTop ℝ) :\n  ((Minorizes (Biconjugate f) g) ∧ (Minorizes g f) → ∀ s, Conjugate g s = Conjugate f s) ∧\n  ((Biconjugate f = f) ↔ (InClosedConvRn f)) := by\n  sorry",
        "proof": "Immediate."
    },
    {
        "statement": "If $f$ satisfying (1.1.1) is 1-coercive, then $f^*(s) < +\\infty$ for all $s \\in \\mathbb{R}^n$.",
        "title": "",
        "label": "prop:FCA-chapE-1.3.8",
        "lean_tag": [
            "FCA_chap_E_1_3_8"
        ],
        "lean_formalization": "lemma FCA_chap_E_1_3_8 {n : ℕ}\n  (f : EuclideanSpace ℝ (Fin n) → WithTop ℝ)\n  (hf_nondegenerate : IsNondegenerate f) (hf_coercive : IsOneCoerciveFun f)\n  : ∀ s, Conjugate f s < ⊤\n  := by sorry",
        "proof": "Let $s$ be given. The 1-coercivity of $f$ implies the existence of a number $R$ such that $f(x) \\ge \\|s\\|\\,\\|x\\|$ (hence $\\langle s,x\\rangle - f(x) \\le 0$) whenever $\\|x\\| \\ge R$. As a result, we have in (1.1.2)\n\\[\n\\sup\\{\\langle s,x\\rangle - \\|s\\|\\,\\|x\\| : \\|x\\| \\ge R\\} \\le 0.\n\\]\nOn the other hand, (1.1.1) implies an upper bound\n\\[\n\\sup \\{\\langle s,x\\rangle - f(x) : \\|x\\| \\le R\\} \\le M .\n\\]\nAltogether, $f^*(s) \\le \\max\\{0,M\\}$. \\qed"
    },
    {
        "statement": "For $f$ satisfying (1.1.1), the following holds:\n\\begin{enumerate}\n\\item[(i)] If $x_0\\in\\operatorname{int}\\dom f$ then $f^*-\\langle x_0,\\cdot\\rangle$ is $0$-coercive;\n\\item[(ii)] in particular, if $f$ is finite over $\\mathbb R^n$, then $f^*$ is $1$-coercive.\n\\end{enumerate}",
        "title": "",
        "label": "prop:FCA-chapE-1.3.9",
        "lean_tag": [
            "FCA_chap_E_1_3_9"
        ],
        "lean_formalization": "lemma FCA_chap_E_1_3_9 {n : ℕ}\n  (f : EuclideanSpace ℝ (Fin n) → WithTop ℝ) (x₀ : EuclideanSpace ℝ (Fin n))\n  (hf_nondegenerate : IsNondegenerate f) :\n  (x₀ ∈ interior (effDom (liftWithToptoEReal f)) → IsZeroCoerciveFun (fun x => Conjugate f x - inner ℝ x₀ x)) ∧\n  (∀ x, f x ≠ ⊤ → IsOneCoerciveFun (Conjugate f))\n  := by sorry",
        "proof": "We know from (1.2.3) that $\\sigma_{\\dom f}=(f^*)^{\\vee}_{\\infty}$ so, using Theorem C.2.2.3(iii),\n$x_0\\in\\operatorname{int}\\dom f\\subset\\int(\\codom f)$ implies $(f^*)^{\\vee}_{\\infty}(s)-\\langle x_0,s\\rangle>0$ for all $s\\neq0$.\nBy virtue of Proposition B.3.2.4, this means exactly that $f^*-\\langle x_0,\\cdot\\rangle$ has compact sublevel-sets; (i) is proved.\n\nThen, as demonstrated in Definition B.3.2.5, $0$-coercivity of $f^*-\\langle x_0,\\cdot\\rangle$ for all $x_0$ means $1$-coercivity of $f^*$."
    },
    {
        "statement": "For $f$ satisfying (1.1.1) and $\\partial f$ defined by (1.4.1), $s\\in\\partial f(x)$ if and only if\n\\[\nf^*(s)+f(x)-\\langle s,x\\rangle=0\\quad(\\text{or }\\le 0).\n\\tag{1.4.2}\n\\]",
        "title": "",
        "label": "thm:FCA-chapE-1.4.1",
        "lean_tag": [
            "FCA_chap_E_1_4_1"
        ],
        "lean_formalization": "lemma FCA_chap_E_1_4_1 {n : ℕ}\n  (f : EuclideanSpace ℝ (Fin n) → WithTop ℝ) (x s : EuclideanSpace ℝ (Fin n))\n  (hf_nondegenerate : IsNondegenerate f)\n  : s ∈ SubdifferentialAt f x ↔ (Conjugate f s) + f x - (inner ℝ s x) = 0\n  := by sorry",
        "proof": "To say that $s$ lies in the set (1.4.1) is to say that\n\\[\n\\langle s,y\\rangle-f(y)\\le\\langle s,x\\rangle-f(x)\\quad\\text{for all }y\\in\\dom f,\n\\]\ni.e.\\ $f^*(s)\\le\\langle s,x\\rangle-f(x)$; but this is indeed an equality, in view of Fenchel's inequality (1.1.3)."
    },
    {
        "statement": "Let $f\\in\\operatorname{Conv}\\mathbb{R}^n$.  Then $\\partial f(x)\\neq\\varnothing$ whenever $x\\in\\operatorname{ri}\\dom f$.",
        "title": "",
        "label": "thm:FCA-chapE-1.4.2",
        "lean_tag": [
            "FCA_chap_E_1_4_2"
        ],
        "lean_formalization": "lemma FCA_chap_E_1_4_2 {n : ℕ}\n  (f : EuclideanSpace ℝ (Fin n) → WithTop ℝ) (x : EuclideanSpace ℝ (Fin n))\n  (hf_convex : InConvRn f)\n  : x ∈ intrinsicInterior ℝ (effDom (liftWithToptoEReal f)) → Set.Nonempty (SubdifferentialAt f x)\n  := by sorry",
        "proof": "This is Proposition B.1.2.1."
    },
    {
        "statement": "For $f$ satisfying (1.1.1), the following properties hold:\n\\[\n\\partial f(x)\\neq\\varnothing \\quad\\Longrightarrow\\quad (\\overline{\\operatorname{co}}\\,f)(x)=f(x);\\tag{1.4.3}\n\\]\n\\[\n\\overline{\\operatorname{co}}\\,f\\le g\\le f\\ \\text{ and }\\ g(x)=f(x)\n\\quad\\Longrightarrow\\quad \\partial g(x)=\\partial f(x);\\tag{1.4.4}\n\\]\n\\[\ns\\in\\partial f(x)\\quad\\Longrightarrow\\quad x\\in\\partial f^*(s).\\tag{1.4.5}\n\\]",
        "title": "",
        "label": "prop:FCA-chapE-1.4.3",
        "lean_tag": [
            "FCA_chap_E_1_4_3"
        ],
        "lean_formalization": "lemma FCA_chap_E_1_4_3 {n : ℕ}\n  (f : EuclideanSpace ℝ (Fin n) → WithTop ℝ)\n  (g : EuclideanSpace ℝ (Fin n) → WithTop ℝ)\n  (x s : EuclideanSpace ℝ (Fin n))\n  (hf_nondegenerate : IsNondegenerate f) :\n  (Set.Nonempty (SubdifferentialAt f x) → Biconjugate f x = f x) ∧\n  ((Minorizes (Biconjugate f) g) ∧ (Minorizes g f) ∧ (g x = f x) →\n  (SubdifferentialAt g x) = (SubdifferentialAt f x)) ∧\n  (s ∈ SubdifferentialAt f x → x ∈ SubdifferentialAt (Conjugate f) s) := by\n  sorry",
        "proof": "Let $s$ be a subgradient of $f$ at $x$. From the definition (1.4.1) itself, the function $y\\mapsto \\ell_s(y):=f(x)+\\langle s,y-x\\rangle$ is affine and minorizes $f$, hence $\\ell_s\\le \\overline{\\operatorname{co}}\\,f\\le f$; because $\\ell_s(x)=f(x)$, this implies (1.4.3).\n\nNow, $s\\in\\partial f(x)$ if and only if (1.4.2) holds. From our assumption in (1.4.4), $f^*=g^*=(\\overline{\\operatorname{co}}\\,f)^*$ (Corollary 1.3.6) and $g(x)=f(x)$. Therefore\n\\[\ns\\in\\partial f(x)\\iff g^*(s)+g(x)-\\langle s,x\\rangle=0,\n\\]\nwhich expresses exactly that $s\\in\\partial g(x)$; (1.4.4) is proved.\n\nFinally, we know that $f^{**}=\\overline{\\operatorname{co}}\\,f\\le f$; so, when $s$ satisfies (1.4.2), we have\n\\[\nf^*(s)+f^{**}(x)-\\langle s,x\\rangle\n= f^*(s)+(\\overline{\\operatorname{co}}\\,f)(x)-\\langle s,x\\rangle\\le 0,\n\\]\nwhich means $x\\in\\partial f^*(s)$: we have just proved (1.4.5)."
    },
    {
        "statement": "If $f\\in\\operatorname{Conv}\\mathbb{R}^n$, the following equivalences hold:\n\\[\nf(x)+f^*(s)-\\langle s,x\\rangle=0\\ (\\text{or }\\le 0)\n\\quad\\Longleftrightarrow\\quad s\\in\\partial f(x)\n\\quad\\Longleftrightarrow\\quad x\\in\\partial f^*(s).\n\\]",
        "title": "",
        "label": "thm:FCA-chapE-1.4.4",
        "lean_tag": [
            "FCA_chap_E_1_4_4"
        ],
        "lean_formalization": "lemma FCA_chap_E_1_4_4 {n : ℕ}\n  (f : EuclideanSpace ℝ (Fin n) → WithTop ℝ) (x s : EuclideanSpace ℝ (Fin n))\n  (hf_closed_convex : InClosedConvRn f)\n  : List.TFAE [\n    f x + Conjugate f s - inner ℝ s x = 0,\n    s ∈ SubdifferentialAt f x,\n    x ∈ SubdifferentialAt (Conjugate f) s\n  ]\n  := by sorry",
        "proof": "This is a rewriting of Theorem 1.4.1, taking into account (1.4.5) and the symmetric role played by $f$ and $f^*$ when $f\\in\\operatorname{Conv}\\mathbb{R}^n$."
    },
    {
        "statement": "With the above notation, assume that $\\operatorname{Im}A^* \\cap \\dom g^* \\neq\\varnothing$. Then $Ag$ satisfies (1.1.1) and its conjugate is\n\\[\n(Ag)^* = g^* \\circ A^* .\n\\]",
        "title": "",
        "label": "thm:FCA-chapE-2.1.1",
        "lean_tag": [
            "FCA_chap_E_2_1_1"
        ],
        "lean_formalization": "lemma FCA_chap_E_2_1_1 {m n : ℕ}\n  (g : EuclideanSpace ℝ (Fin m) → WithTop ℝ)\n  (A : (EuclideanSpace ℝ (Fin m)) →ₗ[ℝ] (EuclideanSpace ℝ (Fin n)))\n  (hg_nondegenerate : IsNondegenerate g)\n  (h_nonempty_domain : Set.Nonempty ((Im A.adjoint) ∩ effDom (liftWithToptoEReal (Conjugate g))))\n  : let h := fun x => sInf (Set.image g {y | A y = x})\n    ∀ s, Conjugate h s = Conjugate g (A.adjoint s)\n  := by sorry",
        "proof": "First, it is clear that $Ag \\not\\equiv +\\infty$ (take $x=Ay$, with $y\\in\\dom g$). On the other hand, our assumption implies the existence of some $p_0 = A^*s_0$ such that $g^*(p_0)<+\\infty$; with Fenchel's inequality (1.1.3), we have for all $y\\in\\mathbb R^m$:\n\\[\ng(y) \\ge \\langle A^*s_0,y\\rangle_m - g^*(p_0) = \\langle s_0,Ay\\rangle_n - g^*(p_0).\n\\]\nFor each $x\\in\\mathbb R^n$, take the infimum over those $y$ satisfying $Ay=x$: the affine function $\\langle s_0,\\cdot\\rangle - g^*(p_0)$ minorizes $Ag$. Altogether, $Ag$ satisfies (1.1.1).\n\nThen we have for $s\\in\\mathbb R^n$\n\\[\n(Ag)^*(s) = \\sup_{x\\in\\mathbb R^n}[\\langle s,x\\rangle - \\inf_{Ay=x} g(y)]\n= \\sup_{x\\in\\mathbb R^n,\\,Ay=x}[\\langle s,x\\rangle - g(y)]\n= \\sup_{y\\in\\mathbb R^m}[\\langle s,Ay\\rangle - g(y)] = g^*(A^*s).\n\\]"
    },
    {
        "statement": "With $g:\\mathbb{R}^n\\times\\mathbb{R}^p=\\mathbb{R}^m\\to\\mathbb{R}\\cup\\{+\\infty\\}$ not identically $+\\infty$, let $g^*$ be associated with a scalar product preserving the structure of $\\mathbb{R}^m$ as a product space: $\\langle\\cdot,\\cdot\\rangle_m=\\langle\\cdot,\\cdot\\rangle_n+\\langle\\cdot,\\cdot\\rangle_p$. If there exists $s_0\\in\\mathbb{R}^n$ such that $(s_0,0)\\in\\operatorname{dom} g^*$, then the conjugate of $f$ defined by (2.1.2) is\n\\[\nf^*(s)=g^*(s,0)\\qquad\\text{for all }s\\in\\mathbb{R}^n.\n\\]",
        "title": "",
        "label": "cor:FCA-chapE-2.1.2",
        "lean_tag": "no-lean-tag",
        "proof": "It suffices to observe that, $A$ being the projection defined above, there holds for all $y_1=(x_1,z_1)\\in\\mathbb{R}^{m}$ and $x_2\\in\\mathbb{R}^n$,\n\\[\n\\langle Ay_1,x_2\\rangle_n=\\langle x_1,x_2\\rangle_n=\\langle x_1,x_2\\rangle_n+\\langle z_1,0\\rangle_p=\\langle y_1,(x_2,0)\\rangle_m,\n\\]\nwhich defines the adjoint $A^*x=(x,0)$ for all $x\\in\\mathbb{R}^n$. Then apply Theorem 2.1.1."
    },
    {
        "statement": "Let $f_1$ and $f_2$ be two functions from $\\mathbb{R}^n$ to $\\mathbb{R}\\cup\\{+\\infty\\}$, not identically $+\\infty$, and satisfying $\\operatorname{dom} f_1^*\\cap\\operatorname{dom} f_2^*\\neq\\varnothing$. Then their inf-convolution satisfies (1.1.1), and $(f_1\\infconv f_2)^*=f_1^*+f_2^*$.",
        "title": "",
        "label": "cor:FCA-chapE-2.1.3",
        "lean_tag": "no-lean-tag",
        "proof": "Equip $\\mathbb{R}^n\\times\\mathbb{R}^n$ with the scalar product $\\langle\\cdot,\\cdot\\rangle+\\langle\\cdot,\\cdot\\rangle$. Using the above notation for $g$ and $A$, we have $g^*(s_1,s_2)=f_1^*(s_1)+f_2^*(s_2)$ (Proposition 1.3.1(ix)) and $A^*(s)=(s,s)$. Then apply the definitions."
    },
    {
        "statement": "Take $g\\in\\Conv\\mathbb{R}^m$, $A_0$ linear from $\\mathbb{R}^n$ to $\\mathbb{R}^m$ and consider the affine operator $A(x):=A_0x+y_0\\in\\mathbb{R}^m$. Suppose that $A(\\mathbb{R}^n)\\cap\\dom g\\neq\\emptyset$. Then\n$g\\circ A\\in\\Conv\\mathbb{R}^n$ and its conjugate is the closure of the convex function\n\\[\n\\mathbb{R}^n\\ni s\\mapsto\\inf_p\\{\\,g^*(p)-\\langle y_0,p\\rangle_m : A_0^*p=s\\,\\}.\n\\tag{2.2.1}\n\\]",
        "title": "",
        "label": "thm:FCA-chapE-2.2.1",
        "lean_tag": "no-lean-tag",
        "proof": "We start with the linear case ($y_0=0$): suppose that $h\\in\\Conv\\mathbb{R}^n$ satisfies $\\Im A_0\\cap\\dom h\\neq\\emptyset$. Then Theorem 2.1.1 applied to $g:=h^*$ and $A:=A_0^*$ gives $(A_0^*h^*)^*=h\\circ A_0$; conjugating both sides, we see that the conjugate of $h\\circ A_0$ is the closure of the image-function $A_0^*h^*$.\n\nIn the affine case, consider the function $h:=g(\\cdot+y_0)\\in\\Conv\\mathbb{R}^m$; its conjugate is given by Proposition 1.3.1(v): $h^*=g^*-\\langle y_0,\\cdot\\rangle_m$. Furthermore, it is clear that\n\\[\n(g\\circ A)(x)=g(A_0x+y_0)=h(A_0x)=(h\\circ A_0)(x),\n\\]\nso (2.2.1) follows from the linear case."
    },
    {
        "statement": "Take $g\\in\\operatorname{Conv}\\mathbb{R}^m$, $A_0$ linear from $\\mathbb{R}^n$ to $\\mathbb{R}^m$ and consider the affine operator $A(x):=A_0x+y_0\\in\\mathbb{R}^m$. Make the following assumption:\n\\[\nA(\\mathbb{R}^n)\\cap\\operatorname{ri}\\operatorname{dom}g\\neq\\varnothing.\n\\tag{2.2.4}\n\\]\nThen, for every $s\\in\\operatorname{dom}(g\\circ A_0)^*$, the problem\n\\[\n\\min_p\\{\\,g^*(p)-\\langle p,y_0\\rangle :\\ A_0^*p=s\\,\\}\n\\tag{2.2.5}\n\\]\nhas at least one optimal solution $\\bar p$ and there holds $(g\\circ A)^*(s)=g^*(\\bar p)-\\langle\\bar p,y_0\\rangle$.",
        "title": "",
        "label": "thm:FCA-chapE-2.2.3",
        "lean_tag": "no-lean-tag",
        "proof": "By assumption, we can choose $\\bar x \\in \\mathbb{R}^n$ such that $\\bar y := A(\\bar x)\\in\\operatorname{ri}\\operatorname{dom} g$. Consider the function $\\bar g:=g(\\bar y+\\cdot)\\in\\operatorname{Conv}\\mathbb{R}^m$. Observing that\n\\[\n(g\\circ A)(x)=\\bar g(A(x)-\\bar y)=(\\bar g\\circ A_0)(x-\\bar x),\n\\]\nwe obtain from the calculus rule 1.3.1(v): $(g\\circ A)^*=(\\bar g\\circ A_0)^*+\\langle\\cdot,\\bar x\\rangle$.\n\nThen Lemma 2.2.2 allows the computation of this conjugate. We have $0$ in the domain of $\\bar g$, and even in its relative interior:\n\\[\n\\operatorname{ri}\\operatorname{dom}\\bar g=\\operatorname{ri}\\operatorname{dom} g-\\{\\bar y\\}\\ni 0\\in\\operatorname{Im}A_0.\n\\]\nWe can therefore write: for all $s\\in\\operatorname{dom}(\\bar g\\circ A_0)^* [=\\operatorname{dom}(g\\circ A)^*]$,\n\\[\n(\\bar g\\circ A_0)^*(s)=\\min_{p}\\{\\bar g^*(p)\\;:\\;A_0^*p=s\\},\n\\]\nwhere the minimum is attained at some $\\bar p$. Using again the calculus rule 1.3.1(v) and various relations from above, we have established\n\\[\n(g\\circ A)^*(s)-\\langle s,\\bar x\\rangle=\\min\\{\\bar g^*(p)-\\langle p,A_0\\bar x+y_0\\rangle:\\;A_0^*p=s\\}\n\\]\n\\[\n=\\min\\{g^*(p)-\\langle p,y_0\\rangle:\\;A_0^*p=s\\}-\\langle s,\\bar x\\rangle.\n\\]"
    },
    {
        "statement": "Let $g_1,g_2$ be in $\\operatorname{Conv}\\mathbb{R}^n$ and assume that $\\operatorname{dom}g_1\\cap\\operatorname{dom}g_2\\neq\\varnothing$. The conjugate $(g_1+g_2)^*$ of their sum is the closure of the convex function $g_1^*\\infconv g_2^*$.",
        "title": "",
        "label": "thm:FCA-chapE-2.3.1",
        "lean_tag": "no-lean-tag",
        "proof": "Call $f_i^*:=g_i$, for $i=1,2$; apply Corollary 2.1.3: $(g_1^*\\infconv g_2^*)^*=g_1+g_2$; then take the conjugate again."
    },
    {
        "statement": "Let $g_1,g_2$ be in $\\Conv\\R^n$ and assume that\n\\[\n\\begin{aligned}\n&\\text{the relative interiors of }\\dom g_1\\text{ and }\\dom g_2\\text{ intersect,}\\\\\n&\\text{or equivalently: }0\\in\\ri(\\dom g_1-\\dom g_2).\n\\end{aligned}\n\\tag{2.3.1}\n\\]\nThen $(g_1+g_2)^* = g_1^*\\infconv g_2^*$ and, for every $s\\in\\dom (g_1+g_2)^*$, the problem\n\\[\n\\inf\\{g_1^*(p)+g_2^*(q)\\;:\\;p+q=s\\}\n\\]\nhas at least one optimal solution $(\\bar p,\\bar q)$, which therefore satisfies\n\\[\ng_1^*(\\bar p)+g_2^*(\\bar q)=(g_1^*\\infconv g_2^*)(s)=(g_1+g_2)^*(s).\n\\]",
        "title": "",
        "label": "thm:FCA-chapE-2.3.2",
        "lean_tag": "no-lean-tag",
        "proof": "Define $g\\in\\Conv(\\R^n\\times\\R^n)$ by $g(x_1,x_2):=g_1(x_1)+g_2(x_2)$ and the linear operator $A:\\R^n\\to\\R^n\\times\\R^n$ by $Ax:=(x,x)$. Then $g\\circ A=g_1+g_2$, and we proceed to use Theorem 2.2.3. As seen in Proposition 1.3.1(ix), $g^*(p,q)=g_1^*(p)+g_2^*(q)$ and straightforward calculation shows that $A^*(p,q)=p+q$. Thus, if we can apply Theorem 2.2.3, we can write\n\\[\n(g_1+g_2)^*(s)=(g\\circ A)^*(s)=(A^*g^*)(s)\n=\\inf_{p,q}\\{g_1^*(p)+g_2^*(q)\\;:\\;p+q=s\\}=(g_1^*\\infconv g_2^*)(s)\n\\]\nand the above minimization problem does have an optimal solution."
    },
    {
        "statement": "Let $\\{f_j\\}_{j\\in J}$ be a collection of functions satisfying (1.1.1) and having a common affine minorant: $\\sup_{j\\in J} f_j^*(s) < +\\infty$ for some $s\\in\\mathbb{R}^n$.\nThen their infimum $f := \\inf_{j\\in J} f_j$ satisfies (1.1.1), and its conjugate is the supremum of the $f_j^*$'s:\n\\[\n(\\inf_{j\\in J} f_j)^* = \\sup_{j\\in J} f_j^*.\n\\tag{2.4.1}\n\\]",
        "title": "",
        "label": "thm:FCA-chapE-2.4.1",
        "lean_tag": "no-lean-tag",
        "proof": "By definition, for all $s\\in\\mathbb{R}^n$\n\\[\n\\begin{aligned}\nf^*(s)\n&= \\sup_x\\big[\\langle s,x\\rangle - \\inf_{j\\in J} f_j(x)\\big] \\\\\n&= \\sup_x \\sup_j\\big[\\langle s,x\\rangle - f_j(x)\\big] \\\\\n&= \\sup_j \\sup_x\\big[\\langle s,x\\rangle - f_j(x)\\big] = \\sup_{j\\in J} f_j^*(s).\n\\end{aligned}\n\\]"
    },
    {
        "statement": "Let $\\{g_j\\}_{j\\in J}$ be a collection of functions in \\(\\Conv\\mathbb{R}^n\\).  If their supremum \\(g:=\\sup_{j\\in J} g_j\\) is not identically \\(+\\infty\\), it is in \\(\\Conv\\mathbb{R}^n\\), and its conjugate is the closed convex hull of the \\(g_j^*\\)'s:\n\\[\n\\bigl(\\sup_{j\\in J} g_j\\bigr)^* = \\overline{\\co}\\bigl(\\inf_{j\\in J} g_j^*\\bigr).\n\\tag{2.4.5}\n\\]",
        "title": "",
        "label": "thm:FCA-chapE-2.4.4",
        "lean_tag": "no-lean-tag",
        "proof": "Call \\(f_j:=g_j^*\\), hence \\(f_j^*=g_j\\), and \\(g\\) is nothing but the \\(f^*\\) of (2.4.1).  Taking the conjugate of both sides, the result follows from (1.3.4)."
    },
    {
        "statement": "With $f$ and $g$ defined as above, assume that $f(\\mathbb R^n)\\cap\\operatorname{int}\\operatorname{dom}g\\neq\\varnothing$. For all $s\\in\\operatorname{dom}(g\\circ f)^*$, define the function $\\psi_s\\in\\operatorname{Conv}\\mathbb R$ by\n\\[\n\\mathbb R\\ni\\alpha\\mapsto\\psi_s(\\alpha):=\n\\begin{cases}\n\\alpha f^*\\!\\big(\\tfrac{1}{\\alpha}s\\big)+g^*(\\alpha)&\\text{if }\\alpha>0,\\\\[4pt]\n\\sigma_{\\operatorname{dom}f}(s)+g^*(0)&\\text{if }\\alpha=0,\\\\[4pt]\n+\\infty&\\text{if }\\alpha<0.\n\\end{cases}\n\\]\nThen $(g\\circ f)^*(s)=\\min_{\\alpha\\in\\mathbb R}\\psi_s(\\alpha)$.",
        "title": "",
        "label": "thm:FCA-chapE-2.5.1",
        "lean_tag": "no-lean-tag",
        "proof": "By definition,\n\\[\n-(g\\circ f)^*(s)=\\inf_x[g(f(x))-\\langle s,x\\rangle]\n=\\inf_{x,r}\\{g(r)-\\langle s,x\\rangle:\\;f(x)\\le r\\}\n=\\inf_{x,r}[g(r)-\\langle s,x\\rangle+\\iota_{\\operatorname{epi}f}(x,r)].\n\\]\n[ $g$ is increasing]\n\nWe must compute the conjugate of the sum of the two functions $f_1(x,r):=g(r)-\\langle s,x\\rangle$ and $f_2:=\\iota_{\\operatorname{epi}f}$, at the obvious argument $(x,r)\\in\\mathbb R^n\\times\\mathbb R$. We have $\\operatorname{dom}f_1=\\mathbb R^n\\times\\operatorname{dom}g$, so that $\\operatorname{dom}f_1=\\mathbb R^n\\times\\operatorname{dom}g$; hence, by assumption:\n\\[\n\\operatorname{int}\\operatorname{dom}f_1\\cap\\operatorname{dom}f_2=(\\mathbb R^n\\times\\operatorname{int}\\operatorname{dom}g)\\cap\\operatorname{epi}f\\neq\\varnothing.\n\\]\n\nTheorem 2.3.2, more precisely Fenchel's duality theorem (2.3.2), can be applied with the qualification condition (2.3.Q.jj'):\n\\[\n(g\\circ f)^*(s)=\\min\\{f_1^*(-p,\\alpha)+f_2^*(p,-\\alpha):\\;(p,\\alpha)\\in\\mathbb R^n\\times\\mathbb R\\}.\n\\]\n\nThe computation of the above two conjugates is straightforward and gives\n\\[\n(g\\circ f)^*(s)=\\min_{p,\\alpha}\\big[g^*(\\alpha)+\\iota_{\\{-s\\}}(-p)+\\sigma_{\\operatorname{epi}f}(p,-\\alpha)\\big]\n=\\min_{\\alpha}\\psi_s(\\alpha),\n\\]\nwhere the second equality comes from (1.2.2)."
    },
    {
        "statement": "The function $g$ of (3.2.1) has the conjugate\n\\[\ng^{*}(s)=\\begin{cases}\n\\tfrac{1}{2}\\langle s, (p_{H}\\circ B\\circ p_{H})^{-}s\\rangle & \\text{if } s\\in\\operatorname{Im}B+H^{\\perp},\\\\[4pt]\n+\\infty & \\text{otherwise},\n\\end{cases}\\tag{3.2.2}\n\\]\nwhere $p_{H}$ is the operator of orthogonal projection onto $H$ and $(\\cdot)^{-}$ is the Moore--Penrose pseudo-inverse.",
        "title": "",
        "label": "prop:FCA-chapE-3.2.1",
        "lean_tag": "no-lean-tag",
        "proof": "Set $f:=\\tfrac{1}{2}\\langle B\\cdot,\\cdot\\rangle$, so that $g=f+i_{H}$ and $g^{*}=(f\\circ p_{H})^{*}\\circ p_{H}$ (Proposition 1.3.2). Knowing that $(f\\circ p_{H})(x)=\\tfrac{1}{2}\\langle (p_{H}\\circ B\\circ p_{H})x,x\\rangle$, we obtain from Example 1.1.4 the conjugate $g^{*}(s)$ under the form\n\\[\n(f\\circ p_{H})^{*}(p_{H}s)=\\begin{cases}\n\\tfrac{1}{2}\\langle s,(p_{H}\\circ B\\circ p_{H})^{-}s\\rangle & \\text{if } p_{H}s\\in\\operatorname{Im}(p_{H}\\circ B\\circ p_{H}),\\\\[4pt]\n+\\infty & \\text{otherwise}.\n\\end{cases}\n\\]\n\nIt could be checked directly that $\\operatorname{Im}(p_{H}\\circ B\\circ p_{H})+H^{\\perp}=\\operatorname{Im}B+H^{\\perp}$. A simpler argument, however, is obtained via Theorem 2.3.2, which can be applied since $\\operatorname{dom}f=\\mathbb{R}^{n}$. Thus,\n\\[\ng^{*}(s)=(f^{*}\\,\\square\\,i_{H^{\\perp}})(s)=\\min\\{\\tfrac{1}{2}\\langle p,B^{-}p\\rangle:\\;p\\in\\operatorname{Im}B,\\;s-p\\in H^{\\perp}\\},\n\\]\nwhich shows that $\\operatorname{dom}g^{*}=\\operatorname{Im}B+H^{\\perp}$."
    },
    {
        "statement": "At each $s\\in\\operatorname{co}\\{s_1,\\ldots,s_k\\}=\\operatorname{dom}f^*$, the conjugate of $f$ has the value ( $\\Delta_k$ is the unit simplex)\n\\[\nf^*(s)=\\min\\Big\\{\\sum_{i=1}^k \\alpha_i b_i:\\ \\alpha\\in\\Delta_k,\\ \\sum_{i=1}^k \\alpha_i s_i=s\\Big\\}.\n\\tag{3.3.2}\n\\]",
        "title": "",
        "label": "prop:FCA-chapE-3.3.1",
        "lean_tag": "no-lean-tag",
        "proof": "Set $g_i(s):= \\iota_{\\{s_i\\}}+b_i$ and\n\\[\ng(s):=(\\inf_i g_i)(s)=\n\\begin{cases}\nb_i &\\text{if } s=s_i\\text{ for some } i=1,\\ldots,k,\\\\[4pt]\n+\\infty &\\text{otherwise}.\n\\end{cases}\n\\]\nApply Proposition B.2.5.4 to see that $\\overline{\\operatorname{co}}\\,g=f^*$ of (3.3.2). The rest follows from Theorem 2.4.1 or 2.4.4, with a notational flip of $f$ and $g$. \\qedhere"
    },
    {
        "statement": "Let $f\\in\\operatorname{Conv}\\R^n$ be strictly convex. Then $\\operatorname{int}\\dom f^*\\neq\\varnothing$ and $f^*$ is continuously differentiable on $\\operatorname{int}\\dom f^*$.",
        "title": "",
        "label": "thm:FCA-chapE-4.1.1",
        "lean_tag": "no-lean-tag",
        "proof": "For arbitrary $x_0\\in\\dom f$ and nonzero $d\\in\\R^n$, consider Example 2.4.6. Strict convexity of $f$ implies that\n\\[\n0<\\frac{f(x_0 - td)-f(x_0)}{t}+\\frac{f(x_0+td)-f(x_0)}{t}\\qquad\\text{for all }t>0,\n\\]\nand this inequality extends to the suprema: $0<f'_ \\infty(-d)+f'_\\infty(d)$. Remembering that $f'_\\infty=\\sigma_{\\dom f^*}$ (Proposition 1.2.2), this means\n\\[\n\\sigma_{\\dom f^*}(d)+\\sigma_{\\dom f^*}(-d)>0,\n\\]\ni.e.\\ $\\dom f^*$ has a positive breadth in every nonzero direction $d$: its interior is nonempty---Theorem C2.2.3(iii).\n\nNow suppose that there is some $s\\in\\int\\dom f^*$ such that $\\partial f^*(s)$ contains two distinct points $x_1$ and $x_2$. Then $s\\in\\partial f(x_1)\\cap\\partial f(x_2)$; by convex combination of the relations\n\\[\nf^*(s)+f(x_i)=\\langle s,x_i\\rangle\\qquad\\text{for }i=1,2\n\\]\nwe deduce, using Fenchel's inequality (1.1.3):\n\\[\nf^*(s)+\\sum_{i=1}^2\\alpha_i f(x_i)=\\langle s,\\sum_{i=1}^2\\alpha_i x_i\\rangle\\le f^*(s)+f\\Big(\\sum_{i=1}^2\\alpha_i x_i\\Big),\n\\]\nwhich implies that $f$ is affine on $[x_1,x_2]$, a contradiction. In other words, $\\partial f^*$ is single-valued on $\\int\\dom f^*$, and this means that $f^*$ is continuously differentiable there (Remark 6.2.6)."
    },
    {
        "statement": "Let $f\\in\\operatorname{Conv}\\mathbb{R}^n$ be differentiable on the set $\\Omega:=\\operatorname{int}\\operatorname{dom}f$.  Then $f^*$ is strictly convex on each convex subset $C\\subset\\nabla f(\\Omega)$.",
        "title": "",
        "label": "thm:FCA-chapE-4.1.2",
        "lean_tag": "no-lean-tag",
        "proof": "Let $C$ be a convex set as stated.  Suppose that there are two distinct points $s_1$ and $s_2$ in $C$ such that $f^*$ is affine on the line-segment $[s_1,s_2]$.  Then, setting $s:=\\tfrac12(s_1+s_2)\\in C\\subset\\nabla f(\\Omega)$, there is $x\\in\\Omega$ such that $\\nabla f(x)=s$, i.e.\\ $x\\in\\partial f^*(s)$.  Using the affine character of $f^*$, we have\n\\[\n0=f(x)+f^*(s)-\\langle s,x\\rangle\n=\\tfrac12\\sum_{i=1}^2\\bigl[f(x)+f^*(s_i)-\\langle s_i,x\\rangle\\bigr]\n\\]\nand, in view of Fenchel's inequality (1.1.3), this implies that each term in the bracket is $0$: $x\\in\\partial f^*(s_1)\\cap\\partial f^*(s_2)$, i.e.\\ $\\partial f(x)$ contains the two points $s_1$ and $s_2$, a contradiction to the existence of $\\nabla f(x)$."
    },
    {
        "statement": "Let $f:\\mathbb{R}^n\\to\\mathbb{R}$ be strictly convex, differentiable and 1-coercive. Then\n\\begin{enumerate}\n\\item[(i)] $f^{*}$ is also finite-valued on $\\mathbb{R}^n$, strictly convex, differentiable and 1-coercive;\n\\item[(ii)] the continuous mapping $\\nabla f$ is one-to-one from $\\mathbb{R}^n$ onto $\\mathbb{R}^n$, and its inverse is continuous;\n\\end{enumerate}\n\n\\[\n\\text{(iii) }\\; f^*(s)=\\langle s,(\\nabla f)^{-1}(s)\\rangle - f\\bigl((\\nabla f)^{-1}(s)\\bigr)\\quad\\text{for all }s\\in\\mathbb{R}^n.\n\\]",
        "title": "",
        "label": "cor:FCA-chapE-4.1.3",
        "lean_tag": "no-lean-tag",
        "proof": ""
    },
    {
        "statement": "Assume that $f:\\mathbb{R}^n\\to\\mathbb{R}$ is strongly convex with modulus $c>0$ on $\\mathbb{R}^n$: for all $(x_1,x_2)\\in\\mathbb{R}^n\\times\\mathbb{R}^n$ and $\\alpha\\in]0,1[$,\n\\[\nf(\\alpha x_1+(1-\\alpha)x_2)\\le \\alpha f(x_1)+(1-\\alpha)f(x_2)-\\tfrac{1}{2}c\\alpha(1-\\alpha)\\|x_1-x_2\\|^2.\n\\tag{4.2.1}\n\\]\nThen $\\operatorname{dom} f^*=\\mathbb{R}^n$ and $\\nabla f^*$ is Lipschitzian with constant $1/c$ on $\\mathbb{R}^n$:\n\\[\n\\|\\nabla f^*(s_1)-\\nabla f^*(s_2)\\|\\le\\tfrac{1}{c}\\|s_1-s_2\\|\\quad\\text{for all }(s_1,s_2)\\in\\mathbb{R}^n\\times\\mathbb{R}^n.\n\\]",
        "title": "",
        "label": "thm:FCA-chapE-4.2.1",
        "lean_tag": "no-lean-tag",
        "proof": "We use the various equivalent definitions of strong convexity (see Theorem D.6.1.2). Fix $x_0$ and $s_0\\in\\partial f(x_0)$: for all $0\\neq d\\in\\mathbb{R}^n$ and $t\\ge 0$\n\\[\nf(x_0+td)\\ge f(x_0)+t\\langle s_0,d\\rangle+\\tfrac{1}{2}ct^2\\|d\\|^2,\n\\]\nhence $f'_+(d)=\\sigma_{\\operatorname{dom} f^*}(d)=+\\infty$, i.e.\\ $\\operatorname{dom} f^*=\\mathbb{R}^n$. Also, $f$ is in particular strictly convex, so we know from Theorem 4.1.1 that $f^*$ is differentiable (on $\\mathbb{R}^n$). Finally, strong convexity of $f$ can also be written $(s_1-s_2,x_1-x_2)\\ge c\\|x_1-x_2\\|^2$, in which we have $s_i\\in\\partial f(x_i)$, i.e.\\ $s_i=\\nabla f^*(s_i)$, for $i=1,2$. The rest follows from the Cauchy--Schwarz inequality."
    },
    {
        "statement": "Let $f:\\mathbb{R}^n\\to\\mathbb{R}$ be convex and have a gradient-mapping Lipschitzian with constant $L>0$ on $\\mathbb{R}^n$: for all $(x_1,x_2)\\in\\mathbb{R}^n\\times\\mathbb{R}^n$,\n\\[\n\\|\\nabla f(x_1)-\\nabla f(x_2)\\|\\le L\\|x_1-x_2\\|.\n\\]\nThen $f^*$ is strongly convex with modulus $1/L$ on each convex subset $C\\subset\\operatorname{dom}\\partial f^*$.\nIn particular, there holds for all $(x_1,x_2)\\in\\mathbb{R}^n\\times\\mathbb{R}^n$\n\\[\n\\langle\\nabla f(x_1)-\\nabla f(x_2),\\,x_1-x_2\\rangle\\ge \\tfrac{1}{L}\\|\\nabla f(x_1)-\\nabla f(x_2)\\|^2.\n\\tag{4.2.2}\n\\]",
        "title": "",
        "label": "thm:FCA-chapE-4.2.2",
        "lean_tag": "no-lean-tag",
        "proof": "Let $s_1$ and $s_2$ be arbitrary in $\\dom\\partial f^* \\subset\\dom f^*$; take $s$ and $s'$ on the segment $[s_1,s_2]$. To establish the strong convexity of $f^*$, we need to minorize the remainder term $f^*(s')-f^*(s)-\\langle x,s'-s\\rangle$, with $x\\in\\partial f^*(s)$. For this, we minorize $f^*(s')=\\sup_y\\{\\langle s',y\\rangle-f(y)\\}$, i.e. we majorize $f(y)$:\n\\[\n\\begin{aligned}\nf(y)&=f(x)+\\langle\\nabla f(x),\\,y-x\\rangle+\\int_0^1\\langle\\nabla f(x+t(y-x))-\\nabla f(x),\\,y-x\\rangle dt\\\\\n&\\le f(x)+\\langle\\nabla f(x),\\,y-x\\rangle+\\tfrac12 L\\|y-x\\|^2\\\\\n&= -f^*(s)+\\langle s,y\\rangle+\\tfrac12 L\\|y-x\\|^2\n\\end{aligned}\n\\]\n(we have used the property $\\int_0^1 t\\,dt=1/2$, as well as $x\\in\\partial f^*(s)$, i.e. $\\nabla f(x)=s$). In summary, we have\n\\[\nf^*(s')\\ge f^*(s)+\\sup_y\\big[\\,\\langle s'-s,y\\rangle-\\tfrac12 L\\|y-x\\|^2\\big].\n\\]\nObserve that the last supremum is nothing but the value at $s'-s$ of the conjugate of $\\tfrac12 L\\|\\,\\cdot\\,\\|^2$. Using the calculus rule 1.3.1, we have therefore proved\n\\begin{equation}\\label{thm:FCA-chapE-4.2.3}\nf^*(s')\\ge f^*(s)+\\langle s'-s,x\\rangle+\\tfrac1{2L}\\|s'-s\\|^2\n\\end{equation}\nfor all $s,s'$ in $[s_1,s_2]$ and all $x\\in\\partial f^*(s)$. Replacing $s'$ in (4.2.3) by $s_1$ and by $s_2$, and setting $s=\\alpha s_1+(1-\\alpha)s_2$, the strong convexity (4.2.1) for $f^*$ is established by convex combination.\n\nOn the other hand, replacing $(s,s')$ by $(s_1,s_2)$ in (4.2.3):\n\\[\nf^*(s_2)\\ge f^*(s_1)+\\langle s_2-s_1,x_1\\rangle+\\tfrac1{2L}\\|s_2-s_1\\|^2\\quad\\text{for all }x_1\\in\\partial f^*(s_1).\n\\]\nThen, replacing $(s,s')$ by $(s_2,s_1)$ and summing: $\\langle x_1-x_2,s_1-s_2\\rangle\\ge\\tfrac1L\\|s_1-s_2\\|^2$. In view of the differentiability of $f$, this is just (4.2.2), which has to hold for all $(x_1,x_2)$ simply because $\\Im\\partial f^*=\\dom\\nabla f=\\mathbb R^n$."
    },
    {
        "statement": "For $i=1,\\ldots,k$, let $C_i\\subset\\mathbb{R}^{n_i}$ be convex sets. Then\n\\[\n\\operatorname{ri}(C_1\\times\\cdots\\times C_k)\n= (\\operatorname{ri} C_1)\\times\\cdots\\times(\\operatorname{ri} C_k).\n\\]",
        "title": "",
        "label": "no-label",
        "lean_tag": "no-lean-tag",
        "proof": "It suffices to apply Definition 2.1.1 alone, observing that\n\\[\n\\operatorname{aff}(C_1\\times\\cdots\\times C_k)\n= (\\operatorname{aff} C_1)\\times\\cdots\\times(\\operatorname{aff} C_k).\n\\]"
    },
    {
        "statement": "Let $x\\in\\operatorname{cl}C$ and $x'\\in\\operatorname{ri}C$. Then the half-open segment\n\\[\n]x,x']=\\{\\alpha x+(1-\\alpha)x':\\;0\\le\\alpha<1\\}\n\\]\nis contained in $\\operatorname{ri}C$.",
        "title": "",
        "label": "no-label",
        "lean_tag": "no-lean-tag",
        "proof": "Take $x''=\\alpha x+(1-\\alpha)x'$, with $1>\\alpha\\ge0$. To avoid writing ``\\(\\operatorname{ri}C\\)'' every time, we assume without loss of generality that $\\operatorname{aff}C=\\mathbb{R}^n$.\n\nSince $x\\in\\operatorname{cl}C$, for all $\\varepsilon>0$, $x\\in C+B(0,\\varepsilon)$ and we can write\n\\[\nB(x'',\\varepsilon)\n= \\alpha x+(1-\\alpha)x' + B(0,\\varepsilon)\n= \\alpha C + (1-\\alpha)x' + (1+\\alpha)B(0,\\varepsilon)\n= \\alpha C + (1-\\alpha)\\{x'+B\\bigl(0,\\tfrac{1+\\alpha}{1-\\alpha}\\varepsilon\\bigr)\\}.\n\\]\n\nSince $x'\\in\\operatorname{int}C$, we can choose $\\varepsilon$ so small that $x'+B\\bigl(0,\\tfrac{1+\\alpha}{1-\\alpha}\\varepsilon\\bigr)\\subset C$. Then we have\n\\[\nB(x'',\\varepsilon)\\subset \\alpha C + (1-\\alpha)C = C\n\\]\n(where the last equality is just the definition of a convex set)."
    },
    {
        "statement": "For given $x\\in C$ and $d\\in\\mathbb{R}^n$, there holds\n\\[\n\\lim_{t\\downarrow0}\\frac{\\operatorname{P}_{C}(x+td)-x}{t}=\\operatorname{P}_{T_C(x)}(d).\n\\tag{5.3.3}\n\\]",
        "title": "",
        "label": "no-label",
        "lean_tag": "no-lean-tag",
        "proof": "HINT. Start from the characterization (3.1.3) of a projection, to observe that the difference quotient $[\\operatorname{P}_C(x+td)-x]/t$ is the projection of $d$ onto $(C-x)/t$. Then let $t\\downarrow0$; the result comes as well with the help of (5.1.4) and Remark 5.2.2."
    },
    {
        "statement": "Let $f\\in\\operatorname{Conv}\\mathbb{R}^n$ [resp.\\ $\\overline{\\operatorname{Conv}}\\mathbb{R}^n$] and let $g\\in\\operatorname{Conv}\\mathbb{R}$ [resp.\\ $\\overline{\\operatorname{Conv}}\\mathbb{R}$] be increasing. Assume that there is $x_0\\in\\mathbb{R}^n$ such that $f(x_0)\\in\\operatorname{dom}g$, and set $g(+\\infty):=+\\infty$. Then the composite function $g\\circ f:\\;x\\mapsto g(f(x))$ is in $\\operatorname{Conv}\\mathbb{R}^n$ [resp.\\ in $\\overline{\\operatorname{Conv}}\\mathbb{R}^n$].",
        "title": "",
        "label": "no-label",
        "lean_tag": "no-lean-tag",
        "proof": "It suffices to check the inequalities of definition: (1.1.1) for convexity, (1.2.3) for closedness."
    },
    {
        "statement": "Let $f\\in\\operatorname{Conv}\\mathbb{R}^n$. All the nonempty sublevel-sets of $f$ have the same asymptotic cone, which is the sublevel-set of $f^\\infty$ at the level $0$:\n\\[\n\\forall r\\in\\mathbb{R}\\ \\text{with } S_r(f)\\neq\\varnothing,\\qquad [S_r(f)]_\\infty=\\{d\\in\\mathbb{R}^n:\\ f^\\infty(d)\\le 0\\}.\n\\]\nIn particular, the following statements are equivalent:\n\\begin{enumerate}\n    \\item[(i)] There is $r$ for which $S_r(f)$ is nonempty and compact;\n    \\item[(ii)] all the sublevel-sets of $f$ are compact;\n    \\item[(iii)]  $f^\\infty_\\circ(d)>0$ for all nonzero $d\\in\\mathbb{R}^n$.\n\\end{enumerate}",
        "title": "",
        "label": "no-label",
        "lean_tag": "no-lean-tag",
        "proof": "By definition (A.2.2.1), a direction $d$ is in the asymptotic cone of the nonempty sublevel-set $S_r(f)$ if and only if\n\\[\nx\\in S_r(f)\\qquad\\Longrightarrow\\qquad [x+td\\in S_r(f)\\ \\text{for all }t>0],\n\\]\nwhich can also be written --- see (1.1.4):\n\\[\n(x,r)\\in\\operatorname{epi} f\\qquad\\Longrightarrow\\qquad (x+td,r+t\\times 0)\\in\\operatorname{epi} f\\ \\text{ for all }t>0;\n\\]\nand this in turn just means that $(d,0)\\in(\\operatorname{epi} f)_\\infty=\\operatorname{epi} f^\\infty_\\circ$. We have proved the first part of the theorem.\n\nA particular case is when the sublevel-set $S_0(f^\\infty_\\circ)$ is reduced to the singleton $\\{0\\}$, which exactly means (iii). This is therefore equivalent to $[S_r(f)]_\\infty=\\{0\\}$ for all $r\\in\\mathbb{R}$ with $S_r(f)\\neq\\emptyset$, which means that $S_r(f)$ is compact (Proposition A.2.2.3). The equivalence between (i), (ii) and (iii) is proved."
    },
    {
        "statement": "Let $g\\in\\operatorname{Conv}\\mathbb{R}^m$ be such that $0\\in\\dom g$ and let $A_0$ be linear from $\\mathbb{R}^n$ to $\\mathbb{R}^m$. Make the following assumption:\n\n\\[\n\\Im A_0\\cap\\ri\\dom g\\neq\\emptyset\\quad\\text{i.e.}\\quad\n0\\in\\ri\\dom g-\\Im A_0\\ [=\\ri(\\dom g-\\Im A_0)].\n\\]\n\nThen $(g\\circ A_0)^* = A_0^* g^*$; for every $s\\in\\dom (g\\circ A_0)^*$, the problem\n\\begin{equation}\\label{eq:2.2.2}\n\\inf_{p}\\{\\,g^*(p):\\ A_0^*p=s\\,\\}\n\\end{equation}\nhas at least one optimal solution $\\bar p$ and there holds $(g\\circ A_0)^*(s)=A_0^*g^*(s)=g^*(\\bar p)$.",
        "title": "",
        "label": "no-label",
        "lean_tag": "no-lean-tag",
        "proof": "To prove $(g\\circ A_0)^* = A_0^* g^*$, we have to prove that $A_0^* g^*$ is a closed function, i.e.\\ that its sublevel-sets are closed (Definition B.1.2.3).\n\nThus, for given $r\\in\\mathbb{R}$, take a sequence $(s_k)$ converging to some $s$ and such that\n\\[\n(A_0^* g^*)(s_k) \\le r.\n\\]\nTake also $\\delta_k\\downarrow 0$; from the definition of the image-function, we can find $p_k\\in\\mathbb{R}^m$ such that\n\\[\ng^*(p_k) \\le r + \\delta_k\\quad\\text{and}\\quad A_0^* p_k = s_k.\n\\]\n\nLet $q_k$ be the orthogonal projection of $p_k$ onto the subspace $V := \\operatorname{lin}\\dom g - \\Im A_0$. Since $V$ contains $\\operatorname{lin}\\dom g$, Proposition 1.3.4 (with $z=0$) gives $g^*(p_k) = g^*(q_k)$. Furthermore, $V^\\perp = (\\lin\\dom g)^\\perp \\cap \\Ker A_0^*$; in particular, $q_k - p_k \\in \\Ker A_0^*$. In summary, we have singled out $q_k\\in V$ such that\n\\[\ng^*(q_k) \\le r + \\delta_k\\quad\\text{and}\\quad A_0^* q_k = s_k\\qquad\\text{for all }k.\n\\tag{2.2.3}\n\\]\n\nSuppose we can bound $q_k$. Extracting a subsequence if necessary, we will have $q_k \\to \\bar q$ and, passing to the limit, we will obtain (since $g^*$ is l.s.c)\n\\[\ng^*(\\bar q) \\le \\liminf g^*(q_k) \\le r\\quad\\text{and}\\quad A_0^* \\bar q = s.\n\\]\n\nThe required closedness property $A_0^* g^*(\\bar q) \\le r$ will follow by definition. Furthermore, this $\\bar q$ will be a solution of (2.2.2) in the particular case $s_k \\equiv s$ and $r = (A_0^* g^*)(s)$. In this case, $(q_k)$ will be actually a minimizing sequence of (2.2.2).\n\nTo prove boundedness of $q_k$, use the assumption: for some $\\varepsilon>0$, $B_m(0,\\varepsilon)\\cap V$ is included in $\\dom g - \\Im A$. Thus, for arbitrary $z\\in B_m(0,\\varepsilon)\\cap V$, we can find $y\\in\\dom g$ and $x\\in\\mathbb{R}^n$ such that $z = y - A_0 x$. Then\n\\begin{align*}\n\\langle q_k, z\\rangle_m &= \\langle q_k, y\\rangle_m - \\langle A_0^* q_k, x\\rangle_n\\\\\n&\\le g(y) + g^*(q_k) - \\langle A_0^* q_k, x\\rangle_n &&[\\text{Fenchel (1.1.3)}]\\\\\n&\\le g(y) + r + \\delta_k - \\langle s_k, x\\rangle_n. &&[(2.2.3)]\n\\end{align*}\n\nWe conclude that $\\sup\\{\\langle q_k, z\\rangle : k=1,2,\\dots\\}$ is bounded for any $z\\in B_m(0,\\varepsilon)\\cap V$, which implies that $q_k$ is bounded; this is Proposition V.2.1.3 in the vector space $V$."
    }
]