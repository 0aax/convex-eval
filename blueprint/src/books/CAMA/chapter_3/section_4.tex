\subsection{Separation between convex sets}

\begin{theorem}\label{thm:4.1.1}
Let \(C\subset\mathbb{R}^n\) be nonempty closed convex, and let \(x\notin C\). Then there exists \(s\in\mathbb{R}^n\) such that
\[
\langle s,x\rangle > \sup_{y\in C}\langle s,y\rangle .
\tag{4.1.1}
\]
\end{theorem}

\begin{proof}
Set $s := x - p_C(x) \neq 0$. We write (3.1.3) as
\[
0 \ge (s,y-x+s) = (s,y) - (s,x) + \|s\|^2.
\]
Thus we have
\[
(s,x) - \|s\|^2 \ge (s,y)\qquad\text{for all }y\in C,
\]
and our $s$ is a convenient answer for (4.1.1).
\end{proof}

\begin{corollary}[Strict Separation of Convex Sets]\label{cor:4.1.3}
Let $C_1,\,C_2$ be two nonempty closed convex sets with $C_1\cap C_2=\varnothing$. If $C_2$ is bounded, there exists $s\in\mathbb{R}^n$ such that
\[
\sup_{y\in C_1}\langle s,y\rangle \;<\; \min_{y\in C_2}\langle s,y\rangle .
\tag{4.1.2}
\]
\end{corollary}

PROOF. The set $C_1-C_2$ is convex (Proposition 1.2.4) and closed (because $C_2$ is compact). To say that $C_1$ and $C_2$ are disjoint is to say that $0\notin C_1-C_2$, so we have by Theorem 4.1.1 an $s\in\mathbb{R}^n$ separating $\{0\}$ from $C_1-C_2$:
\[
\sup\{\langle s,y\rangle : y\in C_1-C_2\}<\langle s,0\rangle=0.
\]
This means:
\[
0 \;>\; \sup_{y_1\in C_1}\langle s,y_1\rangle+\sup_{y_2\in C_2}\langle s,-y_2\rangle
=\sup_{y_1\in C_1}\langle s,y_1\rangle-\inf_{y_2\in C_2}\langle s,y_2\rangle.
\]
Because $C_2$ is bounded, the last infimum (is a min and) is finite and can be moved to the left-hand side. \qed

\begin{theorem}\label{thm:4.1.4}
(Proper Separation of Convex Sets) If the two nonempty convex sets $C_1$ and $C_2$ satisfy $\operatorname{ri}C_1\cap(\operatorname{ri}C_2)=\varnothing$, they can be properly separated.
\end{theorem}

\subsection{First consequences of the separation properties}

\begin{lemma}
\label{lem:4.2.1}
Let $x\in\partial C$, where $C\neq\varnothing$ is convex in $\mathbb{R}^n$ (naturally $C\neq\mathbb{R}^n$). There exists a hyperplane supporting $C$ at $x$.
\end{lemma}

\begin{proof}
Because $C$, $\operatorname{cl}C$ and their complements have the same boundary (remember Remark 2.1.9), a sequence $\{x_k\}$ can be found such that
\[
x_k\notin C\quad\text{for }k=1,2,\dots\quad\text{and}\quad
\lim_{k\to+\infty}x_k=x.
\]

For each $k$ we have by Theorem 4.1.1 some $s_k$ with $\|s_k\|=1$ such that
\[
\langle s_k,\; x_k-y\rangle>0\quad\text{for all }y\in C\subset\operatorname{cl}C .
\]
Extract a subsequence if necessary so that $s_k\to s$ (note: $s\neq 0$) and pass to the limit to obtain
\[
\langle s,\; x-y\rangle\ge0\quad\text{for all }y\in C,
\]
which is the required result $\langle s,x\rangle=r\ge\langle s,y\rangle$ for all $y\in C$.
\end{proof}

\begin{proposition}
\label{prop:4.2.3}
Let $S\subset\mathbb{R}^n$ and $C:=\operatorname{co}S$. Any $x\in C\cap\operatorname{bd}C$ can be represented as a convex combination of $n$ elements of $S$.
\end{proposition}

\begin{proof}
Because $x\in\operatorname{bd}C$, there is a hyperplane $H_{s,r}$ supporting $C$ at $x$: for some $s\neq 0$ and $r\in\mathbb{R}$,
\[
\langle s,x\rangle - r = 0\qquad\text{and}\qquad \langle s,y\rangle - r \le 0\ \text{ for all } y\in C.
\tag{4.2.1}
\]
On the other hand, Carathéodory's Theorem 1.3.6 implies the existence of points $x_1,\dots,x_{n+1}$ in $S$ and convex multipliers $\alpha_1,\dots,\alpha_{n+1}$ such that $x=\sum_{i=1}^{n+1}\alpha_i x_i$; and each $\alpha_i$ can be assumed positive (otherwise the proof is finished).

Setting successively $y=x_i$ in (4.2.1), we obtain by convex combination
\[
0=\langle s,x\rangle - r = \sum_{i=1}^{n+1}\alpha_i\bigl(\langle s,x_i\rangle - r\bigr)\le 0,
\]
so each $\langle s,x_i\rangle - r$ is actually $0$. Each $x_i$ is therefore not only in $S$, but also in $H_{s,r}$, a set whose dimension is $n-1$. It follows that our starting $x$, which is in $\operatorname{co}\{x_1,\dots,x_{n+1}\}$, can be described as the convex hull of only $n$ among these $x_i$'s.
\end{proof}

\begin{theorem}\label{thm:4.2.4}
Let $\varnothing\neq C\subseteq\mathbb{R}^n$ be convex. The set $C^*$ defined above is the closure of $C$.
\end{theorem}

\begin{proof}
By construction, $C^*\supseteq\operatorname{cl}C$. Conversely, take $x\notin\operatorname{cl}C$; we can separate $x$ and $\operatorname{cl}C$: there exists $s_0\neq 0$ such that
\[
\langle s_0,x\rangle>\sup_{y\in C}\langle s_0,y\rangle=:\,r_0.
\]
Then $(s_0,r_0)\in\Sigma_C$; but $x\notin H_{s_0,r_0}$, hence $x\notin C^*$.
\end{proof}

\begin{corollary}\label{cor:4.2.5}
The data $(s_j,r_j)\in\mathbb{R}^n\times\mathbb{R}$ for $j$ in an arbitrary index set $J$ is equivalent to the data of a closed convex set $C$ via the relation
\[
C=\bigcap_{j\in J}\{x\in\mathbb{R}^n:\langle s_j,x\rangle\le r_j\}.
\]
\end{corollary}

\begin{proof}
If $C$ is given, define $\{(s_j,r_j)\}_{J}:=\Sigma_C$ as in (4.2.2). If $\{(s_j,r_j)\}_J$ is given, the intersection of the corresponding half-spaces is a closed convex set. Note here that we can define at the same time the whole of $\mathbb{R}^n$ and the empty sets as two extreme cases.
\end{proof}

\begin{definition}[Polyhedral Sets]\label{def:4.2.6}
A \emph{closed convex polyhedron} is an intersection of finitely many half-spaces.  Take $(s_1,r_1),\ldots,(s_m,r_m)$ in $\mathbb{R}^n\times\mathbb{R}$, with $s_i\neq 0$ for $i=1,\ldots,m$; then define
\[
P := \{x\in\mathbb{R}^n : \langle s_j,x\rangle \le r_j\ \text{ for } j=1,\ldots,m\},
\]
or in matrix notations (assuming the dot-product for $\langle\cdot,\cdot\rangle$),
\[
P = \{x\in\mathbb{R}^n : Ax \le b\},
\]
if $A$ is the matrix whose rows are $s_j$ and $b\in\mathbb{R}^m$ has coordinates $r_j$.

A \emph{closed convex polyhedral cone} is the special case where $b=0$.
\end{definition}

Proposition 4.2.7 Let $K$ be a convex cone with polar $K^{\circ}$; then, the polar $K^{\circ\circ}$ of $K^{\circ}$ is the closure of $K$.

\begin{proof}
We exploit Remark 4.1.2: due to its conical character $(\alpha x\in K$ if $\alpha\in K$ and $\alpha>0)$, $\operatorname{cl}K$ has a very special support function:
\[
\sigma_{\operatorname{cl}K}(s)=
\begin{cases}
\langle s,0\rangle=0 &\text{if }\langle s,x\rangle\le 0\text{ for all }x\in\operatorname{cl}K,\\[4pt]
+\infty &\text{otherwise}.
\end{cases}
\]
In other words, $\sigma_{\operatorname{cl}K}$ is $0$ on $K^{\circ}$, $+\infty$ elsewhere. Thus, the characterization
\[
x\in\operatorname{cl}K \iff \langle\cdot,x\rangle\le\sigma_{\operatorname{cl}K}(\cdot)
\]
becomes
\[
x\in\operatorname{cl}K \iff
\begin{cases}
\langle s,x\rangle\le 0 &\text{for all }s\in K^{\circ}\\[4pt]
(\langle s,x\rangle\text{ arbitrary}) &\text{for }s\notin K^{\circ},
\end{cases}
\]
in which we recognize the definition of $K^{\circ\circ}$.
\end{proof}

\subsection{The lemma of Minkowski and Farkas}

\begin{lemma}[Farkas I]\label{lem:4.3.1}
Let $b,s_1,\dots,s_m$ be given in $\mathbb{R}^n$. The set
\[
\{x\in\mathbb{R}^n:\ \langle s_j,x\rangle\le 0\quad\text{for }j=1,\dots,m\}
\tag{4.3.1}
\]
is contained in the set
\[
\{x\in\mathbb{R}^n:\ \langle b,x\rangle\le 0\}
\tag{4.3.2}
\]
if and only if (see Definition 1.4.5 of a conical hull)
\[
b\in\operatorname{cone}\{s_1,\dots,s_m\}.
\tag{4.3.3}
\]
\end{lemma}

\begin{lemma}[Farkas II]\label{lem:4.3.2}
Let $b,s_1,\dots,s_m$ be given in $\mathbb{R}^n$. Then exactly one of the following statements is true.
\begin{enumerate}
\item[$P:=$] \eqref{4.3.4} has a solution $\alpha\in\mathbb{R}^n$.
\item[$Q:=$] 
\[
\left\{
\begin{aligned}
&\langle b,x\rangle>0,\\
&\langle s_j,x\rangle\le 0\quad\text{for }j=1,\dots,m
\end{aligned}
\right.
\]
has a solution $x\in\mathbb{R}^n$.
\end{enumerate}
\end{lemma}

\begin{lemma}[Farkas III]\label{lem:4.3.3}
Let $s_1,\dots,s_m$ be given in $\mathbb{R}^n$. Then the convex cone
\[
K:=\operatorname{cone}\{s_1,\dots,s_m\}=\Big\{\sum_{j=1}^m\alpha_j s_j:\ \alpha_j\ge 0\ \text{for }j=1,\dots,m\Big\}
\]
is closed.
\end{lemma}

\begin{proof}
It is quite similar to that of Carathéodory's Theorem 1.3.6. First, the proof is easy if the $s_j$'s are linearly independent: then, the convergence of
\begin{equation}\label{eq:4.3.5}
x^k=\sum_{j=1}^m\alpha_j^k s_j\quad\text{for }k\to\infty
\end{equation}
is equivalent to the convergence of each $\{\alpha_j^k\}$ to some $\alpha_j$, which must be nonnegative if each $\alpha_j^k$ in \eqref{eq:4.3.5} is nonnegative.

Suppose, on the contrary, that the system $\sum_{j=1}^m\beta_j s_j=0$ has a nonzero solution $\beta\in\mathbb{R}^m$ and assume $\beta_j<0$ for some $j$ (change $\beta$ to $-\beta$ if necessary). As in the proof of Theorem 1.3.6, write each $x\in K$ as
\[
x=\sum_{j=1}^m\alpha_j s_j=\sum_{j=1}^m[\alpha_j+t^*(x)\beta_j]s_j=\sum_{j\ne i(x)}\alpha'_j s_j,
\]
where
\[
i(x)\in\operatorname{Argmin}_{\beta_j<0}\frac{-\alpha_j}{\beta_j},\qquad
t^*(x):=\frac{-\alpha_{i(x)}}{\beta_{i(x)}},
\]
so that each $\alpha'_j=\alpha_j+t^*(x)\beta_j$ is nonnegative. Letting $x$ vary in $K$, we thus construct
a decomposition
\[
K=\bigcup\{K_i:\; i=1,\ldots,m\},
\]
where $K_i$ is the conical hull of the $m-1$ generators $s_j,\; j\neq i$.

Now, if there is some $i$ such that the generators of $K_i$ are linearly dependent, we repeat the argument for a further decomposition of this $K_i$. After finitely many such operations, we end up with a decomposition of $K$ as a finite union of polyhedral convex cones, each having linearly independent generators. All these cones are therefore closed (first part of the proof), so $K$ is closed as well.
\end{proof}

\begin{theorem}[Generalized Farkas]\label{thm:4.3.4}
Let be given $(b,r)$ and $(s_j,\rho_j)$ in $\mathbb{R}^n\times\mathbb{R}$, where $j$ varies in an (arbitrary) index set $J$. Suppose that the system of inequalities
\[
\langle s_j,x\rangle \le \rho_j\qquad\text{for all }j\in J
\tag{4.3.6}
\]
has a solution $x\in\mathbb{R}^n$ (the system is consistent). Then the following two properties are equivalent:

(i) $\langle b,x\rangle \le r$ for all $x$ satisfying (4.3.6);

(ii) $(b,r)$ is in the closed convex conical hull of $S:=\{(0,1)\}\cup\{(s_j,\rho_j)\}_{j\in J}$.
\end{theorem}

\begin{proof}
[(ii)\,$\Rightarrow$\,(i)] Let first $(b,r)$ be in $K:=\operatorname{cone}S$. In other words, there exists a finite set $\{1,\dots,m\}\subset J$ and nonnegative $\alpha_0,\alpha_1,\dots,\alpha_m$ such that (we adopt the convention $\sum\emptyset=0$)
\[
b=\sum_{j=1}^m\alpha_j s_j\qquad\text{and}\qquad r=\alpha_0+\sum_{j=1}^m\alpha_j\rho_j.
\]
For each $x$ satisfying (4.3.6) we can write
\[
\langle b,x\rangle \le r-\alpha_0\le r.
\tag{4.3.7}
\]
If, now, $(b,r)$ is in the closure of $K$, pass to the limit in (4.3.7) to establish the required conclusion (i) for all $(b,r)$ described by (ii).

[(i)\,$\Rightarrow$\,(ii)] If $(b,r)\notin\operatorname{cl}K$, separate $(b,r)$ from $\operatorname{cl}K$: equipping $\mathbb{R}^n\times\mathbb{R}$ with the scalar product
\[
\langle\!\langle (b,r),(d,t)\rangle\!\rangle:=\langle b,d\rangle+rt,
\]
there exists $(d,-t)\in\mathbb{R}^n\times\mathbb{R}$ such that
\[
\sup_{(s,\rho)\in K}\; [\langle s,d\rangle - \rho t] < \langle b,d\rangle - r t . \tag{4.3.8}
\]

It follows first that the left-hand supremum is a finite number \(\kappa\). Then the conical character of \(K\) implies \(\kappa \le 0\), because \(\alpha\kappa \le \kappa\) for all \(\alpha>0\); actually \(\kappa=0\) because \((0,0)\in K\). In summary, we have singled out \((d,t)\in\mathbb{R}^n\times\mathbb{R}\) such that
\[
t \ge 0 \qquad [\text{take }(0,1)\in K]
\]
\[
(\ast)\qquad \langle s_j,d\rangle - \rho_j t \le 0\ \text{ for all } j\in J \qquad [\text{take }(s_j,\rho_j)\in K]
\]
\[
(\ast\ast)\qquad \langle b,d\rangle - r t > 0. \qquad [\text{don't forget (4.3.8)}]
\]

Now consider two cases:
\begin{itemize}
\item If \(t>0\), divide \((\ast)\) and \((\ast\ast)\) by \(t\) to exhibit the point \(x=d/t\) violating (i).
\item If \(t=0\), take \(x_0\) satisfying (4.3.6). Observe from \((\ast)\) that, for all \(\alpha>0\), the point \(x(\alpha)=x_0+\alpha d\) satisfies (4.3.6) as well. Yet, let \(\alpha\to +\infty\) in
\[
\langle b,x(\alpha)\rangle = \langle b,x_0\rangle + \alpha\langle b,d\rangle
\]
to realize from \((\ast\ast)\) that \(x(\alpha)\) violates (i) if \(\alpha\) is large enough.
\end{itemize}

Thus we have proved in both cases that "not (ii) \(\Rightarrow\) not (i)".
\end{proof}