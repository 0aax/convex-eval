\subsection{Monotonicity property of the subdifferential}

\begin{proposition}
\label{prop:FCA-chapD-6.1.1}
\lean{FCA_chap_D_6_1_1}
The subdifferential mapping is monotone in the sense that, for all $x_1$ and $x_2$ in $\mathbb{R}^n$,
\begin{equation}\tag{6.1.1}
\langle s_2 - s_1, x_2 - x_1\rangle \ge 0\quad\text{for all } s_i\in\partial f(x_i),\; i=1,2.
\end{equation}
\end{proposition}

\begin{proof}
The subgradient inequalities
\[
f(x_2)\ge f(x_1)+\langle s_1, x_2-x_1\rangle\quad\text{for all }s_1\in\partial f(x_1)
\]
\[
f(x_1)\ge f(x_2)+\langle s_2, x_1-x_2\rangle\quad\text{for all }s_2\in\partial f(x_2)
\]
give the result simply by addition.
\end{proof}

\begin{theorem}
\label{thm:FCA-chapD-6.1.2}
\lean{FCA_chap_D_6_1_2}
A necessary and sufficient for a convex function $f:\mathbb{R}^n\to\mathbb{R}$ to be strongly convex with modulus $c>0$ on a convex set $C$ is: for all $x_1,x_2$ in $C$,
\begin{equation}\tag{6.1.3}
f(x_2)\ge f(x_1)+\langle s,x_2-x_1\rangle+\frac{c}{2}\|x_2-x_1\|^2\quad\text{for all }s\in\partial f(x_1),
\end{equation}
or equivalently
\begin{equation}\tag{6.1.4}
\langle s_2-s_1,x_2-x_1\rangle\ge c\|x_2-x_1\|^2\quad\text{for all }s_i\in\partial f(x_i),\; i=1,2.
\end{equation}
\end{theorem}

\begin{proof}
For $x_1,x_2$ given in $C$ and $\alpha\in]0,1[$, we will use the notation
\[
x^\alpha:=\alpha x_2+(1-\alpha)x_1=x_1+\alpha(x_2-x_1)
\]
and we will prove (6.1.3) $\Rightarrow$ (6.1.2) $\Rightarrow$ (6.1.4) $\Rightarrow$ (6.1.3).

[(6.1.3) $\Rightarrow$ (6.1.2)] Write (6.1.3) with $x_1$ replaced by $x^\alpha\in C$: for $s\in\partial f(x^\alpha)$,
\[
f(x_2)\ge f(x^\alpha)+\langle s,x_2-x^\alpha\rangle+\frac{c}{2}\|x_2-x^\alpha\|^2,
\]
or equivalently
\[
f(x_2)\ge f(x^\alpha)+(1-\alpha)\langle s,x_2-x_1\rangle+\frac{c}{2}(1-\alpha)^2\|x_2-x_1\|^2.
\]

Likewise,
\[
f(x_1)\ge f(x^\alpha)+\alpha\langle s,x_1-x_2\rangle+\frac{c}{2}\alpha^2\|x_1-x_2\|^2.
\]

Multiply these last two inequalities by $\alpha$ and $(1-\alpha)$ respectively, and add to obtain
\[
\alpha f(x_2)+(1-\alpha)f(x_1)\ge f(x^\alpha)+\frac{c}{2}\|x_2-x_1\|^2\big[\alpha(1-\alpha)^2+(1-\alpha)\alpha^2\big].
\]
Then realize after simplification that this is just (6.1.2).

[(6.1.2) $\Rightarrow$ (6.1.4)] Write (6.1.2) as
\[
\frac{f(x^\alpha)-f(x_1)}{\alpha}+\frac{c}{2}(1-\alpha)\|x_2-x_1\|^2\le f(x_2)-f(x_1)
\]
and let $\alpha\downarrow0$ to obtain $f'(x_1,x_2-x_1)+\tfrac{c}{2}\|x_2-x_1\|^2\le f(x_2)-f(x_1)$, which implies (6.1.3). Then, copying (6.1.3) with $x_1$ and $x_2$ interchanged and adding yields (6.1.4) directly.

[(6.1.4) $\Rightarrow$ (6.1.3)] Apply Theorem 2.3.4 to the one-dimensional convex function $\mathbb{R}\ni\alpha\mapsto\varphi(\alpha):=f(x^\alpha)$:
\begin{equation}\tag{6.1.5}
f(x_2)-f(x_1)=\varphi(1)-\varphi(0)=\int_0^1\langle s^\alpha,x_2-x_1\rangle\,d\alpha
\end{equation}
where $s^\alpha\in\partial f(x^\alpha)$ for $\alpha\in[0,1]$. Then take $s_1$ arbitrary in $\partial f(x_1)$ and apply (6.1.4): $\langle s^\alpha-s_1,x^\alpha-x_1\rangle\ge c\|x^\alpha-x_1\|^2$ i.e., using the value of $x^\alpha$,
\[
\alpha\langle s^\alpha,x_2-x_1\rangle\ge\alpha\langle s_1,x_2-x_1\rangle+c\alpha^2\|x_2-x_1\|^2.
\]

The result follows by using this inequality to minorize the integral in (6.1.5).
\end{proof}

\begin{proposition}
\label{prop:FCA-chapD-6.1.3}
\lean{FCA_chap_D_6_1_3}
A necessary and sufficient condition for a convex function $f:\mathbb{R}^n\to\mathbb{R}$ to be strictly convex on a convex set $C$ is: for all $x_1,x_2\in C$ with $x_2\neq x_1$,
\[
f(x_2)>f(x_1)+\langle s,x_2-x_1\rangle\quad\text{for all } s\in\partial f(x_1)
\]
or equivalently
\[
\langle s_2-s_1,x_2-x_1\rangle>0\quad\text{for all } s_i\in\partial f(x_i),\ i=1,2.
\]
\end{proposition}

\begin{proof}
Copy the proof of Theorem 6.1.2 with $c=0$ and the relevant ``$ \ge $''-signs replaced by strict inequalities. The only delicate point is in the [(6.1.2) $\Rightarrow$ (6.1.4)]-stage: use monotonicity of the difference quotient.
\end{proof}

\subsection{Continuity properties of the subdifferential}

\begin{proposition}
\label{prop:FCA-chapD-6.2.1}
\lean{FCA_chap_D_6_2_1}
Let $f:\mathbb{R}^n\to\mathbb{R}$ be convex. The graph of its subdifferential mapping is closed in $\mathbb{R}^n\times\mathbb{R}^n$.
\end{proposition}

\begin{proof}
Let $(x_k,s_k)$ be a sequence in $\operatorname{gr}\partial f$ converging to $(x,s)\in\mathbb{R}^n\times\mathbb{R}^n$. We must prove that $(x,s)\in\operatorname{gr}\partial f$, which is easy. We have for all $k$
\[
f(y)\ge f(x_k)+\langle s_k,y-x_k\rangle\quad\text{for all }y\in\mathbb{R}^n;
\]
pass to the limit on $k$, using continuity of $f$ and of the scalar product.
\end{proof}

\begin{proposition}
\label{prop:FCA-chapD-6.2.2}
\lean{FCA_chap_D_6_2_2}
The mapping $\partial f$ is locally bounded, i.e.\ the image $\partial f(B)$ of a bounded set $B\subset\mathbb{R}^n$ is a bounded set in $\mathbb{R}^n$.
\end{proposition}

\begin{proof}
For arbitrary $x$ in $B$ and $s\neq 0$ in $\partial f(x)$, the subgradient inequality implies in particular $f(x+s/\|s\|)\ge f(x)+\|s\|$. On the other hand, $f$ is Lipschitz-continuous on the bounded set $B+B(0,1)$ (Theorem B.3.1.2). Hence $\|s\|\le L$ for some $L$.
\end{proof}

\begin{theorem}
\label{thm:FCA-chapD-6.2.4}
\lean{FCA_chap_D_6_2_4}
The subdifferential mapping of a convex function $f:\mathbb{R}^n\to\mathbb{R}$ is outer semi-continuous at any $x\in\mathbb{R}^n$, i.e.
\[
\forall\varepsilon>0,\ \exists\delta>0:\quad y\in B(x,\delta)\implies \partial f(y)\subset \partial f(x)+B(0,\varepsilon).
\tag{6.2.1}
\]
\end{theorem}

\begin{proof}
Assume for contradiction that, at some $x$, there are $\varepsilon>0$ and a sequence $(x_k,s_k)_k$ with
\[
x_k\to x\quad\text{for }k\to\infty
\qquad\text{and}\qquad
s_k\in\partial f(x_k),\ s_k\not\in\partial f(x)+B(0,\varepsilon)\quad\text{for }k=1,2,\dots
\tag{6.2.2}
\]
A subsequence of the bounded $(s_k)$ (Proposition 6.2.2) converges to $s\in\partial f(x)$ (Proposition 6.2.1). This contradicts (6.2.2), which implies $s\not\in\partial f(x)+B(0,1/2\varepsilon)$.
\end{proof}

\begin{corollary}
\label{cor:FCA-chapD-6.2.5}
\lean{FCA_chap_D_6_2_5}
For $f:\mathbb{R}^n\to\mathbb{R}$ convex, the function $f'(\cdot,d)$ is upper semicontinuous: at all $x\in\mathbb{R}^n$,
\[
f'(x,d)=\limsup_{y\to x} f'(y,d)\qquad\text{for all }d\in\mathbb{R}^n.
\]
\end{corollary}

\begin{proof}
Use Theorem 6.2.4, in conjunction with Proposition C.3.3.7.
\end{proof}

\begin{theorem}
\label{thm:FCA-chapD-6.2.7}
\lean{FCA_chap_D_6_2_7}
Let $(f_k)$ be a sequence of (finite) convex functions converging pointwise to $f:\mathbb{R}^n\to\mathbb{R}$ and let $(x_k)$ converge to $x\in\mathbb{R}^n$. For any $\varepsilon>0$,
\[
\partial f_k(x_k)\subset\partial f(x)+B(0,\varepsilon)\qquad\text{for $k$ large enough.}
\]
\end{theorem}

\begin{proof}
Let $\varepsilon>0$ be given. Recall (Theorem B.3.1.4) that the pointwise convergence of $(f_k)$ to $f$ implies its uniform convergence on every compact set of $\mathbb{R}^n$.

First, we establish boundedness: for $s_k\neq 0$ arbitrary in $\partial f_k(x_k)$, we have
\[
f_k(x_k+s_k/\|s_k\|)\ge f_k(x_k)+\|s_k\|.
\]

The uniform convergence of $(f_k)$ to $f$ on $B(x,2)$ implies for $k$ large enough
\[
\|s_k\|\le f(x_k+s_k/\|s_k\|)-f(x_k)+\varepsilon,
\]
and the Lipschitz property of $f$ on $B(x,2)$ ensures that $(s_k)$ is bounded.

Now suppose for contradiction that, for some infinite subsequence, there is some $s_k\in\partial f_k(x_k)$ which is not in $\partial f(x)+B(0,\varepsilon)$. Any cluster point of this $(s_k)$ --- and there is at least one --- is out of $\partial f(x)+B(0,1/2\varepsilon)$. Yet, with $y$ arbitrary in $\mathbb{R}^n$, write
\[
f_k(y)\ge f_k(x_k)+\langle s_k,y-x_k\rangle
\]
and pass to the limit (on a further subsequence such that $s_k\to s$): pointwise [resp.\ uniform] convergence of $(f_k)$ to $f$ at $y$ [resp.\ around $x$], and continuity of the scalar product give $f(y)\ge f(x)+\langle s,y-x\rangle$. Because $y$ was arbitrary, we obtain the contradiction $s\in\partial f(x)$.
\end{proof}

\begin{corollary}
\label{cor:FCA-chapD-6.2.8}
\lean{FCA_chap_D_6_2_8}
Let $(f_k)$ be a sequence of (finite) differentiable convex functions converging pointwise to the differentiable $f:\mathbb{R}^n\to\mathbb{R}$. Then $\nabla f_k$ converges to $\nabla f$ uniformly on every compact set of $\mathbb{R}^n$.
\end{corollary}

\begin{proof}
Take $S$ compact; suppose for contradiction that there exists $\varepsilon>0$ and a sequence $(x_k)\subset S$ such that
\[
\|\nabla f_k(x_k)-\nabla f(x_k)\|>\varepsilon\quad\text{for }k=1,2,\dots
\]
Extracting a subsequence if necessary, we may suppose $x_k\to x\in S$; Theorem 6.2.7 assures that the sequences $(\nabla f_k(x_k))$ and $(\nabla f(x_k))$ both converge to $\nabla f(x)$, implying $0\ge\varepsilon$.
\end{proof}

\subsection{Subdifferentials and limits of subgradients}

\begin{theorem}
\label{thm:FCA-chapD-6.3.1}
\lean{FCA_chap_D_6_3_1}
Let $f:\mathbb{R}^n\to\mathbb{R}$ be convex. With the notation (6.3.1), $\partial f(x)=\operatorname{co}\gamma f(x)$ for all $x\in\mathbb{R}^n$.
\end{theorem}

\begin{proposition}
\label{prop:FCA-chapD-6.3.4}
Let $x$ and $d\neq 0$ be given in $\mathbb{R}^n$. For any sequence $(t_k,s_k,d_k)\subset\mathbb{R}_+^*\times\mathbb{R}^n\times\mathbb{R}^n$ satisfying
\[
t_k\downarrow0,\quad s_k\in\partial f(x+t_k d_k),\quad d_k\to d,\qquad\text{for }k=1,2,\dots
\]
and any cluster point $s$ of $(s_k)$, there holds
\[
s\in\partial f(x)\qquad\text{and}\qquad\langle s,d\rangle=f'(x,d).
\]
\end{proposition}

\begin{proof}
The first property comes from the results in \S6.2. For the second, use the monotonicity of $\partial f$:
\[
0\le\langle s_k-s',x+t_k d_k-x\rangle = t_k\langle s_k-s',d_k\rangle\qquad\text{for all }s'\in\partial f(x).
\]
Divide by $t_k>0$ and pass to the limit to get $f'(x,d)\le\langle s,d\rangle$. The converse inequality being trivial, the proof is complete.
\end{proof}